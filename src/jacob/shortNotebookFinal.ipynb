{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522065 entries, 0 to 1522064\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count    Dtype  \n",
      "---  ------     --------------    -----  \n",
      " 0   time       1522065 non-null  object \n",
      " 1   cog        1522065 non-null  float64\n",
      " 2   sog        1522065 non-null  float64\n",
      " 3   rot        1522065 non-null  int64  \n",
      " 4   heading    1522065 non-null  int64  \n",
      " 5   navstat    1522065 non-null  int64  \n",
      " 6   etaRaw     1522065 non-null  object \n",
      " 7   latitude   1522065 non-null  float64\n",
      " 8   longitude  1522065 non-null  float64\n",
      " 9   vesselId   1522065 non-null  object \n",
      " 10  portId     1520450 non-null  object \n",
      "dtypes: float64(4), int64(3), object(4)\n",
      "memory usage: 127.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = 'ais_train.csv'\n",
    "\n",
    "# Load the file into a pandas dataframe\n",
    "baseDataset = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "# Display the dataframe\n",
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51739 entries, 0 to 51738\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   ID              51739 non-null  int64  \n",
      " 1   vesselId        51739 non-null  object \n",
      " 2   time            51739 non-null  object \n",
      " 3   scaling_factor  51739 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "testset = pd.read_csv('ais_test.csv')\n",
    "testset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial pre-processing of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ais_train(df):\n",
    "\n",
    "    # Step 1: Convert 'time' to datetime and drop useless columns\n",
    "    df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    df.drop('etaRaw', axis=1, inplace=True)\n",
    "\n",
    "    # Step 2: Sort by vesselId and time\n",
    "    df = df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # Step 3: Convert relevant columns to float\n",
    "    df['cog'] = df['cog'].astype(float)\n",
    "    df['sog'] = df['sog'].astype(float)\n",
    "    df['rot'] = df['rot'].astype(float)\n",
    "    df['heading'] = df['heading'].astype(float)\n",
    "    df['latitude'] = df['latitude'].astype(float)\n",
    "    df['longitude'] = df['longitude'].astype(float)\n",
    "    \n",
    "    # Step 4: Replace invalid or default values with NaN\n",
    "    df['cog'] = np.where((df['cog'] == 360) | (df['cog'] > 360) | (df['cog'] < 0), np.nan, df['cog'])\n",
    "    df['sog'] = np.where((df['sog'] == 1023) | (df['sog'] < 0), np.nan, df['sog'])\n",
    "    df['heading'] = np.where((df['heading'] > 360) | (df['heading'] == 511) | (df['heading'] < 0), np.nan, df['heading'])\n",
    "    df['rot'] = np.where(df['rot'].isin([127, 128, -127, -128]), np.nan, df['rot'])\n",
    "\n",
    "    # Step 5: Normalize 'cog' and 'heading'\n",
    "    df['cog'] = (df['cog'] / 180) - 1\n",
    "    df['heading'] = (df['heading'] / 180) - 1\n",
    "\n",
    "    # Step 6: Remove all moored vessels\n",
    "    \"\"\" \n",
    "    Moored vessels give little to no information about movement, and still vessel and be inferred by the model\n",
    "     due to time elapsed. Hence, we remove all moored instance in light of having more informative data \n",
    "     \"\"\"\n",
    "    df = df[df['navstat'] != 5]\n",
    "\n",
    "    # Step 7: Remove all rows with Nan values\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "baseDataset = preprocess_ais_train(baseDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 892056 entries, 0 to 1522064\n",
      "Data columns (total 10 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   time       892056 non-null  datetime64[ns]\n",
      " 1   cog        892056 non-null  float64       \n",
      " 2   sog        892056 non-null  float64       \n",
      " 3   rot        892056 non-null  float64       \n",
      " 4   heading    892056 non-null  float64       \n",
      " 5   navstat    892056 non-null  int64         \n",
      " 6   latitude   892056 non-null  float64       \n",
      " 7   longitude  892056 non-null  float64       \n",
      " 8   vesselId   892056 non-null  object        \n",
      " 9   portId     892056 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(6), int64(1), object(2)\n",
      "memory usage: 74.9+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    vesselId  record_count  in_test_set\n",
      "0   61e9f3cbb937134a3c4bff09             1        False\n",
      "1   61e9f43fb937134a3c4c016f            77        False\n",
      "2   61e9f39ab937134a3c4bfdb9           138        False\n",
      "3   61e9f45eb937134a3c4c0235           140        False\n",
      "4   61e9f45cb937134a3c4c022b           155        False\n",
      "5   61e9f47ab937134a3c4c02f7           169        False\n",
      "6   61e9f42cb937134a3c4c00f9           172        False\n",
      "7   61e9f3afb937134a3c4bfe47           219        False\n",
      "8   61e9f3bcb937134a3c4bfe91           220         True\n",
      "9   61e9f456b937134a3c4c0203           223        False\n",
      "10  61e9f408b937134a3c4c0023           229        False\n",
      "11  61e9f418b937134a3c4c0077           239        False\n",
      "12  61e9f460b937134a3c4c0243           246        False\n",
      "13  61e9f3aeb937134a3c4bfe45           252        False\n",
      "14  61e9f3c3b937134a3c4bfeb7           264        False\n",
      "15  61e9f3bab937134a3c4bfe8b           267        False\n",
      "16  61e9f42cb937134a3c4c00fb           292        False\n",
      "17  61e9f402b937134a3c4c000f           293        False\n",
      "18  61e9f3f7b937134a3c4bffc5           297        False\n",
      "19  61e9f409b937134a3c4c0027           302        False\n"
     ]
    }
   ],
   "source": [
    "# Get the unique vesselIds from the test set\n",
    "vessel_ids_test = set(testset['vesselId'].unique())\n",
    "\n",
    "# Get the count of records per vesselId in the training set\n",
    "vessel_record_counts = baseDataset['vesselId'].value_counts()\n",
    "\n",
    "# Get the 10 vessels with the lowest number of records\n",
    "lowest_record_vessels = vessel_record_counts.nsmallest(20)\n",
    "\n",
    "# Check if these vessels are in the test set\n",
    "vessels_in_test = lowest_record_vessels.index.isin(vessel_ids_test)\n",
    "\n",
    "# Combine the results into a dataframe for easy viewing\n",
    "vessels_with_low_records = pd.DataFrame({\n",
    "    'vesselId': lowest_record_vessels.index,\n",
    "    'record_count': lowest_record_vessels.values,\n",
    "    'in_test_set': vessels_in_test\n",
    "})\n",
    "\n",
    "# Display the result\n",
    "print(vessels_with_low_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of vessel IDs to remove\n",
    "vessels_to_remove = ['61e9f3cbb937134a3c4bff09', '61e9f3adb937134a3c4bfe37']\n",
    "\n",
    "# Remove vessels from the dataset\n",
    "baseDataset = baseDataset[~baseDataset['vesselId'].isin(vessels_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_intervals(data, min_time_diff_minutes= 8):\n",
    "\n",
    "    # Ensure time column is in datetime format\n",
    "    data['time'] = pd.to_datetime(data['time'])\n",
    "    \n",
    "    # Sort by vesselId and time to ensure proper order\n",
    "    data = data.sort_values(['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "    # Calculate time differences in minutes\n",
    "    data['time_diff'] = data.groupby('vesselId')['time'].diff().dt.total_seconds() / 60  # in minutes\n",
    "\n",
    "    # Filter out records with time differences less than the specified threshold\n",
    "    filtered_data = data[(data['time_diff'].isna()) | (data['time_diff'] >= min_time_diff_minutes)].copy()\n",
    "\n",
    "    # Drop the time_diff column after filtering\n",
    "    filtered_data = filtered_data.drop(columns=['time_diff']).reset_index(drop=True)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "baseDataset = filter_short_intervals(baseDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Haversine formula to calculate the great-circle distance between two points\n",
    "    R = 3440.065  # Radius of Earth in nautical miles\n",
    "    lat1_rad = np.radians(lat1.astype(float))\n",
    "    lon1_rad = np.radians(lon1.astype(float))\n",
    "    lat2_rad = np.radians(lat2.astype(float))\n",
    "    lon2_rad = np.radians(lon2.astype(float))\n",
    "    \n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "def heuristic_clean(data, vthreshold=30):\n",
    "    \"\"\"\n",
    "    Cleans the trajectory data by removing points with unrealistic speeds.\n",
    "    \n",
    "    :param data: DataFrame containing raw trajectory data\n",
    "    :param vthreshold: Speed threshold in knots\n",
    "    :return: Cleaned trajectory DataFrame\n",
    "    \"\"\"\n",
    "    # Drop duplicates and reset index\n",
    "    data = data.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Sort by vessel ID and time to ensure correct order\n",
    "    data = data.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "    \n",
    "    # Shift the columns to align with the previous record within each vessel\n",
    "    data['prev_latitude'] = data.groupby('vesselId')['latitude'].shift(1)\n",
    "    data['prev_longitude'] = data.groupby('vesselId')['longitude'].shift(1)\n",
    "    data['prev_time'] = data.groupby('vesselId')['time'].shift(1)\n",
    "    \n",
    "    # Calculate time differences in hours\n",
    "    data['delta_time'] = (data['time'] - data['prev_time']).dt.total_seconds() / 3600.0  # in hours\n",
    "    \n",
    "    # Calculate distances using the haversine function where we have valid previous points\n",
    "    valid_rows = data['prev_latitude'].notna() & data['prev_longitude'].notna()\n",
    "    data.loc[valid_rows, 'delta_distance'] = haversine(\n",
    "        data.loc[valid_rows, 'prev_latitude'].to_numpy(),\n",
    "        data.loc[valid_rows, 'prev_longitude'].to_numpy(),\n",
    "        data.loc[valid_rows, 'latitude'].to_numpy(),\n",
    "        data.loc[valid_rows, 'longitude'].to_numpy()\n",
    "    )\n",
    "    \n",
    "    # Calculate speed (distance/time) and filter by threshold\n",
    "    data['speed'] = data['delta_distance'] / data['delta_time']\n",
    "    \n",
    "    # Filter out rows where speed exceeds the threshold\n",
    "    cleaned_data = data[(data['speed'] <= vthreshold) | data['speed'].isna()].copy()\n",
    "    \n",
    "    # Drop intermediate calculation columns\n",
    "    cleaned_data = cleaned_data.drop(columns=['prev_latitude', 'prev_longitude', 'prev_time', \n",
    "                                              'delta_time', 'delta_distance', 'speed'])\n",
    "    \n",
    "    return cleaned_data.reset_index(drop=True)\n",
    "\n",
    "baseDataset = heuristic_clean(baseDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kanskje legge til etterhvert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef calculate_movement_deltas(df):\\n\\n    # Calculate time difference in seconds between consecutive points for each vessel\\n    df = df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\\n    df['time_diff'] = df.groupby('vesselId')['time'].diff().dt.total_seconds()\\n    \\n    # Convert cog to radians for trigonometric calculations\\n    df['cog_rad'] = np.radians(df['cog'])\\n\\n    # Calculate delta_lat and delta_lon based on COG, ROT, and time difference\\n    df['delta_lat'] = np.sin(df['cog_rad']) * df['time_diff'] * df['sog']\\n    df['delta_lon'] = np.cos(df['cog_rad']) * df['time_diff'] * df['sog']\\n\\n    # Fill NaN values in delta_lat and delta_lon with 0\\n    df['delta_lat'].fillna(0, inplace=True)\\n    df['delta_lon'].fillna(0, inplace=True)\\n\\n    # Drop the unnecessary columns\\n    df.drop(columns=['cog_rad', 'time_diff'], inplace=True)\\n\\n    return df\\n\\nbaseDataset = calculate_movement_deltas(baseDataset)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def calculate_movement_deltas(df):\n",
    "\n",
    "    # Calculate time difference in seconds between consecutive points for each vessel\n",
    "    df = df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "    df['time_diff'] = df.groupby('vesselId')['time'].diff().dt.total_seconds()\n",
    "    \n",
    "    # Convert cog to radians for trigonometric calculations\n",
    "    df['cog_rad'] = np.radians(df['cog'])\n",
    "\n",
    "    # Calculate delta_lat and delta_lon based on COG, ROT, and time difference\n",
    "    df['delta_lat'] = np.sin(df['cog_rad']) * df['time_diff'] * df['sog']\n",
    "    df['delta_lon'] = np.cos(df['cog_rad']) * df['time_diff'] * df['sog']\n",
    "\n",
    "    # Fill NaN values in delta_lat and delta_lon with 0\n",
    "    df['delta_lat'].fillna(0, inplace=True)\n",
    "    df['delta_lon'].fillna(0, inplace=True)\n",
    "\n",
    "    # Drop the unnecessary columns\n",
    "    df.drop(columns=['cog_rad', 'time_diff'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "baseDataset = calculate_movement_deltas(baseDataset)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Employ Port based feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = 'ports.csv'\n",
    "\n",
    "#Load the file into a pandas dataframe\n",
    "ports = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "def preprocess_ports_df(ports_df):\n",
    "    # Renaming the latitude and longitude columns in ports_df to portLatitude and portLongitude\n",
    "    ports_df = ports_df.rename(columns={'latitude': 'portLatitude', 'longitude': 'portLongitude'})\n",
    "    # List of columns to keep\n",
    "    columns_to_keep = ['portId', 'portLatitude', 'portLongitude']\n",
    "\n",
    "    # Drop all other columns except the ones specified in columns_to_keep\n",
    "    ports_df = ports_df[columns_to_keep]\n",
    "    return ports_df\n",
    "\n",
    "ports = preprocess_ports_df(ports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_close_to_port(df, ports, distance_threshold_nm=3, \n",
    "                       vessel_lat_col='latitude', vessel_lon_col='longitude', \n",
    "                       port_lat_col='portLatitude', port_lon_col='portLongitude'):\n",
    "    \"\"\"\n",
    "    Determines if each vessel in df is close to a port within a specified distance (in nautical miles).\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): DataFrame containing vessel data with specified latitude and longitude columns.\n",
    "    - ports (DataFrame): DataFrame containing port data with specified latitude and longitude columns.\n",
    "    - distance_threshold_nm (float): Distance threshold in nautical miles for identifying proximity to ports.\n",
    "    - vessel_lat_col (str): Column name for the vessel latitude in df.\n",
    "    - vessel_lon_col (str): Column name for the vessel longitude in df.\n",
    "    - port_lat_col (str): Column name for the port latitude in ports.\n",
    "    - port_lon_col (str): Column name for the port longitude in ports.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Updated df with a new binary column 'close_to_port', where 1 indicates proximity to a port within the threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert latitude and longitude to radians\n",
    "    df['lat_rad'] = np.radians(df[vessel_lat_col])\n",
    "    df['lon_rad'] = np.radians(df[vessel_lon_col])\n",
    "    ports['lat_rad'] = np.radians(ports[port_lat_col])\n",
    "    ports['lon_rad'] = np.radians(ports[port_lon_col])\n",
    "\n",
    "    # Build a k-d tree using port coordinates\n",
    "    port_coords = np.vstack((ports['lat_rad'], ports['lon_rad'])).T\n",
    "    port_tree = KDTree(port_coords, metric='euclidean')  # Using 'euclidean' since we pre-convert to radians\n",
    "\n",
    "    # Convert nautical miles to radians (1 nautical mile ≈ 1/3437.75 radians)\n",
    "    distance_threshold_radians = distance_threshold_nm / 3437.75\n",
    "\n",
    "    # Query each vessel against the port k-d tree for the closest port within the threshold\n",
    "    vessel_coords = np.vstack((df['lat_rad'], df['lon_rad'])).T\n",
    "    distances, indices = port_tree.query(vessel_coords, k=1, return_distance=True)\n",
    "\n",
    "    # Update df with close_to_port information based on the distance threshold\n",
    "    df['close_to_port'] = (distances.flatten() < distance_threshold_radians).astype(int)\n",
    "\n",
    "    # Drop the temporary radian columns\n",
    "    df.drop(columns=['lat_rad', 'lon_rad'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "baseDataset = find_close_to_port(\n",
    "    df=baseDataset, \n",
    "    ports=ports, \n",
    "    distance_threshold_nm=3, \n",
    "    vessel_lat_col='latitude', \n",
    "    vessel_lon_col='longitude', \n",
    "    port_lat_col='portLatitude', \n",
    "    port_lon_col='portLongitude'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 871978 entries, 0 to 871977\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   time           871978 non-null  datetime64[ns]\n",
      " 1   cog            871978 non-null  float64       \n",
      " 2   sog            871978 non-null  float64       \n",
      " 3   rot            871978 non-null  float64       \n",
      " 4   heading        871978 non-null  float64       \n",
      " 5   navstat        871978 non-null  int64         \n",
      " 6   latitude       871978 non-null  float64       \n",
      " 7   longitude      871978 non-null  float64       \n",
      " 8   vesselId       871978 non-null  object        \n",
      " 9   portId         871978 non-null  object        \n",
      " 10  close_to_port  871978 non-null  int32         \n",
      "dtypes: datetime64[ns](1), float64(6), int32(1), int64(1), object(2)\n",
      "memory usage: 69.9+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We finalize the traning data and fill the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "vessels = baseDataset[\"vesselId\"].unique().tolist()\n",
    "le.fit(vessels)\n",
    "baseDataset[\"vesselId\"] = le.transform(baseDataset[\"vesselId\"])\n",
    "\n",
    "baseDataset = baseDataset.drop(columns=['navstat', 'portId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_test_data(training_filepath, test_filepath, ports_df, le, distance_threshold_nm=3):\n",
    "\n",
    "    # Load and process training data to get the latest known position of each vessel\n",
    "    ais_train = pd.read_csv(training_filepath, sep='|')\n",
    "    ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "    ais_train = ais_train.sort_values(by='time')\n",
    "    final_positions = ais_train.groupby(\"vesselId\").last()\n",
    "\n",
    "    # Rename columns in final_positions for feature engineering in the test data\n",
    "    final_positions = final_positions.rename(columns={\n",
    "        'time': 'time_previous',\n",
    "        'latitude': 'latitude_previous',\n",
    "        'longitude': 'longitude_previous',\n",
    "        'sog': 'sog_previous',\n",
    "        'cog': 'cog_previous',\n",
    "        'rot': 'rot_previous',\n",
    "        'heading': 'heading_previous'\n",
    "    })\n",
    "\n",
    "    # Load test data and merge with latest known positions\n",
    "    ais_test = pd.read_csv(test_filepath)\n",
    "    ais_test['time'] = pd.to_datetime(ais_test['time'])\n",
    "    ais_test= ais_test.merge(final_positions, on=\"vesselId\", how=\"left\")\n",
    "\n",
    "    # Normalize course and heading\n",
    "    ais_test['cog_previous'] = (ais_test['cog_previous'] / 180) - 1\n",
    "    ais_test['cog_previous'] = (ais_test['cog_previous'] / 180) - 1\n",
    "\n",
    "    # Encode vessel IDs using the provided LabelEncoder\n",
    "    ais_test['vesselId'] = le.transform(ais_test['vesselId'])\n",
    "\n",
    "    # Calculate time difference in seconds between the test time and last known time\n",
    "    ais_test['time_gap'] = (ais_test['time'] - ais_test['time_previous']).dt.total_seconds()\n",
    "\n",
    "    # Determine proximity to ports by using the find_close_to_port function\n",
    "    testset = find_close_to_port(\n",
    "        df=ais_test,\n",
    "        ports=ports_df,\n",
    "        distance_threshold_nm=distance_threshold_nm,\n",
    "        vessel_lat_col='latitude_previous',\n",
    "        vessel_lon_col='longitude_previous',\n",
    "        port_lat_col='portLatitude',\n",
    "        port_lon_col='portLongitude'\n",
    "    )\n",
    "\n",
    "    testset = testset.drop(columns=['scaling_factor', 'time_previous', 'navstat', 'etaRaw', 'portId'])\n",
    "\n",
    "    return testset\n",
    "\n",
    "\n",
    "test_filled = fill_test_data(\n",
    "    training_filepath='ais_train.csv',\n",
    "    test_filepath='ais_test.csv',\n",
    "    ports_df=ports,\n",
    "    le=le,\n",
    "    distance_threshold_nm=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only vital information for dataset we want to create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We create the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagged_training_data(df, lag_steps):\n",
    "    \"\"\"\n",
    "    Creates a training set with features that include previous values for latitude, longitude,\n",
    "    speed over ground (sog), and time-related features like hour and day of the week.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing the AIS data.\n",
    "    - lag_steps (int): The number of time steps to shift for creating lagged features.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A new DataFrame with added lagged features and time-related information.\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame to work on\n",
    "    data = df.copy()\n",
    "\n",
    "    # Generate lagged features for latitude, longitude, and speed over ground (sog)\n",
    "    data['latitude_previous'] = data.groupby('vesselId')['latitude'].shift(lag_steps)\n",
    "    data['longitude_previous'] = data.groupby('vesselId')['longitude'].shift(lag_steps)\n",
    "    data['sog_previous'] = data.groupby('vesselId')['sog'].shift(lag_steps)\n",
    "    #data['delta_lat_previous'] = data.groupby('vesselId')['delta_lat'].shift(lag_steps)\n",
    "    #data['delta_lon_previous'] = data.groupby('vesselId')['delta_lon'].shift(lag_steps)\n",
    "    data['cog_previous'] = data.groupby('vesselId')['cog'].shift(lag_steps)\n",
    "    data['heading_previous'] = data.groupby('vesselId')['heading'].shift(lag_steps)\n",
    "    data['rot_previous'] = data.groupby('vesselId')['rot'].shift(lag_steps)\n",
    "    \n",
    "\n",
    "    # Calculate time difference between consecutive points in seconds\n",
    "    data['temp_time_gap'] = data.groupby('vesselId')['time'].diff(lag_steps)\n",
    "    data['time_gap'] = data['temp_time_gap'].dt.total_seconds()\n",
    "\n",
    "    # Drop any rows with missing values due to shifting or time differences\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    data.drop(columns=['temp_time_gap'], inplace=True)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We make a function where we can specify amount of lags! For example, if we want to train multiple models based on different lags, as we want each model to capture different time horizons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, each vessel should apprxoimately have 240 instances for all the 5 days, i.e. we should train on 240 lags to capture all possibilities, however this reduces amount of data for the first lags (which are more important), thus we could also specify 50 lags, for example to train on the first day, nd get more data for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainingset(df, max_size=12000000, total_lags=240):\n",
    "    \"\"\"\n",
    "    Creates a training dataset by sampling data from specified lags, with sample size per lag \n",
    "    dynamically adjusted based on the total number of lags.\n",
    "\n",
    "    Parameters:\n",
    "    - make_training_set_func (function): Function that generates the training set for a given lag.\n",
    "    - max_size (int): Maximum number of total samples to accumulate across all lags.\n",
    "    - total_lags (int): Total number of lag steps to include in the training dataset.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Concatenated training dataset containing sampled instances across specified lags.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    total_size = 0\n",
    "    lag = 1\n",
    "    \n",
    "    # Dynamically calculate max_samples_per_lag based on total_lags and max_size\n",
    "    max_samples_per_lag = max_size // total_lags\n",
    "\n",
    "    while lag <= total_lags and total_size < max_size:\n",
    "        # Generate dataset for the current lag\n",
    "        current_set = lagged_training_data(df, lag)\n",
    "        \n",
    "        # If the dataset is empty for the current lag, break the loop\n",
    "        if len(current_set) == 0:\n",
    "            break\n",
    "\n",
    "        # Sample the dataset with the dynamically calculated sample size for this lag\n",
    "        sampled_set = current_set.sample(min(max_samples_per_lag, len(current_set)), random_state=42)\n",
    "        datasets.append(sampled_set)\n",
    "        total_size += len(sampled_set)\n",
    "\n",
    "        print(f\"Size after lag {lag}: {total_size:_} samples accumulated with {len(sampled_set)} samples from lag {lag}\")\n",
    "\n",
    "        # Move to the next lag step\n",
    "        lag += 1\n",
    "\n",
    "    # Concatenate all sampled datasets into a single training set\n",
    "    training_data = pd.concat(datasets, ignore_index=True)\n",
    "    print(\"Total length of training data:\", len(training_data))\n",
    "\n",
    "    # Clear out memory by deleting intermediate datasets list\n",
    "    del datasets\n",
    "\n",
    "    training_data = training_data.drop(columns=['time', 'sog', 'cog', 'heading', 'rot'])\n",
    "\n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after lag 1: 41_666 samples accumulated with 41666 samples from lag 1\n",
      "Size after lag 2: 83_332 samples accumulated with 41666 samples from lag 2\n",
      "Size after lag 3: 124_998 samples accumulated with 41666 samples from lag 3\n",
      "Size after lag 4: 166_664 samples accumulated with 41666 samples from lag 4\n",
      "Size after lag 5: 208_330 samples accumulated with 41666 samples from lag 5\n",
      "Size after lag 6: 249_996 samples accumulated with 41666 samples from lag 6\n",
      "Size after lag 7: 291_662 samples accumulated with 41666 samples from lag 7\n",
      "Size after lag 8: 333_328 samples accumulated with 41666 samples from lag 8\n",
      "Size after lag 9: 374_994 samples accumulated with 41666 samples from lag 9\n",
      "Size after lag 10: 416_660 samples accumulated with 41666 samples from lag 10\n",
      "Size after lag 11: 458_326 samples accumulated with 41666 samples from lag 11\n",
      "Size after lag 12: 499_992 samples accumulated with 41666 samples from lag 12\n",
      "Size after lag 13: 541_658 samples accumulated with 41666 samples from lag 13\n",
      "Size after lag 14: 583_324 samples accumulated with 41666 samples from lag 14\n",
      "Size after lag 15: 624_990 samples accumulated with 41666 samples from lag 15\n",
      "Size after lag 16: 666_656 samples accumulated with 41666 samples from lag 16\n",
      "Size after lag 17: 708_322 samples accumulated with 41666 samples from lag 17\n",
      "Size after lag 18: 749_988 samples accumulated with 41666 samples from lag 18\n",
      "Size after lag 19: 791_654 samples accumulated with 41666 samples from lag 19\n",
      "Size after lag 20: 833_320 samples accumulated with 41666 samples from lag 20\n",
      "Size after lag 21: 874_986 samples accumulated with 41666 samples from lag 21\n",
      "Size after lag 22: 916_652 samples accumulated with 41666 samples from lag 22\n",
      "Size after lag 23: 958_318 samples accumulated with 41666 samples from lag 23\n",
      "Size after lag 24: 999_984 samples accumulated with 41666 samples from lag 24\n",
      "Size after lag 25: 1_041_650 samples accumulated with 41666 samples from lag 25\n",
      "Size after lag 26: 1_083_316 samples accumulated with 41666 samples from lag 26\n",
      "Size after lag 27: 1_124_982 samples accumulated with 41666 samples from lag 27\n",
      "Size after lag 28: 1_166_648 samples accumulated with 41666 samples from lag 28\n",
      "Size after lag 29: 1_208_314 samples accumulated with 41666 samples from lag 29\n",
      "Size after lag 30: 1_249_980 samples accumulated with 41666 samples from lag 30\n",
      "Size after lag 31: 1_291_646 samples accumulated with 41666 samples from lag 31\n",
      "Size after lag 32: 1_333_312 samples accumulated with 41666 samples from lag 32\n",
      "Size after lag 33: 1_374_978 samples accumulated with 41666 samples from lag 33\n",
      "Size after lag 34: 1_416_644 samples accumulated with 41666 samples from lag 34\n",
      "Size after lag 35: 1_458_310 samples accumulated with 41666 samples from lag 35\n",
      "Size after lag 36: 1_499_976 samples accumulated with 41666 samples from lag 36\n",
      "Size after lag 37: 1_541_642 samples accumulated with 41666 samples from lag 37\n",
      "Size after lag 38: 1_583_308 samples accumulated with 41666 samples from lag 38\n",
      "Size after lag 39: 1_624_974 samples accumulated with 41666 samples from lag 39\n",
      "Size after lag 40: 1_666_640 samples accumulated with 41666 samples from lag 40\n",
      "Size after lag 41: 1_708_306 samples accumulated with 41666 samples from lag 41\n",
      "Size after lag 42: 1_749_972 samples accumulated with 41666 samples from lag 42\n",
      "Size after lag 43: 1_791_638 samples accumulated with 41666 samples from lag 43\n",
      "Size after lag 44: 1_833_304 samples accumulated with 41666 samples from lag 44\n",
      "Size after lag 45: 1_874_970 samples accumulated with 41666 samples from lag 45\n",
      "Size after lag 46: 1_916_636 samples accumulated with 41666 samples from lag 46\n",
      "Size after lag 47: 1_958_302 samples accumulated with 41666 samples from lag 47\n",
      "Size after lag 48: 1_999_968 samples accumulated with 41666 samples from lag 48\n",
      "Size after lag 49: 2_041_634 samples accumulated with 41666 samples from lag 49\n",
      "Size after lag 50: 2_083_300 samples accumulated with 41666 samples from lag 50\n",
      "Size after lag 51: 2_124_966 samples accumulated with 41666 samples from lag 51\n",
      "Size after lag 52: 2_166_632 samples accumulated with 41666 samples from lag 52\n",
      "Size after lag 53: 2_208_298 samples accumulated with 41666 samples from lag 53\n",
      "Size after lag 54: 2_249_964 samples accumulated with 41666 samples from lag 54\n",
      "Size after lag 55: 2_291_630 samples accumulated with 41666 samples from lag 55\n",
      "Size after lag 56: 2_333_296 samples accumulated with 41666 samples from lag 56\n",
      "Size after lag 57: 2_374_962 samples accumulated with 41666 samples from lag 57\n",
      "Size after lag 58: 2_416_628 samples accumulated with 41666 samples from lag 58\n",
      "Size after lag 59: 2_458_294 samples accumulated with 41666 samples from lag 59\n",
      "Size after lag 60: 2_499_960 samples accumulated with 41666 samples from lag 60\n",
      "Size after lag 61: 2_541_626 samples accumulated with 41666 samples from lag 61\n",
      "Size after lag 62: 2_583_292 samples accumulated with 41666 samples from lag 62\n",
      "Size after lag 63: 2_624_958 samples accumulated with 41666 samples from lag 63\n",
      "Size after lag 64: 2_666_624 samples accumulated with 41666 samples from lag 64\n",
      "Size after lag 65: 2_708_290 samples accumulated with 41666 samples from lag 65\n",
      "Size after lag 66: 2_749_956 samples accumulated with 41666 samples from lag 66\n",
      "Size after lag 67: 2_791_622 samples accumulated with 41666 samples from lag 67\n",
      "Size after lag 68: 2_833_288 samples accumulated with 41666 samples from lag 68\n",
      "Size after lag 69: 2_874_954 samples accumulated with 41666 samples from lag 69\n",
      "Size after lag 70: 2_916_620 samples accumulated with 41666 samples from lag 70\n",
      "Size after lag 71: 2_958_286 samples accumulated with 41666 samples from lag 71\n",
      "Size after lag 72: 2_999_952 samples accumulated with 41666 samples from lag 72\n",
      "Size after lag 73: 3_041_618 samples accumulated with 41666 samples from lag 73\n",
      "Size after lag 74: 3_083_284 samples accumulated with 41666 samples from lag 74\n",
      "Size after lag 75: 3_124_950 samples accumulated with 41666 samples from lag 75\n",
      "Size after lag 76: 3_166_616 samples accumulated with 41666 samples from lag 76\n",
      "Size after lag 77: 3_208_282 samples accumulated with 41666 samples from lag 77\n",
      "Size after lag 78: 3_249_948 samples accumulated with 41666 samples from lag 78\n",
      "Size after lag 79: 3_291_614 samples accumulated with 41666 samples from lag 79\n",
      "Size after lag 80: 3_333_280 samples accumulated with 41666 samples from lag 80\n",
      "Size after lag 81: 3_374_946 samples accumulated with 41666 samples from lag 81\n",
      "Size after lag 82: 3_416_612 samples accumulated with 41666 samples from lag 82\n",
      "Size after lag 83: 3_458_278 samples accumulated with 41666 samples from lag 83\n",
      "Size after lag 84: 3_499_944 samples accumulated with 41666 samples from lag 84\n",
      "Size after lag 85: 3_541_610 samples accumulated with 41666 samples from lag 85\n",
      "Size after lag 86: 3_583_276 samples accumulated with 41666 samples from lag 86\n",
      "Size after lag 87: 3_624_942 samples accumulated with 41666 samples from lag 87\n",
      "Size after lag 88: 3_666_608 samples accumulated with 41666 samples from lag 88\n",
      "Size after lag 89: 3_708_274 samples accumulated with 41666 samples from lag 89\n",
      "Size after lag 90: 3_749_940 samples accumulated with 41666 samples from lag 90\n",
      "Size after lag 91: 3_791_606 samples accumulated with 41666 samples from lag 91\n",
      "Size after lag 92: 3_833_272 samples accumulated with 41666 samples from lag 92\n",
      "Size after lag 93: 3_874_938 samples accumulated with 41666 samples from lag 93\n",
      "Size after lag 94: 3_916_604 samples accumulated with 41666 samples from lag 94\n",
      "Size after lag 95: 3_958_270 samples accumulated with 41666 samples from lag 95\n",
      "Size after lag 96: 3_999_936 samples accumulated with 41666 samples from lag 96\n",
      "Size after lag 97: 4_041_602 samples accumulated with 41666 samples from lag 97\n",
      "Size after lag 98: 4_083_268 samples accumulated with 41666 samples from lag 98\n",
      "Size after lag 99: 4_124_934 samples accumulated with 41666 samples from lag 99\n",
      "Size after lag 100: 4_166_600 samples accumulated with 41666 samples from lag 100\n",
      "Size after lag 101: 4_208_266 samples accumulated with 41666 samples from lag 101\n",
      "Size after lag 102: 4_249_932 samples accumulated with 41666 samples from lag 102\n",
      "Size after lag 103: 4_291_598 samples accumulated with 41666 samples from lag 103\n",
      "Size after lag 104: 4_333_264 samples accumulated with 41666 samples from lag 104\n",
      "Size after lag 105: 4_374_930 samples accumulated with 41666 samples from lag 105\n",
      "Size after lag 106: 4_416_596 samples accumulated with 41666 samples from lag 106\n",
      "Size after lag 107: 4_458_262 samples accumulated with 41666 samples from lag 107\n",
      "Size after lag 108: 4_499_928 samples accumulated with 41666 samples from lag 108\n",
      "Size after lag 109: 4_541_594 samples accumulated with 41666 samples from lag 109\n",
      "Size after lag 110: 4_583_260 samples accumulated with 41666 samples from lag 110\n",
      "Size after lag 111: 4_624_926 samples accumulated with 41666 samples from lag 111\n",
      "Size after lag 112: 4_666_592 samples accumulated with 41666 samples from lag 112\n",
      "Size after lag 113: 4_708_258 samples accumulated with 41666 samples from lag 113\n",
      "Size after lag 114: 4_749_924 samples accumulated with 41666 samples from lag 114\n",
      "Size after lag 115: 4_791_590 samples accumulated with 41666 samples from lag 115\n",
      "Size after lag 116: 4_833_256 samples accumulated with 41666 samples from lag 116\n",
      "Size after lag 117: 4_874_922 samples accumulated with 41666 samples from lag 117\n",
      "Size after lag 118: 4_916_588 samples accumulated with 41666 samples from lag 118\n",
      "Size after lag 119: 4_958_254 samples accumulated with 41666 samples from lag 119\n",
      "Size after lag 120: 4_999_920 samples accumulated with 41666 samples from lag 120\n",
      "Size after lag 121: 5_041_586 samples accumulated with 41666 samples from lag 121\n",
      "Size after lag 122: 5_083_252 samples accumulated with 41666 samples from lag 122\n",
      "Size after lag 123: 5_124_918 samples accumulated with 41666 samples from lag 123\n",
      "Size after lag 124: 5_166_584 samples accumulated with 41666 samples from lag 124\n",
      "Size after lag 125: 5_208_250 samples accumulated with 41666 samples from lag 125\n",
      "Size after lag 126: 5_249_916 samples accumulated with 41666 samples from lag 126\n",
      "Size after lag 127: 5_291_582 samples accumulated with 41666 samples from lag 127\n",
      "Size after lag 128: 5_333_248 samples accumulated with 41666 samples from lag 128\n",
      "Size after lag 129: 5_374_914 samples accumulated with 41666 samples from lag 129\n",
      "Size after lag 130: 5_416_580 samples accumulated with 41666 samples from lag 130\n",
      "Size after lag 131: 5_458_246 samples accumulated with 41666 samples from lag 131\n",
      "Size after lag 132: 5_499_912 samples accumulated with 41666 samples from lag 132\n",
      "Size after lag 133: 5_541_578 samples accumulated with 41666 samples from lag 133\n",
      "Size after lag 134: 5_583_244 samples accumulated with 41666 samples from lag 134\n",
      "Size after lag 135: 5_624_910 samples accumulated with 41666 samples from lag 135\n",
      "Size after lag 136: 5_666_576 samples accumulated with 41666 samples from lag 136\n",
      "Size after lag 137: 5_708_242 samples accumulated with 41666 samples from lag 137\n",
      "Size after lag 138: 5_749_908 samples accumulated with 41666 samples from lag 138\n",
      "Size after lag 139: 5_791_574 samples accumulated with 41666 samples from lag 139\n",
      "Size after lag 140: 5_833_240 samples accumulated with 41666 samples from lag 140\n",
      "Size after lag 141: 5_874_906 samples accumulated with 41666 samples from lag 141\n",
      "Size after lag 142: 5_916_572 samples accumulated with 41666 samples from lag 142\n",
      "Size after lag 143: 5_958_238 samples accumulated with 41666 samples from lag 143\n",
      "Size after lag 144: 5_999_904 samples accumulated with 41666 samples from lag 144\n",
      "Size after lag 145: 6_041_570 samples accumulated with 41666 samples from lag 145\n",
      "Size after lag 146: 6_083_236 samples accumulated with 41666 samples from lag 146\n",
      "Size after lag 147: 6_124_902 samples accumulated with 41666 samples from lag 147\n",
      "Size after lag 148: 6_166_568 samples accumulated with 41666 samples from lag 148\n",
      "Size after lag 149: 6_208_234 samples accumulated with 41666 samples from lag 149\n",
      "Size after lag 150: 6_249_900 samples accumulated with 41666 samples from lag 150\n",
      "Size after lag 151: 6_291_566 samples accumulated with 41666 samples from lag 151\n",
      "Size after lag 152: 6_333_232 samples accumulated with 41666 samples from lag 152\n",
      "Size after lag 153: 6_374_898 samples accumulated with 41666 samples from lag 153\n",
      "Size after lag 154: 6_416_564 samples accumulated with 41666 samples from lag 154\n",
      "Size after lag 155: 6_458_230 samples accumulated with 41666 samples from lag 155\n",
      "Size after lag 156: 6_499_896 samples accumulated with 41666 samples from lag 156\n",
      "Size after lag 157: 6_541_562 samples accumulated with 41666 samples from lag 157\n",
      "Size after lag 158: 6_583_228 samples accumulated with 41666 samples from lag 158\n",
      "Size after lag 159: 6_624_894 samples accumulated with 41666 samples from lag 159\n",
      "Size after lag 160: 6_666_560 samples accumulated with 41666 samples from lag 160\n",
      "Size after lag 161: 6_708_226 samples accumulated with 41666 samples from lag 161\n",
      "Size after lag 162: 6_749_892 samples accumulated with 41666 samples from lag 162\n",
      "Size after lag 163: 6_791_558 samples accumulated with 41666 samples from lag 163\n",
      "Size after lag 164: 6_833_224 samples accumulated with 41666 samples from lag 164\n",
      "Size after lag 165: 6_874_890 samples accumulated with 41666 samples from lag 165\n",
      "Size after lag 166: 6_916_556 samples accumulated with 41666 samples from lag 166\n",
      "Size after lag 167: 6_958_222 samples accumulated with 41666 samples from lag 167\n",
      "Size after lag 168: 6_999_888 samples accumulated with 41666 samples from lag 168\n",
      "Size after lag 169: 7_041_554 samples accumulated with 41666 samples from lag 169\n",
      "Size after lag 170: 7_083_220 samples accumulated with 41666 samples from lag 170\n",
      "Size after lag 171: 7_124_886 samples accumulated with 41666 samples from lag 171\n",
      "Size after lag 172: 7_166_552 samples accumulated with 41666 samples from lag 172\n",
      "Size after lag 173: 7_208_218 samples accumulated with 41666 samples from lag 173\n",
      "Size after lag 174: 7_249_884 samples accumulated with 41666 samples from lag 174\n",
      "Size after lag 175: 7_291_550 samples accumulated with 41666 samples from lag 175\n",
      "Size after lag 176: 7_333_216 samples accumulated with 41666 samples from lag 176\n",
      "Size after lag 177: 7_374_882 samples accumulated with 41666 samples from lag 177\n",
      "Size after lag 178: 7_416_548 samples accumulated with 41666 samples from lag 178\n",
      "Size after lag 179: 7_458_214 samples accumulated with 41666 samples from lag 179\n",
      "Size after lag 180: 7_499_880 samples accumulated with 41666 samples from lag 180\n",
      "Size after lag 181: 7_541_546 samples accumulated with 41666 samples from lag 181\n",
      "Size after lag 182: 7_583_212 samples accumulated with 41666 samples from lag 182\n",
      "Size after lag 183: 7_624_878 samples accumulated with 41666 samples from lag 183\n",
      "Size after lag 184: 7_666_544 samples accumulated with 41666 samples from lag 184\n",
      "Size after lag 185: 7_708_210 samples accumulated with 41666 samples from lag 185\n",
      "Size after lag 186: 7_749_876 samples accumulated with 41666 samples from lag 186\n",
      "Size after lag 187: 7_791_542 samples accumulated with 41666 samples from lag 187\n",
      "Size after lag 188: 7_833_208 samples accumulated with 41666 samples from lag 188\n",
      "Size after lag 189: 7_874_874 samples accumulated with 41666 samples from lag 189\n",
      "Size after lag 190: 7_916_540 samples accumulated with 41666 samples from lag 190\n",
      "Size after lag 191: 7_958_206 samples accumulated with 41666 samples from lag 191\n",
      "Size after lag 192: 7_999_872 samples accumulated with 41666 samples from lag 192\n",
      "Size after lag 193: 8_041_538 samples accumulated with 41666 samples from lag 193\n",
      "Size after lag 194: 8_083_204 samples accumulated with 41666 samples from lag 194\n",
      "Size after lag 195: 8_124_870 samples accumulated with 41666 samples from lag 195\n",
      "Size after lag 196: 8_166_536 samples accumulated with 41666 samples from lag 196\n",
      "Size after lag 197: 8_208_202 samples accumulated with 41666 samples from lag 197\n",
      "Size after lag 198: 8_249_868 samples accumulated with 41666 samples from lag 198\n",
      "Size after lag 199: 8_291_534 samples accumulated with 41666 samples from lag 199\n",
      "Size after lag 200: 8_333_200 samples accumulated with 41666 samples from lag 200\n",
      "Size after lag 201: 8_374_866 samples accumulated with 41666 samples from lag 201\n",
      "Size after lag 202: 8_416_532 samples accumulated with 41666 samples from lag 202\n",
      "Size after lag 203: 8_458_198 samples accumulated with 41666 samples from lag 203\n",
      "Size after lag 204: 8_499_864 samples accumulated with 41666 samples from lag 204\n",
      "Size after lag 205: 8_541_530 samples accumulated with 41666 samples from lag 205\n",
      "Size after lag 206: 8_583_196 samples accumulated with 41666 samples from lag 206\n",
      "Size after lag 207: 8_624_862 samples accumulated with 41666 samples from lag 207\n",
      "Size after lag 208: 8_666_528 samples accumulated with 41666 samples from lag 208\n",
      "Size after lag 209: 8_708_194 samples accumulated with 41666 samples from lag 209\n",
      "Size after lag 210: 8_749_860 samples accumulated with 41666 samples from lag 210\n",
      "Size after lag 211: 8_791_526 samples accumulated with 41666 samples from lag 211\n",
      "Size after lag 212: 8_833_192 samples accumulated with 41666 samples from lag 212\n",
      "Size after lag 213: 8_874_858 samples accumulated with 41666 samples from lag 213\n",
      "Size after lag 214: 8_916_524 samples accumulated with 41666 samples from lag 214\n",
      "Size after lag 215: 8_958_190 samples accumulated with 41666 samples from lag 215\n",
      "Size after lag 216: 8_999_856 samples accumulated with 41666 samples from lag 216\n",
      "Size after lag 217: 9_041_522 samples accumulated with 41666 samples from lag 217\n",
      "Size after lag 218: 9_083_188 samples accumulated with 41666 samples from lag 218\n",
      "Size after lag 219: 9_124_854 samples accumulated with 41666 samples from lag 219\n",
      "Size after lag 220: 9_166_520 samples accumulated with 41666 samples from lag 220\n",
      "Size after lag 221: 9_208_186 samples accumulated with 41666 samples from lag 221\n",
      "Size after lag 222: 9_249_852 samples accumulated with 41666 samples from lag 222\n",
      "Size after lag 223: 9_291_518 samples accumulated with 41666 samples from lag 223\n",
      "Size after lag 224: 9_333_184 samples accumulated with 41666 samples from lag 224\n",
      "Size after lag 225: 9_374_850 samples accumulated with 41666 samples from lag 225\n",
      "Size after lag 226: 9_416_516 samples accumulated with 41666 samples from lag 226\n",
      "Size after lag 227: 9_458_182 samples accumulated with 41666 samples from lag 227\n",
      "Size after lag 228: 9_499_848 samples accumulated with 41666 samples from lag 228\n",
      "Size after lag 229: 9_541_514 samples accumulated with 41666 samples from lag 229\n",
      "Size after lag 230: 9_583_180 samples accumulated with 41666 samples from lag 230\n",
      "Size after lag 231: 9_624_846 samples accumulated with 41666 samples from lag 231\n",
      "Size after lag 232: 9_666_512 samples accumulated with 41666 samples from lag 232\n",
      "Size after lag 233: 9_708_178 samples accumulated with 41666 samples from lag 233\n",
      "Size after lag 234: 9_749_844 samples accumulated with 41666 samples from lag 234\n",
      "Size after lag 235: 9_791_510 samples accumulated with 41666 samples from lag 235\n",
      "Size after lag 236: 9_833_176 samples accumulated with 41666 samples from lag 236\n",
      "Size after lag 237: 9_874_842 samples accumulated with 41666 samples from lag 237\n",
      "Size after lag 238: 9_916_508 samples accumulated with 41666 samples from lag 238\n",
      "Size after lag 239: 9_958_174 samples accumulated with 41666 samples from lag 239\n",
      "Size after lag 240: 9_999_840 samples accumulated with 41666 samples from lag 240\n",
      "Total length of training data: 9999840\n"
     ]
    }
   ],
   "source": [
    "train_data = create_trainingset(baseDataset, max_size=12000000, total_lags=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Define features (X) and targets (y)\n",
    "X = train_data.drop(columns=['latitude', 'longitude'])\n",
    "y = train_data[['latitude', 'longitude']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseDataset: 53.22 MB\n",
      "testset: 8.54 MB\n",
      "vessels_with_low_records: 0.00 MB\n",
      "ports: 0.14 MB\n",
      "test_filled: 3.95 MB\n",
      "train_data: 762.93 MB\n",
      "X: 610.34 MB\n",
      "y: 152.59 MB\n"
     ]
    }
   ],
   "source": [
    "# Find all DataFrames in memory\n",
    "dataframes_in_memory = {name: obj for name, obj in globals().items() if isinstance(obj, pd.DataFrame)}\n",
    "\n",
    "# Print the names and memory usage of each DataFrame\n",
    "for name, df in dataframes_in_memory.items():\n",
    "    print(f\"{name}: {df.memory_usage(deep=True).sum() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "#del gdf_with_near_land\n",
    "del train_data\n",
    "del testset\n",
    "del baseDataset # Ikke dersom vi vil lage flere modeller for å stacke tid\n",
    "\n",
    "\n",
    "# Run garbage collection to free up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed on the scaled training dataset.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the MultiOutputRegressor with RandomForest\n",
    "# Initialize and train the MultiOutputRegressor with an optimized RandomForest\n",
    "model = MultiOutputRegressor(\n",
    "    RandomForestRegressor(\n",
    "        n_estimators=10,           # Adjusted number of trees for balance between performance and training time\n",
    "        n_jobs=6,                  # Use all CPU cores dynamically\n",
    "        random_state=42 ,            # For reproducibility\n",
    "        warm_start=False,\n",
    "        criterion='squared_error',\n",
    "        max_depth=30\n",
    "    )\n",
    ")\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Model training completed on the training dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test set for prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = test_filled['ID'].copy()\n",
    "test_filled = test_filled.drop(columns=['ID'])\n",
    "# Set testset columns in the same order as the training set X\n",
    "testset = test_filled[X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vesselId</th>\n",
       "      <th>close_to_port</th>\n",
       "      <th>latitude_previous</th>\n",
       "      <th>longitude_previous</th>\n",
       "      <th>sog_previous</th>\n",
       "      <th>cog_previous</th>\n",
       "      <th>heading_previous</th>\n",
       "      <th>rot_previous</th>\n",
       "      <th>time_gap</th>\n",
       "      <th>latitude_predicted</th>\n",
       "      <th>longitude_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>31.14647</td>\n",
       "      <td>-81.49789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000012</td>\n",
       "      <td>344</td>\n",
       "      <td>0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>30.652396</td>\n",
       "      <td>-81.517125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>619</td>\n",
       "      <td>1</td>\n",
       "      <td>14.81694</td>\n",
       "      <td>120.29625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.004793</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>14.576297</td>\n",
       "      <td>121.154367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>592</td>\n",
       "      <td>0</td>\n",
       "      <td>38.27895</td>\n",
       "      <td>10.78280</td>\n",
       "      <td>18.7</td>\n",
       "      <td>-1.005309</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>38.918529</td>\n",
       "      <td>10.919319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>538</td>\n",
       "      <td>0</td>\n",
       "      <td>-43.53785</td>\n",
       "      <td>172.83522</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.995639</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>-43.534144</td>\n",
       "      <td>172.835347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48.53320</td>\n",
       "      <td>-6.12003</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.996574</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>1258.0</td>\n",
       "      <td>48.527350</td>\n",
       "      <td>-7.662199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>682</td>\n",
       "      <td>1</td>\n",
       "      <td>51.35306</td>\n",
       "      <td>3.19241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.003981</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>1087.0</td>\n",
       "      <td>51.478282</td>\n",
       "      <td>3.556257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>43.34995</td>\n",
       "      <td>-8.50739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.994981</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>42.421574</td>\n",
       "      <td>-9.070712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>51.31666</td>\n",
       "      <td>3.22506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.005074</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>1082.0</td>\n",
       "      <td>51.322766</td>\n",
       "      <td>3.786315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>40.84712</td>\n",
       "      <td>29.29052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.995370</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>40.850883</td>\n",
       "      <td>29.302719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>174</td>\n",
       "      <td>1</td>\n",
       "      <td>38.47378</td>\n",
       "      <td>15.91582</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.999145</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>39.526332</td>\n",
       "      <td>15.900867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>38.96142</td>\n",
       "      <td>-12.00502</td>\n",
       "      <td>17.1</td>\n",
       "      <td>-1.005176</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>996.0</td>\n",
       "      <td>39.122383</td>\n",
       "      <td>-12.091806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>589</td>\n",
       "      <td>0</td>\n",
       "      <td>43.72483</td>\n",
       "      <td>-5.33660</td>\n",
       "      <td>19.7</td>\n",
       "      <td>-0.997006</td>\n",
       "      <td>277</td>\n",
       "      <td>-10</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>43.789091</td>\n",
       "      <td>-5.732661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>645</td>\n",
       "      <td>1</td>\n",
       "      <td>54.37278</td>\n",
       "      <td>18.65716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.999537</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>54.551351</td>\n",
       "      <td>18.501677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>51.27275</td>\n",
       "      <td>4.21398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.999682</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>51.275683</td>\n",
       "      <td>2.727242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>240</td>\n",
       "      <td>1</td>\n",
       "      <td>40.66726</td>\n",
       "      <td>-74.07082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.996407</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>1084.0</td>\n",
       "      <td>40.666121</td>\n",
       "      <td>-74.165400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>654</td>\n",
       "      <td>1</td>\n",
       "      <td>59.73643</td>\n",
       "      <td>10.23870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000025</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>59.154742</td>\n",
       "      <td>9.896255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>667</td>\n",
       "      <td>1</td>\n",
       "      <td>-27.37772</td>\n",
       "      <td>153.16645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000605</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>-28.852458</td>\n",
       "      <td>153.119492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>648</td>\n",
       "      <td>1</td>\n",
       "      <td>53.56397</td>\n",
       "      <td>8.56593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.995194</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>55.004874</td>\n",
       "      <td>7.726822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>51.79319</td>\n",
       "      <td>3.34467</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1.004222</td>\n",
       "      <td>311</td>\n",
       "      <td>0</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>51.933996</td>\n",
       "      <td>3.816442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>575</td>\n",
       "      <td>1</td>\n",
       "      <td>52.40704</td>\n",
       "      <td>4.86081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.004605</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>52.433181</td>\n",
       "      <td>4.570027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>658</td>\n",
       "      <td>0</td>\n",
       "      <td>53.57648</td>\n",
       "      <td>-0.09023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.005062</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>53.474702</td>\n",
       "      <td>0.413703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>378</td>\n",
       "      <td>0</td>\n",
       "      <td>51.26926</td>\n",
       "      <td>4.21159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.005269</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>51.271377</td>\n",
       "      <td>4.154781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>295</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17835</td>\n",
       "      <td>103.75237</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.996441</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1.178506</td>\n",
       "      <td>103.751517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>323</td>\n",
       "      <td>0</td>\n",
       "      <td>47.33692</td>\n",
       "      <td>-122.47557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.996528</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>47.336156</td>\n",
       "      <td>-122.474694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>266</td>\n",
       "      <td>1</td>\n",
       "      <td>50.90418</td>\n",
       "      <td>-1.42857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.996204</td>\n",
       "      <td>297</td>\n",
       "      <td>0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>50.831639</td>\n",
       "      <td>-1.346207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>511</td>\n",
       "      <td>1</td>\n",
       "      <td>51.30661</td>\n",
       "      <td>3.22715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000512</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>51.327538</td>\n",
       "      <td>3.196732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>587</td>\n",
       "      <td>0</td>\n",
       "      <td>24.92350</td>\n",
       "      <td>-109.76272</td>\n",
       "      <td>18.9</td>\n",
       "      <td>-1.004321</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>25.079058</td>\n",
       "      <td>-109.235118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>560</td>\n",
       "      <td>1</td>\n",
       "      <td>40.84103</td>\n",
       "      <td>14.27640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.004046</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1438.0</td>\n",
       "      <td>40.821491</td>\n",
       "      <td>14.113797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>572</td>\n",
       "      <td>1</td>\n",
       "      <td>53.94362</td>\n",
       "      <td>10.86065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.004997</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1081.0</td>\n",
       "      <td>53.946968</td>\n",
       "      <td>11.255399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>576</td>\n",
       "      <td>1</td>\n",
       "      <td>28.14658</td>\n",
       "      <td>-15.41917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.999769</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>28.146577</td>\n",
       "      <td>-15.418708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>34.70981</td>\n",
       "      <td>134.84946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.002090</td>\n",
       "      <td>292</td>\n",
       "      <td>0</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>34.695487</td>\n",
       "      <td>134.884713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>393</td>\n",
       "      <td>1</td>\n",
       "      <td>54.40609</td>\n",
       "      <td>18.65503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.996012</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>54.701313</td>\n",
       "      <td>18.672605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>51.26723</td>\n",
       "      <td>4.21002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.999568</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>51.272254</td>\n",
       "      <td>1.833956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>37.53028</td>\n",
       "      <td>16.62312</td>\n",
       "      <td>16.4</td>\n",
       "      <td>-0.996485</td>\n",
       "      <td>293</td>\n",
       "      <td>0</td>\n",
       "      <td>1849.0</td>\n",
       "      <td>37.574188</td>\n",
       "      <td>16.413145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>571</td>\n",
       "      <td>1</td>\n",
       "      <td>53.94312</td>\n",
       "      <td>10.86149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.004951</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>54.340057</td>\n",
       "      <td>11.214245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>56.15072</td>\n",
       "      <td>10.23467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000972</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>56.528206</td>\n",
       "      <td>10.145746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>245</td>\n",
       "      <td>1</td>\n",
       "      <td>53.57480</td>\n",
       "      <td>8.55801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.002636</td>\n",
       "      <td>242</td>\n",
       "      <td>0</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>52.526453</td>\n",
       "      <td>7.472609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>6.03446</td>\n",
       "      <td>116.08002</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.994660</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>6.034309</td>\n",
       "      <td>116.017640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>271</td>\n",
       "      <td>1</td>\n",
       "      <td>53.57839</td>\n",
       "      <td>8.55239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.994765</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1263.0</td>\n",
       "      <td>53.126795</td>\n",
       "      <td>6.557346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>397</td>\n",
       "      <td>0</td>\n",
       "      <td>53.58649</td>\n",
       "      <td>-0.06694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.997034</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>1259.0</td>\n",
       "      <td>53.304346</td>\n",
       "      <td>0.482964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>513</td>\n",
       "      <td>1</td>\n",
       "      <td>43.34959</td>\n",
       "      <td>5.33275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.004787</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>43.389472</td>\n",
       "      <td>5.709414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "      <td>18.83830</td>\n",
       "      <td>-73.38830</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.995247</td>\n",
       "      <td>334</td>\n",
       "      <td>0</td>\n",
       "      <td>1133.0</td>\n",
       "      <td>20.316573</td>\n",
       "      <td>-72.832976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>636</td>\n",
       "      <td>0</td>\n",
       "      <td>54.68123</td>\n",
       "      <td>-0.76436</td>\n",
       "      <td>13.6</td>\n",
       "      <td>-1.001333</td>\n",
       "      <td>136</td>\n",
       "      <td>-7</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>54.627339</td>\n",
       "      <td>-0.678288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>601</td>\n",
       "      <td>1</td>\n",
       "      <td>43.58091</td>\n",
       "      <td>10.30402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.994676</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>44.092174</td>\n",
       "      <td>10.210445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>10.63966</td>\n",
       "      <td>106.76221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.002049</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>1119.0</td>\n",
       "      <td>11.064650</td>\n",
       "      <td>105.062118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>638</td>\n",
       "      <td>1</td>\n",
       "      <td>47.26898</td>\n",
       "      <td>-122.41612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.005127</td>\n",
       "      <td>318</td>\n",
       "      <td>0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>46.105873</td>\n",
       "      <td>-122.465326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>417</td>\n",
       "      <td>1</td>\n",
       "      <td>-27.37955</td>\n",
       "      <td>153.16483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.996204</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>-28.521759</td>\n",
       "      <td>153.151962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>478</td>\n",
       "      <td>1</td>\n",
       "      <td>29.86279</td>\n",
       "      <td>-93.93649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.997438</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>29.975832</td>\n",
       "      <td>-93.946886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>363</td>\n",
       "      <td>1</td>\n",
       "      <td>53.53161</td>\n",
       "      <td>9.99845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.002809</td>\n",
       "      <td>313</td>\n",
       "      <td>0</td>\n",
       "      <td>1259.0</td>\n",
       "      <td>53.686789</td>\n",
       "      <td>9.599343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>51.29873</td>\n",
       "      <td>3.23363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000759</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>1255.0</td>\n",
       "      <td>51.323563</td>\n",
       "      <td>4.197936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>37.34931</td>\n",
       "      <td>16.29756</td>\n",
       "      <td>18.6</td>\n",
       "      <td>-0.998796</td>\n",
       "      <td>219</td>\n",
       "      <td>-8</td>\n",
       "      <td>1147.0</td>\n",
       "      <td>37.455346</td>\n",
       "      <td>16.160932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>483</td>\n",
       "      <td>1</td>\n",
       "      <td>51.29938</td>\n",
       "      <td>3.23925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.994846</td>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>51.316338</td>\n",
       "      <td>3.421392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>36.69340</td>\n",
       "      <td>-4.38806</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1.001596</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>36.684286</td>\n",
       "      <td>-4.638825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>350</td>\n",
       "      <td>0</td>\n",
       "      <td>50.63193</td>\n",
       "      <td>-1.01217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.999475</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>50.566330</td>\n",
       "      <td>-1.171807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>646</td>\n",
       "      <td>0</td>\n",
       "      <td>47.54429</td>\n",
       "      <td>-122.52558</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.995222</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1256.0</td>\n",
       "      <td>47.544401</td>\n",
       "      <td>-122.524058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>-32.04416</td>\n",
       "      <td>115.75027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.997386</td>\n",
       "      <td>39</td>\n",
       "      <td>-1</td>\n",
       "      <td>1262.0</td>\n",
       "      <td>-31.999562</td>\n",
       "      <td>115.439510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>657</td>\n",
       "      <td>0</td>\n",
       "      <td>-43.42598</td>\n",
       "      <td>173.05472</td>\n",
       "      <td>17.4</td>\n",
       "      <td>-0.998546</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>-43.154132</td>\n",
       "      <td>173.344053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>282</td>\n",
       "      <td>1</td>\n",
       "      <td>58.01013</td>\n",
       "      <td>11.70418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.999475</td>\n",
       "      <td>282</td>\n",
       "      <td>0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>57.964849</td>\n",
       "      <td>11.649542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>550</td>\n",
       "      <td>1</td>\n",
       "      <td>39.44153</td>\n",
       "      <td>-0.30527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.003012</td>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>1258.0</td>\n",
       "      <td>40.220217</td>\n",
       "      <td>0.268149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>611</td>\n",
       "      <td>0</td>\n",
       "      <td>40.71220</td>\n",
       "      <td>29.46757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.001231</td>\n",
       "      <td>306</td>\n",
       "      <td>0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>40.914285</td>\n",
       "      <td>27.395283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vesselId  close_to_port  latitude_previous  longitude_previous  \\\n",
       "0         83              1           31.14647           -81.49789   \n",
       "1        619              1           14.81694           120.29625   \n",
       "2        592              0           38.27895            10.78280   \n",
       "3        538              0          -43.53785           172.83522   \n",
       "4          1              0           48.53320            -6.12003   \n",
       "5        682              1           51.35306             3.19241   \n",
       "6        159              0           43.34995            -8.50739   \n",
       "7         90              1           51.31666             3.22506   \n",
       "8        486              0           40.84712            29.29052   \n",
       "9        174              1           38.47378            15.91582   \n",
       "10        84              0           38.96142           -12.00502   \n",
       "11       589              0           43.72483            -5.33660   \n",
       "12       645              1           54.37278            18.65716   \n",
       "13       161              0           51.27275             4.21398   \n",
       "14       240              1           40.66726           -74.07082   \n",
       "15       654              1           59.73643            10.23870   \n",
       "16       667              1          -27.37772           153.16645   \n",
       "17       648              1           53.56397             8.56593   \n",
       "18       162              0           51.79319             3.34467   \n",
       "19       575              1           52.40704             4.86081   \n",
       "20       658              0           53.57648            -0.09023   \n",
       "21       378              0           51.26926             4.21159   \n",
       "22       295              0            1.17835           103.75237   \n",
       "23       323              0           47.33692          -122.47557   \n",
       "24       266              1           50.90418            -1.42857   \n",
       "25       511              1           51.30661             3.22715   \n",
       "26       587              0           24.92350          -109.76272   \n",
       "27       560              1           40.84103            14.27640   \n",
       "28       572              1           53.94362            10.86065   \n",
       "29       576              1           28.14658           -15.41917   \n",
       "30        76              1           34.70981           134.84946   \n",
       "31       393              1           54.40609            18.65503   \n",
       "32       165              0           51.26723             4.21002   \n",
       "33       175              0           37.53028            16.62312   \n",
       "34       571              1           53.94312            10.86149   \n",
       "35        81              0           56.15072            10.23467   \n",
       "36       245              1           53.57480             8.55801   \n",
       "37        23              1            6.03446           116.08002   \n",
       "38       271              1           53.57839             8.55239   \n",
       "39       397              0           53.58649            -0.06694   \n",
       "40       513              1           43.34959             5.33275   \n",
       "41       329              0           18.83830           -73.38830   \n",
       "42       636              0           54.68123            -0.76436   \n",
       "43       601              1           43.58091            10.30402   \n",
       "44       132              0           10.63966           106.76221   \n",
       "45       638              1           47.26898          -122.41612   \n",
       "46       417              1          -27.37955           153.16483   \n",
       "47       478              1           29.86279           -93.93649   \n",
       "48       363              1           53.53161             9.99845   \n",
       "49       116              1           51.29873             3.23363   \n",
       "50       200              0           37.34931            16.29756   \n",
       "51       483              1           51.29938             3.23925   \n",
       "52        62              1           36.69340            -4.38806   \n",
       "53       350              0           50.63193            -1.01217   \n",
       "54       646              0           47.54429          -122.52558   \n",
       "55        71              1          -32.04416           115.75027   \n",
       "56       657              0          -43.42598           173.05472   \n",
       "57       282              1           58.01013            11.70418   \n",
       "58       550              1           39.44153            -0.30527   \n",
       "59       611              0           40.71220            29.46757   \n",
       "\n",
       "    sog_previous  cog_previous  heading_previous  rot_previous  time_gap  \\\n",
       "0            0.0     -1.000012               344             0     900.0   \n",
       "1            0.0     -1.004793               214             0     541.0   \n",
       "2           18.7     -1.005309                 6             0     654.0   \n",
       "3            0.1     -0.995639                70             0    1080.0   \n",
       "4            0.3     -0.996574               275             0    1258.0   \n",
       "5            0.0     -1.003981                51             0    1087.0   \n",
       "6            0.0     -0.994981               298             0    1077.0   \n",
       "7            0.0     -1.005074               332             0    1082.0   \n",
       "8            0.0     -0.995370               269             0    1083.0   \n",
       "9            0.1     -0.999145               252             0    1080.0   \n",
       "10          17.1     -1.005176                13             0     996.0   \n",
       "11          19.7     -0.997006               277           -10    1080.0   \n",
       "12           0.0     -0.999537                25             0    1075.0   \n",
       "13           0.0     -0.999682               205             0    1080.0   \n",
       "14           0.0     -0.996407               299             0    1084.0   \n",
       "15           0.0     -1.000025               179             0    1083.0   \n",
       "16           0.0     -1.000605                38             0    1080.0   \n",
       "17           0.0     -0.995194                33             0    1080.0   \n",
       "18           0.2     -1.004222               311             0    1077.0   \n",
       "19           0.0     -1.004605                36             0    1080.0   \n",
       "20           0.0     -1.005062                93             0    1080.0   \n",
       "21           0.0     -1.005269               208             0    1441.0   \n",
       "22           0.1     -0.996441                 5             0    1080.0   \n",
       "23           0.0     -0.996528               117             0    1075.0   \n",
       "24           0.0     -0.996204               297             0    1078.0   \n",
       "25           0.0     -1.000512               155             0    1083.0   \n",
       "26          18.9     -1.004321                38             0    1129.0   \n",
       "27           0.0     -1.004046                21             0    1438.0   \n",
       "28           0.0     -1.004997                18             0    1081.0   \n",
       "29           0.0     -0.999769               272             0    1261.0   \n",
       "30           0.0     -1.002090               292             0    1253.0   \n",
       "31           0.0     -0.996012                66             0    1260.0   \n",
       "32           0.0     -0.999568               206             0    1260.0   \n",
       "33          16.4     -0.996485               293             0    1849.0   \n",
       "34           0.0     -1.004951                18             0    1261.0   \n",
       "35           0.0     -1.000972                41             0    1261.0   \n",
       "36           0.0     -1.002636               242             0    1264.0   \n",
       "37           0.1     -0.994660               215             0    1260.0   \n",
       "38           0.0     -0.994765                 4             0    1263.0   \n",
       "39           0.0     -0.997034               104             0    1259.0   \n",
       "40           0.0     -1.004787               308             0    1260.0   \n",
       "41          12.0     -0.995247               334             0    1133.0   \n",
       "42          13.6     -1.001333               136            -7    1158.0   \n",
       "43           0.0     -0.994676               173             0    1265.0   \n",
       "44           0.0     -1.002049               113             0    1119.0   \n",
       "45           0.0     -1.005127               318             0    1260.0   \n",
       "46           0.0     -0.996204                38             0    1260.0   \n",
       "47           0.0     -0.997438               220             0    1260.0   \n",
       "48           0.0     -1.002809               313             0    1259.0   \n",
       "49           0.0     -1.000759               152             0    1255.0   \n",
       "50          18.6     -0.998796               219            -8    1147.0   \n",
       "51           0.0     -0.994846               333             0    1260.0   \n",
       "52           0.1     -1.001596                36             0    1264.0   \n",
       "53           0.0     -0.999475                64             0    1265.0   \n",
       "54           0.2     -0.995222                37             0    1256.0   \n",
       "55           0.0     -0.997386                39            -1    1262.0   \n",
       "56          17.4     -0.998546               228             0    1174.0   \n",
       "57           0.0     -0.999475               282             0    1260.0   \n",
       "58           0.0     -1.003012               359             0    1258.0   \n",
       "59           0.0     -1.001231               306             0    1442.0   \n",
       "\n",
       "    latitude_predicted  longitude_predicted  \n",
       "0            30.652396           -81.517125  \n",
       "1            14.576297           121.154367  \n",
       "2            38.918529            10.919319  \n",
       "3           -43.534144           172.835347  \n",
       "4            48.527350            -7.662199  \n",
       "5            51.478282             3.556257  \n",
       "6            42.421574            -9.070712  \n",
       "7            51.322766             3.786315  \n",
       "8            40.850883            29.302719  \n",
       "9            39.526332            15.900867  \n",
       "10           39.122383           -12.091806  \n",
       "11           43.789091            -5.732661  \n",
       "12           54.551351            18.501677  \n",
       "13           51.275683             2.727242  \n",
       "14           40.666121           -74.165400  \n",
       "15           59.154742             9.896255  \n",
       "16          -28.852458           153.119492  \n",
       "17           55.004874             7.726822  \n",
       "18           51.933996             3.816442  \n",
       "19           52.433181             4.570027  \n",
       "20           53.474702             0.413703  \n",
       "21           51.271377             4.154781  \n",
       "22            1.178506           103.751517  \n",
       "23           47.336156          -122.474694  \n",
       "24           50.831639            -1.346207  \n",
       "25           51.327538             3.196732  \n",
       "26           25.079058          -109.235118  \n",
       "27           40.821491            14.113797  \n",
       "28           53.946968            11.255399  \n",
       "29           28.146577           -15.418708  \n",
       "30           34.695487           134.884713  \n",
       "31           54.701313            18.672605  \n",
       "32           51.272254             1.833956  \n",
       "33           37.574188            16.413145  \n",
       "34           54.340057            11.214245  \n",
       "35           56.528206            10.145746  \n",
       "36           52.526453             7.472609  \n",
       "37            6.034309           116.017640  \n",
       "38           53.126795             6.557346  \n",
       "39           53.304346             0.482964  \n",
       "40           43.389472             5.709414  \n",
       "41           20.316573           -72.832976  \n",
       "42           54.627339            -0.678288  \n",
       "43           44.092174            10.210445  \n",
       "44           11.064650           105.062118  \n",
       "45           46.105873          -122.465326  \n",
       "46          -28.521759           153.151962  \n",
       "47           29.975832           -93.946886  \n",
       "48           53.686789             9.599343  \n",
       "49           51.323563             4.197936  \n",
       "50           37.455346            16.160932  \n",
       "51           51.316338             3.421392  \n",
       "52           36.684286            -4.638825  \n",
       "53           50.566330            -1.171807  \n",
       "54           47.544401          -122.524058  \n",
       "55          -31.999562           115.439510  \n",
       "56          -43.154132           173.344053  \n",
       "57           57.964849            11.649542  \n",
       "58           40.220217             0.268149  \n",
       "59           40.914285            27.395283  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_and_append_coordinates(predictor, testset):\n",
    "    \"\"\"\n",
    "    Predicts longitude and latitude for the test set using a MultiOutputRegressor model\n",
    "    and appends them as new columns.\n",
    "\n",
    "    Parameters:\n",
    "    - predictor (MultiOutputRegressor): The trained model for predicting both latitude and longitude.\n",
    "    - testset (pd.DataFrame): The test set including all features required for prediction.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The test set DataFrame with appended 'latitude_predicted' and 'longitude_predicted' columns.\n",
    "    \"\"\"\n",
    "    # Predict both latitude and longitude on the test set\n",
    "    predictions = predictor.predict(testset)\n",
    "\n",
    "    # Extract predictions for latitude and longitude\n",
    "    latitude_predictions = predictions[:, 0]\n",
    "    longitude_predictions = predictions[:, 1]\n",
    "\n",
    "    # Append predictions to the test set DataFrame as new columns\n",
    "    testset_with_predictions = testset.copy()\n",
    "    testset_with_predictions['latitude_predicted'] = latitude_predictions\n",
    "    testset_with_predictions['longitude_predicted'] = longitude_predictions\n",
    "\n",
    "    return testset_with_predictions\n",
    "\n",
    "# Example usage:\n",
    "testset_with_predictions = predict_and_append_coordinates(model, testset)\n",
    "testset_with_predictions.head(60)  # To verify the appended predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>longitude_predicted</th>\n",
       "      <th>latitude_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-81.517125</td>\n",
       "      <td>30.652396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>121.154367</td>\n",
       "      <td>14.576297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10.919319</td>\n",
       "      <td>38.918529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>172.835347</td>\n",
       "      <td>-43.534144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-7.662199</td>\n",
       "      <td>48.527350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3.556257</td>\n",
       "      <td>51.478282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>-9.070712</td>\n",
       "      <td>42.421574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3.786315</td>\n",
       "      <td>51.322766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>29.302719</td>\n",
       "      <td>40.850883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>15.900867</td>\n",
       "      <td>39.526332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>-12.091806</td>\n",
       "      <td>39.122383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>-5.732661</td>\n",
       "      <td>43.789091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>18.501677</td>\n",
       "      <td>54.551351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2.727242</td>\n",
       "      <td>51.275683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>-74.165400</td>\n",
       "      <td>40.666121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>9.896255</td>\n",
       "      <td>59.154742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>153.119492</td>\n",
       "      <td>-28.852458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>7.726822</td>\n",
       "      <td>55.004874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>3.816442</td>\n",
       "      <td>51.933996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>4.570027</td>\n",
       "      <td>52.433181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.413703</td>\n",
       "      <td>53.474702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>4.154781</td>\n",
       "      <td>51.271377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>103.751517</td>\n",
       "      <td>1.178506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>-122.474694</td>\n",
       "      <td>47.336156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>-1.346207</td>\n",
       "      <td>50.831639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>3.196732</td>\n",
       "      <td>51.327538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>-109.235118</td>\n",
       "      <td>25.079058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>14.113797</td>\n",
       "      <td>40.821491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>11.255399</td>\n",
       "      <td>53.946968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>-15.418708</td>\n",
       "      <td>28.146577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>134.884713</td>\n",
       "      <td>34.695487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>18.672605</td>\n",
       "      <td>54.701312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>1.833956</td>\n",
       "      <td>51.272254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>16.413145</td>\n",
       "      <td>37.574188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>11.214245</td>\n",
       "      <td>54.340057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>10.145746</td>\n",
       "      <td>56.528206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>7.472609</td>\n",
       "      <td>52.526453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>116.017640</td>\n",
       "      <td>6.034309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>6.557346</td>\n",
       "      <td>53.126795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.482964</td>\n",
       "      <td>53.304346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>5.709414</td>\n",
       "      <td>43.389472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>-72.832976</td>\n",
       "      <td>20.316573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>-0.678288</td>\n",
       "      <td>54.627339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>10.210445</td>\n",
       "      <td>44.092174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>105.062118</td>\n",
       "      <td>11.064650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>-122.465326</td>\n",
       "      <td>46.105873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>153.151962</td>\n",
       "      <td>-28.521759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>-93.946886</td>\n",
       "      <td>29.975832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>9.599343</td>\n",
       "      <td>53.686789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>4.197936</td>\n",
       "      <td>51.323563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  longitude_predicted  latitude_predicted\n",
       "0    0           -81.517125           30.652396\n",
       "1    1           121.154367           14.576297\n",
       "2    2            10.919319           38.918529\n",
       "3    3           172.835347          -43.534144\n",
       "4    4            -7.662199           48.527350\n",
       "5    5             3.556257           51.478282\n",
       "6    6            -9.070712           42.421574\n",
       "7    7             3.786315           51.322766\n",
       "8    8            29.302719           40.850883\n",
       "9    9            15.900867           39.526332\n",
       "10  10           -12.091806           39.122383\n",
       "11  11            -5.732661           43.789091\n",
       "12  12            18.501677           54.551351\n",
       "13  13             2.727242           51.275683\n",
       "14  14           -74.165400           40.666121\n",
       "15  15             9.896255           59.154742\n",
       "16  16           153.119492          -28.852458\n",
       "17  17             7.726822           55.004874\n",
       "18  18             3.816442           51.933996\n",
       "19  19             4.570027           52.433181\n",
       "20  20             0.413703           53.474702\n",
       "21  21             4.154781           51.271377\n",
       "22  22           103.751517            1.178506\n",
       "23  23          -122.474694           47.336156\n",
       "24  24            -1.346207           50.831639\n",
       "25  25             3.196732           51.327538\n",
       "26  26          -109.235118           25.079058\n",
       "27  27            14.113797           40.821491\n",
       "28  28            11.255399           53.946968\n",
       "29  29           -15.418708           28.146577\n",
       "30  30           134.884713           34.695487\n",
       "31  31            18.672605           54.701312\n",
       "32  32             1.833956           51.272254\n",
       "33  33            16.413145           37.574188\n",
       "34  34            11.214245           54.340057\n",
       "35  35            10.145746           56.528206\n",
       "36  36             7.472609           52.526453\n",
       "37  37           116.017640            6.034309\n",
       "38  38             6.557346           53.126795\n",
       "39  39             0.482964           53.304346\n",
       "40  40             5.709414           43.389472\n",
       "41  41           -72.832976           20.316573\n",
       "42  42            -0.678288           54.627339\n",
       "43  43            10.210445           44.092174\n",
       "44  44           105.062118           11.064650\n",
       "45  45          -122.465326           46.105873\n",
       "46  46           153.151962          -28.521759\n",
       "47  47           -93.946886           29.975832\n",
       "48  48             9.599343           53.686789\n",
       "49  49             4.197936           51.323563"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_and_prepare_submission(predictor, testset, test_ids, submission_path='submission.csv'):\n",
    "    \"\"\"\n",
    "    Predicts latitude and longitude for the test set using a MultiOutputRegressor model, \n",
    "    merges the predictions with test IDs, and creates a submission file in the required Kaggle format.\n",
    "\n",
    "    Parameters:\n",
    "    - predictor (MultiOutputRegressor): The trained model for predicting both latitude and longitude.\n",
    "    - testset (pd.DataFrame): The test set including all features required for prediction.\n",
    "    - test_ids (pd.Series or list): The IDs for each entry in the test set.\n",
    "    - submission_path (str): The path to save the submission CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The final submission dataframe.\n",
    "    \"\"\"\n",
    "    # Step 1: Predict both latitude and longitude on the test set\n",
    "    predictions = predictor.predict(testset)\n",
    "\n",
    "    # Step 2: Extract latitude and longitude predictions\n",
    "    latitude_predictions = predictions[:, 0]\n",
    "    longitude_predictions = predictions[:, 1]\n",
    "\n",
    "    # Step 3: Combine predictions with test IDs into a single DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': test_ids,\n",
    "        'longitude_predicted': longitude_predictions,\n",
    "        'latitude_predicted': latitude_predictions\n",
    "    })\n",
    "\n",
    "    # Step 4: Save to CSV with the correct column order\n",
    "    submission_df.to_csv(submission_path, index=False, columns=['ID', 'longitude_predicted', 'latitude_predicted'])\n",
    "    print(f\"Submission file saved to {submission_path}\")\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "# Example usage:\n",
    "submission_df = predict_and_prepare_submission(model, testset, test_ids, 'submission.csv')\n",
    "submission_df.head(50)  # To verify the structure of the submission file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
