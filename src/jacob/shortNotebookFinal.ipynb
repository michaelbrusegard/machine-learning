{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522065 entries, 0 to 1522064\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count    Dtype  \n",
      "---  ------     --------------    -----  \n",
      " 0   time       1522065 non-null  object \n",
      " 1   cog        1522065 non-null  float64\n",
      " 2   sog        1522065 non-null  float64\n",
      " 3   rot        1522065 non-null  int64  \n",
      " 4   heading    1522065 non-null  int64  \n",
      " 5   navstat    1522065 non-null  int64  \n",
      " 6   etaRaw     1522065 non-null  object \n",
      " 7   latitude   1522065 non-null  float64\n",
      " 8   longitude  1522065 non-null  float64\n",
      " 9   vesselId   1522065 non-null  object \n",
      " 10  portId     1520450 non-null  object \n",
      "dtypes: float64(4), int64(3), object(4)\n",
      "memory usage: 127.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = '../../original_data/ais_train.csv'\n",
    "\n",
    "# Load the file into a pandas dataframe\n",
    "baseDataset = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "# Display the dataframe\n",
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51739 entries, 0 to 51738\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   ID              51739 non-null  int64  \n",
      " 1   vesselId        51739 non-null  object \n",
      " 2   time            51739 non-null  object \n",
      " 3   scaling_factor  51739 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "testset = pd.read_csv('../../original_data/ais_test.csv')\n",
    "testset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial pre-processing of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ais_train(df):\n",
    "\n",
    "    # Step 1: Convert 'time' to datetime and drop useless columns\n",
    "    df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    df.drop('etaRaw', axis=1, inplace=True)\n",
    "\n",
    "    # Step 2: Sort by vesselId and time\n",
    "    df = df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # Step 3: Convert relevant columns to float\n",
    "    df['cog'] = df['cog'].astype(float)\n",
    "    df['sog'] = df['sog'].astype(float)\n",
    "    df['rot'] = df['rot'].astype(float)\n",
    "    df['heading'] = df['heading'].astype(float)\n",
    "    df['latitude'] = df['latitude'].astype(float)\n",
    "    df['longitude'] = df['longitude'].astype(float)\n",
    "    \n",
    "    # Step 4: Replace invalid or default values with NaN\n",
    "    df['cog'] = np.where((df['cog'] == 360) | (df['cog'] > 360) | (df['cog'] < 0), np.nan, df['cog'])\n",
    "    df['sog'] = np.where((df['sog'] == 1023) | (df['sog'] < 0), np.nan, df['sog'])\n",
    "    df['heading'] = np.where((df['heading'] > 360) | (df['heading'] == 511) | (df['heading'] < 0), np.nan, df['heading'])\n",
    "    df['rot'] = np.where(df['rot'].isin([127, 128, -127, -128]), np.nan, df['rot'])\n",
    "\n",
    "    # Step 5: Normalize 'cog' and 'heading'\n",
    "    df['cog'] = (df['cog'] / 180) - 1\n",
    "    df['heading'] = (df['heading'] / 180) - 1\n",
    "\n",
    "    # Step 6: Remove all moored vessels\n",
    "    \"\"\" \n",
    "    Moored vessels give little to no information about movement, and still vessel and be inferred by the model\n",
    "     due to time elapsed. Hence, we remove all moored instance in light of having more informative data \n",
    "     \"\"\"\n",
    "    df = df[df['navstat'] != 5]\n",
    "\n",
    "    # Step 7: Remove all rows with Nan values\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "baseDataset = preprocess_ais_train(baseDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 892056 entries, 0 to 1522064\n",
      "Data columns (total 10 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   time       892056 non-null  datetime64[ns]\n",
      " 1   cog        892056 non-null  float64       \n",
      " 2   sog        892056 non-null  float64       \n",
      " 3   rot        892056 non-null  float64       \n",
      " 4   heading    892056 non-null  float64       \n",
      " 5   navstat    892056 non-null  int64         \n",
      " 6   latitude   892056 non-null  float64       \n",
      " 7   longitude  892056 non-null  float64       \n",
      " 8   vesselId   892056 non-null  object        \n",
      " 9   portId     892056 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(6), int64(1), object(2)\n",
      "memory usage: 74.9+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    vesselId  record_count  in_test_set\n",
      "0   61e9f3cbb937134a3c4bff09             1        False\n",
      "1   61e9f43fb937134a3c4c016f            77        False\n",
      "2   61e9f39ab937134a3c4bfdb9           138        False\n",
      "3   61e9f45eb937134a3c4c0235           140        False\n",
      "4   61e9f45cb937134a3c4c022b           155        False\n",
      "5   61e9f47ab937134a3c4c02f7           169        False\n",
      "6   61e9f42cb937134a3c4c00f9           172        False\n",
      "7   61e9f3afb937134a3c4bfe47           219        False\n",
      "8   61e9f3bcb937134a3c4bfe91           220         True\n",
      "9   61e9f456b937134a3c4c0203           223        False\n",
      "10  61e9f408b937134a3c4c0023           229        False\n",
      "11  61e9f418b937134a3c4c0077           239        False\n",
      "12  61e9f460b937134a3c4c0243           246        False\n",
      "13  61e9f3aeb937134a3c4bfe45           252        False\n",
      "14  61e9f3c3b937134a3c4bfeb7           264        False\n",
      "15  61e9f3bab937134a3c4bfe8b           267        False\n",
      "16  61e9f42cb937134a3c4c00fb           292        False\n",
      "17  61e9f402b937134a3c4c000f           293        False\n",
      "18  61e9f3f7b937134a3c4bffc5           297        False\n",
      "19  61e9f409b937134a3c4c0027           302        False\n"
     ]
    }
   ],
   "source": [
    "# Get the unique vesselIds from the test set\n",
    "vessel_ids_test = set(testset['vesselId'].unique())\n",
    "\n",
    "# Get the count of records per vesselId in the training set\n",
    "vessel_record_counts = baseDataset['vesselId'].value_counts()\n",
    "\n",
    "# Get the 10 vessels with the lowest number of records\n",
    "lowest_record_vessels = vessel_record_counts.nsmallest(20)\n",
    "\n",
    "# Check if these vessels are in the test set\n",
    "vessels_in_test = lowest_record_vessels.index.isin(vessel_ids_test)\n",
    "\n",
    "# Combine the results into a dataframe for easy viewing\n",
    "vessels_with_low_records = pd.DataFrame({\n",
    "    'vesselId': lowest_record_vessels.index,\n",
    "    'record_count': lowest_record_vessels.values,\n",
    "    'in_test_set': vessels_in_test\n",
    "})\n",
    "\n",
    "# Display the result\n",
    "print(vessels_with_low_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of vessel IDs to remove\n",
    "vessels_to_remove = ['61e9f3cbb937134a3c4bff09', '61e9f3adb937134a3c4bfe37']\n",
    "\n",
    "# Remove vessels from the dataset\n",
    "baseDataset = baseDataset[~baseDataset['vesselId'].isin(vessels_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_intervals(data, min_time_diff_minutes= 8):\n",
    "\n",
    "    # Ensure time column is in datetime format\n",
    "    data['time'] = pd.to_datetime(data['time'])\n",
    "    \n",
    "    # Sort by vesselId and time to ensure proper order\n",
    "    data = data.sort_values(['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "    # Calculate time differences in minutes\n",
    "    data['time_diff'] = data.groupby('vesselId')['time'].diff().dt.total_seconds() / 60  # in minutes\n",
    "\n",
    "    # Filter out records with time differences less than the specified threshold\n",
    "    filtered_data = data[(data['time_diff'].isna()) | (data['time_diff'] >= min_time_diff_minutes)].copy()\n",
    "\n",
    "    # Drop the time_diff column after filtering\n",
    "    filtered_data = filtered_data.drop(columns=['time_diff']).reset_index(drop=True)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "baseDataset = filter_short_intervals(baseDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Haversine formula to calculate the great-circle distance between two points\n",
    "    R = 3440.065  # Radius of Earth in nautical miles\n",
    "    lat1_rad = np.radians(lat1.astype(float))\n",
    "    lon1_rad = np.radians(lon1.astype(float))\n",
    "    lat2_rad = np.radians(lat2.astype(float))\n",
    "    lon2_rad = np.radians(lon2.astype(float))\n",
    "    \n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "def heuristic_clean(data, vthreshold=30):\n",
    "    \"\"\"\n",
    "    Cleans the trajectory data by removing points with unrealistic speeds.\n",
    "    \n",
    "    :param data: DataFrame containing raw trajectory data\n",
    "    :param vthreshold: Speed threshold in knots\n",
    "    :return: Cleaned trajectory DataFrame\n",
    "    \"\"\"\n",
    "    # Drop duplicates and reset index\n",
    "    data = data.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Sort by vessel ID and time to ensure correct order\n",
    "    data = data.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "    \n",
    "    # Shift the columns to align with the previous record within each vessel\n",
    "    data['prev_latitude'] = data.groupby('vesselId')['latitude'].shift(1)\n",
    "    data['prev_longitude'] = data.groupby('vesselId')['longitude'].shift(1)\n",
    "    data['prev_time'] = data.groupby('vesselId')['time'].shift(1)\n",
    "    \n",
    "    # Calculate time differences in hours\n",
    "    data['delta_time'] = (data['time'] - data['prev_time']).dt.total_seconds() / 3600.0  # in hours\n",
    "    \n",
    "    # Calculate distances using the haversine function where we have valid previous points\n",
    "    valid_rows = data['prev_latitude'].notna() & data['prev_longitude'].notna()\n",
    "    data.loc[valid_rows, 'delta_distance'] = haversine(\n",
    "        data.loc[valid_rows, 'prev_latitude'].to_numpy(),\n",
    "        data.loc[valid_rows, 'prev_longitude'].to_numpy(),\n",
    "        data.loc[valid_rows, 'latitude'].to_numpy(),\n",
    "        data.loc[valid_rows, 'longitude'].to_numpy()\n",
    "    )\n",
    "    \n",
    "    # Calculate speed (distance/time) and filter by threshold\n",
    "    data['speed'] = data['delta_distance'] / data['delta_time']\n",
    "    \n",
    "    # Filter out rows where speed exceeds the threshold\n",
    "    cleaned_data = data[(data['speed'] <= vthreshold) | data['speed'].isna()].copy()\n",
    "    \n",
    "    # Drop intermediate calculation columns\n",
    "    cleaned_data = cleaned_data.drop(columns=['prev_latitude', 'prev_longitude', 'prev_time', \n",
    "                                              'delta_time', 'delta_distance', 'speed'])\n",
    "    \n",
    "    return cleaned_data.reset_index(drop=True)\n",
    "\n",
    "baseDataset = heuristic_clean(baseDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kanskje legge til etterhvert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef calculate_movement_deltas(df):\\n\\n    # Calculate time difference in seconds between consecutive points for each vessel\\n    df = df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\\n    df['time_diff'] = df.groupby('vesselId')['time'].diff().dt.total_seconds()\\n    \\n    # Convert cog to radians for trigonometric calculations\\n    df['cog_rad'] = np.radians(df['cog'])\\n\\n    # Calculate delta_lat and delta_lon based on COG, ROT, and time difference\\n    df['delta_lat'] = np.sin(df['cog_rad']) * df['time_diff'] * df['sog']\\n    df['delta_lon'] = np.cos(df['cog_rad']) * df['time_diff'] * df['sog']\\n\\n    # Fill NaN values in delta_lat and delta_lon with 0\\n    df['delta_lat'].fillna(0, inplace=True)\\n    df['delta_lon'].fillna(0, inplace=True)\\n\\n    # Drop the unnecessary columns\\n    df.drop(columns=['cog_rad', 'time_diff'], inplace=True)\\n\\n    return df\\n\\nbaseDataset = calculate_movement_deltas(baseDataset)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def calculate_movement_deltas(df):\n",
    "\n",
    "    # Calculate time difference in seconds between consecutive points for each vessel\n",
    "    df = df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "    df['time_diff'] = df.groupby('vesselId')['time'].diff().dt.total_seconds()\n",
    "    \n",
    "    # Convert cog to radians for trigonometric calculations\n",
    "    df['cog_rad'] = np.radians(df['cog'])\n",
    "\n",
    "    # Calculate delta_lat and delta_lon based on COG, ROT, and time difference\n",
    "    df['delta_lat'] = np.sin(df['cog_rad']) * df['time_diff'] * df['sog']\n",
    "    df['delta_lon'] = np.cos(df['cog_rad']) * df['time_diff'] * df['sog']\n",
    "\n",
    "    # Fill NaN values in delta_lat and delta_lon with 0\n",
    "    df['delta_lat'].fillna(0, inplace=True)\n",
    "    df['delta_lon'].fillna(0, inplace=True)\n",
    "\n",
    "    # Drop the unnecessary columns\n",
    "    df.drop(columns=['cog_rad', 'time_diff'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "baseDataset = calculate_movement_deltas(baseDataset)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_features(df):\n",
    "    # Ensure 'time' column is in datetime format\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Extract day of the week (0=Monday, 6=Sunday)\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    \n",
    "    # Extract hour of the day\n",
    "    df['hour_of_day'] = df['time'].dt.hour\n",
    "    \n",
    "    # Create a binary feature for weekends (0=Weekday, 1=Weekend)\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Create a categorical feature for seasons\n",
    "    df['month'] = df['time'].dt.month\n",
    "    df['season'] = df['month'].apply(lambda x: (x%12 + 3)//3)\n",
    "    df['season'] = df['season'].map({1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Fall'})\n",
    "    \n",
    "    # One-hot encode the 'season' feature\n",
    "    df = pd.get_dummies(df, columns=['season'], prefix='season')\n",
    "    \n",
    "    # Drop the intermediate 'month' column\n",
    "    df.drop(columns=['month'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the feature engineering function to the baseDataset\n",
    "baseDataset = add_temporal_features(baseDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Employ Port based feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = '../../original_data/ports.csv'\n",
    "\n",
    "#Load the file into a pandas dataframe\n",
    "ports = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "def preprocess_ports_df(ports_df):\n",
    "    # Renaming the latitude and longitude columns in ports_df to portLatitude and portLongitude\n",
    "    ports_df = ports_df.rename(columns={'latitude': 'portLatitude', 'longitude': 'portLongitude'})\n",
    "    # List of columns to keep\n",
    "    columns_to_keep = ['portId', 'portLatitude', 'portLongitude']\n",
    "\n",
    "    # Drop all other columns except the ones specified in columns_to_keep\n",
    "    ports_df = ports_df[columns_to_keep]\n",
    "    return ports_df\n",
    "\n",
    "ports = preprocess_ports_df(ports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_close_to_port(df, ports, distance_threshold_nm=3, \n",
    "                       vessel_lat_col='latitude', vessel_lon_col='longitude', \n",
    "                       port_lat_col='portLatitude', port_lon_col='portLongitude'):\n",
    "    \"\"\"\n",
    "    Determines if each vessel in df is close to a port within a specified distance (in nautical miles).\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): DataFrame containing vessel data with specified latitude and longitude columns.\n",
    "    - ports (DataFrame): DataFrame containing port data with specified latitude and longitude columns.\n",
    "    - distance_threshold_nm (float): Distance threshold in nautical miles for identifying proximity to ports.\n",
    "    - vessel_lat_col (str): Column name for the vessel latitude in df.\n",
    "    - vessel_lon_col (str): Column name for the vessel longitude in df.\n",
    "    - port_lat_col (str): Column name for the port latitude in ports.\n",
    "    - port_lon_col (str): Column name for the port longitude in ports.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Updated df with a new binary column 'close_to_port', where 1 indicates proximity to a port within the threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert latitude and longitude to radians\n",
    "    df['lat_rad'] = np.radians(df[vessel_lat_col])\n",
    "    df['lon_rad'] = np.radians(df[vessel_lon_col])\n",
    "    ports['lat_rad'] = np.radians(ports[port_lat_col])\n",
    "    ports['lon_rad'] = np.radians(ports[port_lon_col])\n",
    "\n",
    "    # Build a k-d tree using port coordinates\n",
    "    port_coords = np.vstack((ports['lat_rad'], ports['lon_rad'])).T\n",
    "    port_tree = KDTree(port_coords, metric='euclidean')  # Using 'euclidean' since we pre-convert to radians\n",
    "\n",
    "    # Convert nautical miles to radians (1 nautical mile ≈ 1/3437.75 radians)\n",
    "    distance_threshold_radians = distance_threshold_nm / 3437.75\n",
    "\n",
    "    # Query each vessel against the port k-d tree for the closest port within the threshold\n",
    "    vessel_coords = np.vstack((df['lat_rad'], df['lon_rad'])).T\n",
    "    distances, indices = port_tree.query(vessel_coords, k=1, return_distance=True)\n",
    "\n",
    "    # Update df with close_to_port information based on the distance threshold\n",
    "    df['close_to_port'] = (distances.flatten() < distance_threshold_radians).astype(int)\n",
    "\n",
    "    # Drop the temporary radian columns\n",
    "    df.drop(columns=['lat_rad', 'lon_rad'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "baseDataset = find_close_to_port(\n",
    "    df=baseDataset, \n",
    "    ports=ports, \n",
    "    distance_threshold_nm=3, \n",
    "    vessel_lat_col='latitude', \n",
    "    vessel_lon_col='longitude', \n",
    "    port_lat_col='portLatitude', \n",
    "    port_lon_col='portLongitude'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 871978 entries, 0 to 871977\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   time           871978 non-null  datetime64[ns]\n",
      " 1   cog            871978 non-null  float64       \n",
      " 2   sog            871978 non-null  float64       \n",
      " 3   rot            871978 non-null  float64       \n",
      " 4   heading        871978 non-null  float64       \n",
      " 5   navstat        871978 non-null  int64         \n",
      " 6   latitude       871978 non-null  float64       \n",
      " 7   longitude      871978 non-null  float64       \n",
      " 8   vesselId       871978 non-null  object        \n",
      " 9   portId         871978 non-null  object        \n",
      " 10  day_of_week    871978 non-null  int32         \n",
      " 11  hour_of_day    871978 non-null  int32         \n",
      " 12  is_weekend     871978 non-null  int64         \n",
      " 13  season_Spring  871978 non-null  bool          \n",
      " 14  season_Winter  871978 non-null  bool          \n",
      " 15  close_to_port  871978 non-null  int64         \n",
      "dtypes: bool(2), datetime64[ns](1), float64(6), int32(2), int64(3), object(2)\n",
      "memory usage: 88.1+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We finalize the traning data and fill the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "vessels = baseDataset[\"vesselId\"].unique().tolist()\n",
    "le.fit(vessels)\n",
    "baseDataset[\"vesselId\"] = le.transform(baseDataset[\"vesselId\"])\n",
    "\n",
    "baseDataset = baseDataset.drop(columns=['navstat', 'portId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_test_data(training_filepath, test_filepath, ports_df, le, distance_threshold_nm=3):\n",
    "\n",
    "    # Load and process training data to get the latest known position of each vessel\n",
    "    ais_train = pd.read_csv(training_filepath, sep='|')\n",
    "    ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "    ais_train = ais_train.sort_values(by='time')\n",
    "    final_positions = ais_train.groupby(\"vesselId\").last()\n",
    "\n",
    "    # Rename columns in final_positions for feature engineering in the test data\n",
    "    final_positions = final_positions.rename(columns={\n",
    "        'time': 'time_previous',\n",
    "        'latitude': 'latitude_previous',\n",
    "        'longitude': 'longitude_previous',\n",
    "        'sog': 'sog_previous',\n",
    "        'cog': 'cog_previous',\n",
    "        'rot': 'rot_previous',\n",
    "        'heading': 'heading_previous'\n",
    "    })\n",
    "\n",
    "    # Load test data and merge with latest known positions\n",
    "    ais_test = pd.read_csv(test_filepath)\n",
    "    ais_test['time'] = pd.to_datetime(ais_test['time'])\n",
    "    ais_test= ais_test.merge(final_positions, on=\"vesselId\", how=\"left\")\n",
    "\n",
    "    # Normalize course and heading\n",
    "    ais_test['cog_previous'] = (ais_test['cog_previous'] / 180) - 1\n",
    "    ais_test['cog_previous'] = (ais_test['cog_previous'] / 180) - 1\n",
    "\n",
    "    # Encode vessel IDs using the provided LabelEncoder\n",
    "    ais_test['vesselId'] = le.transform(ais_test['vesselId'])\n",
    "\n",
    "    # Calculate time difference in seconds between the test time and last known time\n",
    "    ais_test['time_gap'] = (ais_test['time'] - ais_test['time_previous']).dt.total_seconds()\n",
    "\n",
    "    # Determine proximity to ports by using the find_close_to_port function\n",
    "    testset = find_close_to_port(\n",
    "        df=ais_test,\n",
    "        ports=ports_df,\n",
    "        distance_threshold_nm=distance_threshold_nm,\n",
    "        vessel_lat_col='latitude_previous',\n",
    "        vessel_lon_col='longitude_previous',\n",
    "        port_lat_col='portLatitude',\n",
    "        port_lon_col='portLongitude'\n",
    "    )\n",
    "\n",
    "    testset = testset.drop(columns=['scaling_factor', 'time_previous', 'navstat', 'etaRaw', 'portId'])\n",
    "\n",
    "    return testset\n",
    "\n",
    "\n",
    "test_filled = fill_test_data(\n",
    "    training_filepath='../../original_data/ais_train.csv',\n",
    "    test_filepath='../../original_data/ais_test.csv',\n",
    "    ports_df=ports,\n",
    "    le=le,\n",
    "    distance_threshold_nm=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only vital information for dataset we want to create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We create the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagged_training_data(df, lag_steps):\n",
    "    \"\"\"\n",
    "    Creates a training set with features that include previous values for latitude, longitude,\n",
    "    speed over ground (sog), and time-related features like hour and day of the week.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing the AIS data.\n",
    "    - lag_steps (int): The number of time steps to shift for creating lagged features.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A new DataFrame with added lagged features and time-related information.\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame to work on\n",
    "    data = df.copy()\n",
    "\n",
    "    # Generate lagged features for latitude, longitude, and speed over ground (sog)\n",
    "    data['latitude_previous'] = data.groupby('vesselId')['latitude'].shift(lag_steps)\n",
    "    data['longitude_previous'] = data.groupby('vesselId')['longitude'].shift(lag_steps)\n",
    "    data['sog_previous'] = data.groupby('vesselId')['sog'].shift(lag_steps)\n",
    "    #data['delta_lat_previous'] = data.groupby('vesselId')['delta_lat'].shift(lag_steps)\n",
    "    #data['delta_lon_previous'] = data.groupby('vesselId')['delta_lon'].shift(lag_steps)\n",
    "    data['cog_previous'] = data.groupby('vesselId')['cog'].shift(lag_steps)\n",
    "    data['heading_previous'] = data.groupby('vesselId')['heading'].shift(lag_steps)\n",
    "    data['rot_previous'] = data.groupby('vesselId')['rot'].shift(lag_steps)\n",
    "    \n",
    "\n",
    "    # Calculate time difference between consecutive points in seconds\n",
    "    data['temp_time_gap'] = data.groupby('vesselId')['time'].diff(lag_steps)\n",
    "    data['time_gap'] = data['temp_time_gap'].dt.total_seconds()\n",
    "\n",
    "    # Drop any rows with missing values due to shifting or time differences\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    data.drop(columns=['temp_time_gap'], inplace=True)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We make a function where we can specify amount of lags! For example, if we want to train multiple models based on different lags, as we want each model to capture different time horizons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, each vessel should apprxoimately have 240 instances for all the 5 days, i.e. we should train on 240 lags to capture all possibilities, however this reduces amount of data for the first lags (which are more important), thus we could also specify 50 lags, for example to train on the first day, nd get more data for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainingset(df, max_size=12000000, total_lags=69):\n",
    "    \"\"\"\n",
    "    Creates a training dataset by sampling data from specified lags, with sample size per lag \n",
    "    dynamically adjusted based on the total number of lags.\n",
    "\n",
    "    Parameters:\n",
    "    - make_training_set_func (function): Function that generates the training set for a given lag.\n",
    "    - max_size (int): Maximum number of total samples to accumulate across all lags.\n",
    "    - total_lags (int): Total number of lag steps to include in the training dataset.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Concatenated training dataset containing sampled instances across specified lags.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    total_size = 0\n",
    "    lag = 1\n",
    "    \n",
    "    # Dynamically calculate max_samples_per_lag based on total_lags and max_size\n",
    "    max_samples_per_lag = max_size // total_lags\n",
    "\n",
    "    while lag <= total_lags and total_size < max_size:\n",
    "        # Generate dataset for the current lag\n",
    "        current_set = lagged_training_data(df, lag)\n",
    "        \n",
    "        # If the dataset is empty for the current lag, break the loop\n",
    "        if len(current_set) == 0:\n",
    "            break\n",
    "\n",
    "        # Sample the dataset with the dynamically calculated sample size for this lag\n",
    "        sampled_set = current_set.sample(min(max_samples_per_lag, len(current_set)), random_state=42)\n",
    "        datasets.append(sampled_set)\n",
    "        total_size += len(sampled_set)\n",
    "\n",
    "        print(f\"Size after lag {lag}: {total_size:_} samples accumulated with {len(sampled_set)} samples from lag {lag}\")\n",
    "\n",
    "        # Move to the next lag step\n",
    "        lag += 1\n",
    "\n",
    "    # Concatenate all sampled datasets into a single training set\n",
    "    training_data = pd.concat(datasets, ignore_index=True)\n",
    "    print(\"Total length of training data:\", len(training_data))\n",
    "\n",
    "    # Clear out memory by deleting intermediate datasets list\n",
    "    del datasets\n",
    "\n",
    "    training_data = training_data.drop(columns=['time', 'sog', 'cog', 'heading', 'rot'])\n",
    "\n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after lag 1: 857_142 samples accumulated with 857142 samples from lag 1\n",
      "Size after lag 2: 1_714_284 samples accumulated with 857142 samples from lag 2\n",
      "Size after lag 3: 2_571_426 samples accumulated with 857142 samples from lag 3\n",
      "Size after lag 4: 3_428_568 samples accumulated with 857142 samples from lag 4\n",
      "Size after lag 5: 4_285_710 samples accumulated with 857142 samples from lag 5\n",
      "Size after lag 6: 5_142_852 samples accumulated with 857142 samples from lag 6\n",
      "Size after lag 7: 5_999_994 samples accumulated with 857142 samples from lag 7\n",
      "Size after lag 8: 6_857_136 samples accumulated with 857142 samples from lag 8\n",
      "Size after lag 9: 7_714_278 samples accumulated with 857142 samples from lag 9\n",
      "Size after lag 10: 8_571_420 samples accumulated with 857142 samples from lag 10\n",
      "Size after lag 11: 9_428_562 samples accumulated with 857142 samples from lag 11\n",
      "Size after lag 12: 10_285_704 samples accumulated with 857142 samples from lag 12\n",
      "Size after lag 13: 11_142_846 samples accumulated with 857142 samples from lag 13\n",
      "Size after lag 14: 11_999_988 samples accumulated with 857142 samples from lag 14\n",
      "Total length of training data: 11999988\n"
     ]
    }
   ],
   "source": [
    "train_data = create_trainingset(baseDataset, max_size=12000000, total_lags=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Define features (X) and targets (y)\n",
    "X = train_data.drop(columns=['latitude', 'longitude'])\n",
    "y = train_data[['latitude', 'longitude']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseDataset: 74.84 MB\n",
      "testset: 8.54 MB\n",
      "vessels_with_low_records: 0.00 MB\n",
      "ports: 0.14 MB\n",
      "test_filled: 4.34 MB\n",
      "train_data: 1213.07 MB\n",
      "X: 1029.97 MB\n",
      "y: 183.11 MB\n"
     ]
    }
   ],
   "source": [
    "# Find all DataFrames in memory\n",
    "dataframes_in_memory = {name: obj for name, obj in globals().items() if isinstance(obj, pd.DataFrame)}\n",
    "\n",
    "# Print the names and memory usage of each DataFrame\n",
    "for name, df in dataframes_in_memory.items():\n",
    "    print(f\"{name}: {df.memory_usage(deep=True).sum() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "#del gdf_with_near_land\n",
    "del train_data\n",
    "del testset\n",
    "del baseDataset # Ikke dersom vi vil lage flere modeller for å stacke tid\n",
    "\n",
    "\n",
    "# Run garbage collection to free up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed on the training dataset.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the MultiOutputRegressor with RandomForest\n",
    "# Initialize and train the MultiOutputRegressor with an optimized RandomForest\n",
    "model = MultiOutputRegressor(\n",
    "    RandomForestRegressor(\n",
    "        n_estimators=10,           # Adjusted number of trees for balance between performance and training time\n",
    "        n_jobs=6,                  # Use all CPU cores dynamically\n",
    "        random_state=42 ,            # For reproducibility\n",
    "        warm_start=False,\n",
    "        criterion='squared_error',\n",
    "        max_depth=30\n",
    "    )\n",
    ")\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Model training completed on the training dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test set for prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['day_of_week', 'hour_of_day', 'is_weekend', 'season_Spring', 'season_Winter'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m test_filled \u001b[38;5;241m=\u001b[39m test_filled\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Set testset columns in the same order as the training set X\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mtest_filled\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/machine-learning/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/machine-learning/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/machine-learning/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['day_of_week', 'hour_of_day', 'is_weekend', 'season_Spring', 'season_Winter'] not in index\""
     ]
    }
   ],
   "source": [
    "test_ids = test_filled['ID'].copy()\n",
    "test_filled = test_filled.drop(columns=['ID'])\n",
    "# Set testset columns in the same order as the training set X\n",
    "testset = test_filled[X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_append_coordinates(predictor, testset):\n",
    "    \"\"\"\n",
    "    Predicts longitude and latitude for the test set using a MultiOutputRegressor model\n",
    "    and appends them as new columns.\n",
    "\n",
    "    Parameters:\n",
    "    - predictor (MultiOutputRegressor): The trained model for predicting both latitude and longitude.\n",
    "    - testset (pd.DataFrame): The test set including all features required for prediction.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The test set DataFrame with appended 'latitude_predicted' and 'longitude_predicted' columns.\n",
    "    \"\"\"\n",
    "    # Predict both latitude and longitude on the test set\n",
    "    predictions = predictor.predict(testset)\n",
    "\n",
    "    # Extract predictions for latitude and longitude\n",
    "    latitude_predictions = predictions[:, 0]\n",
    "    longitude_predictions = predictions[:, 1]\n",
    "\n",
    "    # Append predictions to the test set DataFrame as new columns\n",
    "    testset_with_predictions = testset.copy()\n",
    "    testset_with_predictions['latitude_predicted'] = latitude_predictions\n",
    "    testset_with_predictions['longitude_predicted'] = longitude_predictions\n",
    "\n",
    "    return testset_with_predictions\n",
    "\n",
    "# Example usage:\n",
    "testset_with_predictions = predict_and_append_coordinates(model, testset)\n",
    "testset_with_predictions.head(60)  # To verify the appended predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_prepare_submission(predictor, testset, test_ids, submission_path='submission.csv'):\n",
    "    \"\"\"\n",
    "    Predicts latitude and longitude for the test set using a MultiOutputRegressor model, \n",
    "    merges the predictions with test IDs, and creates a submission file in the required Kaggle format.\n",
    "\n",
    "    Parameters:\n",
    "    - predictor (MultiOutputRegressor): The trained model for predicting both latitude and longitude.\n",
    "    - testset (pd.DataFrame): The test set including all features required for prediction.\n",
    "    - test_ids (pd.Series or list): The IDs for each entry in the test set.\n",
    "    - submission_path (str): The path to save the submission CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The final submission dataframe.\n",
    "    \"\"\"\n",
    "    # Step 1: Predict both latitude and longitude on the test set\n",
    "    predictions = predictor.predict(testset)\n",
    "\n",
    "    # Step 2: Extract latitude and longitude predictions\n",
    "    latitude_predictions = predictions[:, 0]\n",
    "    longitude_predictions = predictions[:, 1]\n",
    "\n",
    "    # Step 3: Combine predictions with test IDs into a single DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': test_ids,\n",
    "        'longitude_predicted': longitude_predictions,\n",
    "        'latitude_predicted': latitude_predictions\n",
    "    })\n",
    "\n",
    "    # Step 4: Save to CSV with the correct column order\n",
    "    submission_df.to_csv(submission_path, index=False, columns=['ID', 'longitude_predicted', 'latitude_predicted'])\n",
    "    print(f\"Submission file saved to {submission_path}\")\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "# Example usage:\n",
    "submission_df = predict_and_prepare_submission(model, testset, test_ids, '../../submissions/subfinal.csv')\n",
    "submission_df.head(50)  # To verify the structure of the submission file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
