{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d49ef1-f6b0-4500-a7eb-c5111046c6b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "# Calculations\n",
    "import math\n",
    "from math import atan2, radians, degrees, sin, cos\n",
    "from haversine import haversine, Unit\n",
    "from geopy.distance import geodesic\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial import KDTree\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from gluonts.torch.model.tft import TemporalFusionTransformerEstimator  # TFT from Torch\n",
    "from gluonts.torch.model.deepar import DeepAREstimator  # DeepAR from Torch\n",
    "from lightning.pytorch import Trainer  # PyTorch Lightning Trainer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim  # For optimization\n",
    "\n",
    "# AutoML libraries\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Other libraries\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a2887-dd71-42ef-a560-cb02ff671d53",
   "metadata": {},
   "source": [
    "### Models Employed\n",
    "\\\n",
    "tabularPredictor\n",
    "\\\n",
    "GRU-D \n",
    "\\\n",
    "Both of these for one-step recurrent prediction\n",
    "\\\n",
    "TFT\n",
    "\\\n",
    "deepAR\n",
    "\\\n",
    "Both of these for sequence prediction\n",
    "\\\n",
    "These will be stacked using autogluon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c80638-f501-4230-b8fd-80759629ad94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522065 entries, 0 to 1522064\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count    Dtype  \n",
      "---  ------     --------------    -----  \n",
      " 0   time       1522065 non-null  object \n",
      " 1   cog        1522065 non-null  float64\n",
      " 2   sog        1522065 non-null  float64\n",
      " 3   rot        1522065 non-null  int64  \n",
      " 4   heading    1522065 non-null  int64  \n",
      " 5   navstat    1522065 non-null  int64  \n",
      " 6   etaRaw     1522065 non-null  object \n",
      " 7   latitude   1522065 non-null  float64\n",
      " 8   longitude  1522065 non-null  float64\n",
      " 9   vesselId   1522065 non-null  object \n",
      " 10  portId     1520450 non-null  object \n",
      "dtypes: float64(4), int64(3), object(4)\n",
      "memory usage: 127.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = os.path.join(current_dir, '../../original_data/ais_train.csv')\n",
    "\n",
    "# Load the file into a pandas dataframe\n",
    "ais_train_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "# Display the dataframe\n",
    "ais_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac1f7b1-4a3c-4372-9ea0-0c6647f94499",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51739 entries, 0 to 51738\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   ID              51739 non-null  int64  \n",
      " 1   vesselId        51739 non-null  object \n",
      " 2   time            51739 non-null  object \n",
      " 3   scaling_factor  51739 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "ais_test_df = pd.read_csv(os.path.join(current_dir, '../../original_data/ais_test.csv'))\n",
    "ais_test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b31dbc-e3f3-47c1-9b44-122c4a57792d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1329 entries, 0 to 1328\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   portId        1329 non-null   object \n",
      " 1   name          1329 non-null   object \n",
      " 2   portLocation  1329 non-null   object \n",
      " 3   longitude     1329 non-null   float64\n",
      " 4   latitude      1329 non-null   float64\n",
      " 5   UN_LOCODE     1329 non-null   object \n",
      " 6   countryName   1329 non-null   object \n",
      " 7   ISO           1329 non-null   object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 83.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = os.path.join(current_dir, '../../original_data/ports.csv')\n",
    "\n",
    "#Load the file into a pandas dataframe\n",
    "ports_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "ports_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905df1ee-9403-42c8-b9e4-341e17dc2170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 711 entries, 0 to 710\n",
      "Data columns (total 20 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   shippingLineId  711 non-null    object \n",
      " 1   vesselId        711 non-null    object \n",
      " 2   CEU             711 non-null    int64  \n",
      " 3   DWT             703 non-null    float64\n",
      " 4   GT              711 non-null    int64  \n",
      " 5   NT              187 non-null    float64\n",
      " 6   vesselType      699 non-null    float64\n",
      " 7   breadth         703 non-null    float64\n",
      " 8   depth           242 non-null    float64\n",
      " 9   draft           10 non-null     float64\n",
      " 10  enginePower     691 non-null    float64\n",
      " 11  freshWater      221 non-null    float64\n",
      " 12  fuel            221 non-null    float64\n",
      " 13  homePort        573 non-null    object \n",
      " 14  length          711 non-null    float64\n",
      " 15  maxHeight       35 non-null     float64\n",
      " 16  maxSpeed        213 non-null    float64\n",
      " 17  maxWidth        35 non-null     float64\n",
      " 18  rampCapacity    34 non-null     float64\n",
      " 19  yearBuilt       711 non-null    int64  \n",
      "dtypes: float64(14), int64(3), object(3)\n",
      "memory usage: 111.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = os.path.join(current_dir, '../../original_data/vessels.csv')\n",
    "\n",
    "#Load the file into a pandas dataframe\n",
    "vessels_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "vessels_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8443323-6694-40d7-a014-e9f1b3d95cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = os.path.join(current_dir, '../../original_data/schedules_to_may_2024.csv')\n",
    "#Load the file into a pandas dataframe\n",
    "schedules_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b6c1a-7511-4310-b95b-b1d46438b58a",
   "metadata": {},
   "source": [
    "# We pre-process ais_train and combine it with the relevant features from ports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2ccbcd8-8d1b-40b9-a575-2df69deeb712",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522065 entries, 0 to 1522064\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count    Dtype         \n",
      "---  ------         --------------    -----         \n",
      " 0   time           1522065 non-null  datetime64[ns]\n",
      " 1   cog            1516207 non-null  float64       \n",
      " 2   sog            1522065 non-null  float64       \n",
      " 3   rot            1522065 non-null  float64       \n",
      " 4   heading        1517169 non-null  float64       \n",
      " 5   navstat        1522065 non-null  int64         \n",
      " 6   latitude       1522065 non-null  float64       \n",
      " 7   longitude      1522065 non-null  float64       \n",
      " 8   vesselId       1522065 non-null  object        \n",
      " 9   portId         1520450 non-null  object        \n",
      " 10  portLatitude   1520450 non-null  float64       \n",
      " 11  portLongitude  1520450 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(8), int64(1), object(2)\n",
      "memory usage: 139.3+ MB\n"
     ]
    }
   ],
   "source": [
    "def preprocess_ais_train(ais_train_df, ports_df):\n",
    "    \"\"\"\n",
    "    Preprocess the ais_train_df by converting columns, handling missing or invalid values, \n",
    "    merging port information, and mapping NAVSTAT codes to descriptions.\n",
    "    \n",
    "    Additionally, set 'etaRaw' to NaN if its value is less than the current time.\n",
    "\n",
    "    Parameters:\n",
    "    - ais_train_df: DataFrame containing the raw AIS train data.\n",
    "    - ports_df: DataFrame containing port information with portId, latitude, and longitude.\n",
    "\n",
    "    Returns:\n",
    "    - ais_train_df_cleaned: A cleaned and preprocessed version of ais_train_df.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert 'time' to datetime and drop 'etaRaw'\n",
    "    ais_train_df['time'] = pd.to_datetime(ais_train_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    ais_train_df.drop('etaRaw', axis=1, inplace=True)\n",
    "\n",
    "    # Step 4: Convert relevant columns to float\n",
    "    ais_train_df['cog'] = ais_train_df['cog'].astype(float)\n",
    "    ais_train_df['sog'] = ais_train_df['sog'].astype(float)\n",
    "    ais_train_df['rot'] = ais_train_df['rot'].astype(float)\n",
    "    ais_train_df['heading'] = ais_train_df['heading'].astype(float)\n",
    "    ais_train_df['latitude'] = ais_train_df['latitude'].astype(float)\n",
    "    ais_train_df['longitude'] = ais_train_df['longitude'].astype(float)\n",
    "    \n",
    "    # Step 5: Replace invalid or default values with NaN\n",
    "    ais_train_df['cog'] = np.where((ais_train_df['cog'] == 360) | (ais_train_df['cog'] > 360) | (ais_train_df['cog'] < 0), np.nan, ais_train_df['cog'])\n",
    "    ais_train_df['sog'] = np.where((ais_train_df['sog'] == 1023) | (ais_train_df['sog'] < 0), np.nan, ais_train_df['sog'])\n",
    "    ais_train_df['rot'] = np.where((ais_train_df['rot'] == -128), np.nan, ais_train_df['rot'])\n",
    "    ais_train_df['heading'] = np.where((ais_train_df['heading'] > 360) | (ais_train_df['heading'] == 511) | (ais_train_df['heading'] < 0), np.nan, ais_train_df['heading'])\n",
    "\n",
    "    # Step 6: Merge with ports to get port latitude and longitude\n",
    "\n",
    "    # Renaming the latitude and longitude columns in ports_df to portLatitude and portLongitude\n",
    "    ports_df = ports_df.rename(columns={'latitude': 'portLatitude', 'longitude': 'portLongitude'})\n",
    "    \n",
    "    # Merging ais_train_df with the updated ports_df on 'portId'\n",
    "    ais_train_df = ais_train_df.merge(ports_df[['portId', 'portLatitude', 'portLongitude']], on='portId', how='left')\n",
    "    \n",
    "    # Step 7: Sort by vesselId and time\n",
    "    ais_train_df = ais_train_df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "    return ais_train_df\n",
    "\n",
    "ais_train_df = preprocess_ais_train(ais_train_df, ports_df)\n",
    "\n",
    "ais_train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db667999-8e0e-4706-97f4-544dbd16cdd2",
   "metadata": {},
   "source": [
    "# We now pre-process the vessels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fd2d45d-26bc-4de2-af26-b92af62e9c23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 711 entries, 0 to 710\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   shippingLineId  711 non-null    object  \n",
      " 1   vesselId        711 non-null    object  \n",
      " 2   CEU             711 non-null    int64   \n",
      " 3   DWT             703 non-null    float64 \n",
      " 4   GT              711 non-null    int64   \n",
      " 5   vesselType      711 non-null    category\n",
      " 6   breadth         703 non-null    float64 \n",
      " 7   homePort        711 non-null    object  \n",
      " 8   length          711 non-null    float64 \n",
      " 9   age             711 non-null    int64   \n",
      "dtypes: category(1), float64(3), int64(3), object(3)\n",
      "memory usage: 51.0+ KB\n"
     ]
    }
   ],
   "source": [
    "def preprocess_vessels(vessels_df):\n",
    "    \"\"\"\n",
    "    Preprocess the vessels_df by converting 'yearBuilt' to 'age', handling missing values, \n",
    "    mapping 'homePort', and converting 'shippingLineId' into a categorical feature.\n",
    "    \n",
    "    Parameters:\n",
    "    - vessels_df: DataFrame containing the raw vessels data.\n",
    "    \n",
    "    Returns:\n",
    "    - vessels_df_cleaned: A cleaned and preprocessed version of vessels_df.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Convert 'yearBuilt' to 'age'\n",
    "    current_year = 2024\n",
    "    vessels_df['age'] = vessels_df['yearBuilt'].apply(lambda x: current_year - x if pd.notna(x) else np.nan)\n",
    "    vessels_df.drop(columns=['yearBuilt'], inplace=True)\n",
    "    \n",
    "    # Step 2: Drop columns with high missing values and low predictive power\n",
    "    columns_to_drop = ['NT', 'depth', 'draft', 'freshWater', 'enginePower', 'fuel', \n",
    "                       'maxHeight', 'maxWidth', 'rampCapacity', 'maxSpeed']\n",
    "    vessels_df.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    # Step 3: Make vesselType into category\n",
    "    # Convert 'vesselType' from float to categorical without knowing the exact mapping\n",
    "    vessels_df['vesselType'] = vessels_df['vesselType'].astype('category')\n",
    "\n",
    "    # Optionally, handle missing values (nan) by filling them with 'Unknown' or leaving them as is\n",
    "    vessels_df['vesselType'] = vessels_df['vesselType'].cat.add_categories('Unknown').fillna('Unknown')\n",
    "    \n",
    "    return vessels_df\n",
    "\n",
    "\n",
    "def map_homePort_to_country(vessels_df):\n",
    "    \"\"\"\n",
    "    Maps 'homePort' city names to their respective countries and groups rare countries into 'OTHER'.\n",
    "    \"\"\"\n",
    "    initial_mapping = {\n",
    "        'PANAMA': 'Panama', 'UNKNOWN': 'Unknown', 'PALERMO': 'Italy', 'NASSAU': 'Bahamas', \n",
    "        'TOKYO': 'Japan', 'VALLETTA': 'Malta', 'OSLO': 'Norway', 'MONROVIA': 'Liberia', \n",
    "        'MAJURO': 'Marshall Islands', 'JEJU CHEJU': 'South Korea', 'HELSINKI': 'Finland',\n",
    "        # Add other mappings here...\n",
    "    }\n",
    "    \n",
    "    vessels_df['homePort'] = vessels_df['homePort'].map(initial_mapping).fillna('OTHER')\n",
    "    \n",
    "    # Group rare countries into 'OTHER' (with fewer than 10 occurrences)\n",
    "    country_counts = vessels_df['homePort'].value_counts()\n",
    "    rare_countries = country_counts[country_counts < 10].index.tolist()\n",
    "    vessels_df['homePort'] = vessels_df['homePort'].replace(rare_countries, 'OTHER')\n",
    "    \n",
    "    return vessels_df\n",
    "\n",
    "\n",
    "def process_shippingLineId(vessels_df):\n",
    "    \"\"\"\n",
    "    Converts 'shippingLineId' into a categorical feature, grouping those with less than 13 occurrences \n",
    "    into 'Unknown'.\n",
    "    \"\"\"\n",
    "    # Calculate the frequency of each shippingLineId\n",
    "    shipping_freq = vessels_df['shippingLineId'].value_counts()\n",
    "    \n",
    "    # Map rare shippingLineIds (occurring less than 13 times) to 'Unknown'\n",
    "    vessels_df['shippingLineId'] = vessels_df['shippingLineId'].apply(\n",
    "        lambda x: x if shipping_freq[x] >= 13 else 'Unknown')\n",
    "    \n",
    "    return vessels_df\n",
    "\n",
    "\n",
    "# Main Preprocessing Pipeline\n",
    "def preprocess_all(vessels_df):\n",
    "    \"\"\"\n",
    "    Full preprocessing pipeline for vessels_df including handling shippingLineId, homePort,\n",
    "    KNN imputation, and maxSpeed adjustments.\n",
    "    \"\"\"\n",
    "    # Step 1: Basic preprocessing (convert yearBuilt to age and drop unnecessary columns)\n",
    "    vessels_df = preprocess_vessels(vessels_df)\n",
    "    \n",
    "    # Step 2: Map 'homePort' to country\n",
    "    vessels_df = map_homePort_to_country(vessels_df)\n",
    "    \n",
    "    # Step 3: Convert 'shippingLineId' to categorical with grouping of rare values\n",
    "    vessels_df = process_shippingLineId(vessels_df)\n",
    "    \n",
    "    return vessels_df\n",
    "\n",
    "\n",
    "# Apply the full pipeline\n",
    "vessels_df = preprocess_all(vessels_df)\n",
    "vessels_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e88c96-5a01-4ff4-86b2-3dc3ee64b640",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Note: Probably drop some of these features later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dddf24-363c-40ad-8549-701ea181e2cd",
   "metadata": {},
   "source": [
    "### Wait with schedules for now, as only 67 of schedules are for vessels in test set, and there are few schedules that match with ais, hence, a lot of Nan values. Perhaps use it for common ports or routes, but I feel like we could get this more realiably from the ais data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6efea-80d0-41a6-9a58-b30ebacb14e7",
   "metadata": {},
   "source": [
    "# We now merge vessel data with ais to get a base dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c251ab7-fa45-49e5-bc19-a6d6b14c6ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_ais_with_vessels(ais_data, vessel_data):\n",
    "    \"\"\"\n",
    "    Merges the AIS data with the vessel data on 'vesselId' using a left join.\n",
    "    \n",
    "    Parameters:\n",
    "    - ais_data (pd.DataFrame): DataFrame containing AIS data.\n",
    "    - vessel_data (pd.DataFrame): DataFrame containing vessel data.\n",
    "    \n",
    "    Returns:\n",
    "    - baseDataset (pd.DataFrame): Merged DataFrame containing the AIS data with vessel information.\n",
    "    \"\"\"\n",
    "    # Perform a left merge on 'vesselId'\n",
    "    baseDataset = pd.merge(ais_data, vessel_data, how='left', on='vesselId')\n",
    "    \n",
    "    return baseDataset\n",
    "\n",
    "# Example usage:\n",
    "baseDataset = merge_ais_with_vessels(ais_train_df, vessels_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e4c88c-ba10-42b7-b6b4-e08889e96672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522065 entries, 0 to 1522064\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count    Dtype         \n",
      "---  ------          --------------    -----         \n",
      " 0   time            1522065 non-null  datetime64[ns]\n",
      " 1   cog             1516207 non-null  float64       \n",
      " 2   sog             1522065 non-null  float64       \n",
      " 3   rot             1522065 non-null  float64       \n",
      " 4   heading         1517169 non-null  float64       \n",
      " 5   navstat         1522065 non-null  int64         \n",
      " 6   latitude        1522065 non-null  float64       \n",
      " 7   longitude       1522065 non-null  float64       \n",
      " 8   vesselId        1522065 non-null  object        \n",
      " 9   portId          1520450 non-null  object        \n",
      " 10  portLatitude    1520450 non-null  float64       \n",
      " 11  portLongitude   1520450 non-null  float64       \n",
      " 12  shippingLineId  1522065 non-null  object        \n",
      " 13  CEU             1522065 non-null  int64         \n",
      " 14  DWT             1507116 non-null  float64       \n",
      " 15  GT              1522065 non-null  int64         \n",
      " 16  vesselType      1522065 non-null  category      \n",
      " 17  breadth         1507116 non-null  float64       \n",
      " 18  homePort        1522065 non-null  object        \n",
      " 19  length          1522065 non-null  float64       \n",
      " 20  age             1522065 non-null  int64         \n",
      "dtypes: category(1), datetime64[ns](1), float64(11), int64(4), object(4)\n",
      "memory usage: 233.7+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d74c86-99b2-4b8a-90df-2ac59ba4f8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>cog</th>\n",
       "      <th>sog</th>\n",
       "      <th>rot</th>\n",
       "      <th>heading</th>\n",
       "      <th>navstat</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>vesselId</th>\n",
       "      <th>portId</th>\n",
       "      <th>...</th>\n",
       "      <th>portLongitude</th>\n",
       "      <th>shippingLineId</th>\n",
       "      <th>CEU</th>\n",
       "      <th>DWT</th>\n",
       "      <th>GT</th>\n",
       "      <th>vesselType</th>\n",
       "      <th>breadth</th>\n",
       "      <th>homePort</th>\n",
       "      <th>length</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-12 14:07:47</td>\n",
       "      <td>308.1</td>\n",
       "      <td>17.1</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.50361</td>\n",
       "      <td>77.58340</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "      <td>61d376b393c6feb83e5eb50c</td>\n",
       "      <td>...</td>\n",
       "      <td>80.341111</td>\n",
       "      <td>61a8e672f9cba188601e84ab</td>\n",
       "      <td>6500</td>\n",
       "      <td>21200.0</td>\n",
       "      <td>58684</td>\n",
       "      <td>83.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Norway</td>\n",
       "      <td>199.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-12 14:31:00</td>\n",
       "      <td>307.6</td>\n",
       "      <td>17.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.57302</td>\n",
       "      <td>77.49505</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "      <td>61d376d893c6feb83e5eb546</td>\n",
       "      <td>...</td>\n",
       "      <td>72.885278</td>\n",
       "      <td>61a8e672f9cba188601e84ab</td>\n",
       "      <td>6500</td>\n",
       "      <td>21200.0</td>\n",
       "      <td>58684</td>\n",
       "      <td>83.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Norway</td>\n",
       "      <td>199.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-12 14:57:23</td>\n",
       "      <td>306.8</td>\n",
       "      <td>16.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.65043</td>\n",
       "      <td>77.39404</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "      <td>61d376d893c6feb83e5eb546</td>\n",
       "      <td>...</td>\n",
       "      <td>72.885278</td>\n",
       "      <td>61a8e672f9cba188601e84ab</td>\n",
       "      <td>6500</td>\n",
       "      <td>21200.0</td>\n",
       "      <td>58684</td>\n",
       "      <td>83.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Norway</td>\n",
       "      <td>199.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-12 15:18:48</td>\n",
       "      <td>307.9</td>\n",
       "      <td>16.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.71275</td>\n",
       "      <td>77.31394</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "      <td>61d376d893c6feb83e5eb546</td>\n",
       "      <td>...</td>\n",
       "      <td>72.885278</td>\n",
       "      <td>61a8e672f9cba188601e84ab</td>\n",
       "      <td>6500</td>\n",
       "      <td>21200.0</td>\n",
       "      <td>58684</td>\n",
       "      <td>83.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Norway</td>\n",
       "      <td>199.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-12 15:39:47</td>\n",
       "      <td>307.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.77191</td>\n",
       "      <td>77.23585</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "      <td>61d376d893c6feb83e5eb546</td>\n",
       "      <td>...</td>\n",
       "      <td>72.885278</td>\n",
       "      <td>61a8e672f9cba188601e84ab</td>\n",
       "      <td>6500</td>\n",
       "      <td>21200.0</td>\n",
       "      <td>58684</td>\n",
       "      <td>83.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Norway</td>\n",
       "      <td>199.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time    cog   sog  rot  heading  navstat  latitude  \\\n",
       "0 2024-01-12 14:07:47  308.1  17.1 -6.0    316.0        0   7.50361   \n",
       "1 2024-01-12 14:31:00  307.6  17.3  5.0    313.0        0   7.57302   \n",
       "2 2024-01-12 14:57:23  306.8  16.9  5.0    312.0        0   7.65043   \n",
       "3 2024-01-12 15:18:48  307.9  16.9  6.0    313.0        0   7.71275   \n",
       "4 2024-01-12 15:39:47  307.0  16.3  7.0    313.0        0   7.77191   \n",
       "\n",
       "   longitude                  vesselId                    portId  ...  \\\n",
       "0   77.58340  61e9f38eb937134a3c4bfd8b  61d376b393c6feb83e5eb50c  ...   \n",
       "1   77.49505  61e9f38eb937134a3c4bfd8b  61d376d893c6feb83e5eb546  ...   \n",
       "2   77.39404  61e9f38eb937134a3c4bfd8b  61d376d893c6feb83e5eb546  ...   \n",
       "3   77.31394  61e9f38eb937134a3c4bfd8b  61d376d893c6feb83e5eb546  ...   \n",
       "4   77.23585  61e9f38eb937134a3c4bfd8b  61d376d893c6feb83e5eb546  ...   \n",
       "\n",
       "   portLongitude            shippingLineId   CEU      DWT     GT  vesselType  \\\n",
       "0      80.341111  61a8e672f9cba188601e84ab  6500  21200.0  58684        83.0   \n",
       "1      72.885278  61a8e672f9cba188601e84ab  6500  21200.0  58684        83.0   \n",
       "2      72.885278  61a8e672f9cba188601e84ab  6500  21200.0  58684        83.0   \n",
       "3      72.885278  61a8e672f9cba188601e84ab  6500  21200.0  58684        83.0   \n",
       "4      72.885278  61a8e672f9cba188601e84ab  6500  21200.0  58684        83.0   \n",
       "\n",
       "  breadth  homePort length  age  \n",
       "0    32.0    Norway  199.0   24  \n",
       "1    32.0    Norway  199.0   24  \n",
       "2    32.0    Norway  199.0   24  \n",
       "3    32.0    Norway  199.0   24  \n",
       "4    32.0    Norway  199.0   24  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26af3a-523a-43e8-8ba1-1588c692b2b1",
   "metadata": {},
   "source": [
    "# We now remove anomalies and impute some missing values for this dataset before moving onto feature engineering\n",
    "This includes removing vessels with very few records\n",
    "Flagging unusually large gaps and getting time diff feature\n",
    "Fixing sog values\n",
    "Fixing cog values\n",
    "Fixing heading values\n",
    "Fixing rot values\n",
    "generalizing navstat to fewer more meaningful values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c07fe-7d0b-4d92-a150-7cbb3ef1f373",
   "metadata": {},
   "source": [
    "We check to see the vessels with least records, and whether they are inn test set, and hence must be in dataset! If we find few records we remove them from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c21c0cfb-42b8-4bec-aed0-3d240d3c9091",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    vesselId  record_count  in_test_set\n",
      "0   61e9f3cbb937134a3c4bff09             1        False\n",
      "1   61e9f3adb937134a3c4bfe37            31        False\n",
      "2   61e9f3c6b937134a3c4bfed5           160        False\n",
      "3   61e9f42cb937134a3c4c00f9           191        False\n",
      "4   61e9f45cb937134a3c4c022b           196        False\n",
      "5   61e9f39ab937134a3c4bfdb9           197        False\n",
      "6   61e9f45eb937134a3c4c0235           250        False\n",
      "7   61e9f3bcb937134a3c4bfe91           328         True\n",
      "8   61e9f418b937134a3c4c0077           332        False\n",
      "9   61e9f408b937134a3c4c0023           355        False\n",
      "10  61e9f460b937134a3c4c0243           361        False\n",
      "11  61e9f3f7b937134a3c4bffc5           373        False\n",
      "12  61e9f409b937134a3c4c0027           391        False\n",
      "13  620bf33a718775aca4a81900           401        False\n",
      "14  61e9f423b937134a3c4c00c7           402        False\n",
      "15  61e9f38eb937134a3c4bfd8b           402        False\n",
      "16  61e9f456b937134a3c4c0203           408        False\n",
      "17  61e9f3afb937134a3c4bfe47           416        False\n",
      "18  61e9f3bab937134a3c4bfe8b           451        False\n",
      "19  6326eed6c46d6a20d22ca319           451         True\n"
     ]
    }
   ],
   "source": [
    "# Get the unique vesselIds from the test set\n",
    "vessel_ids_test = set(ais_test_df['vesselId'].unique())\n",
    "\n",
    "# Get the count of records per vesselId in the training set\n",
    "vessel_record_counts = ais_train_df['vesselId'].value_counts()\n",
    "\n",
    "# Get the 10 vessels with the lowest number of records\n",
    "lowest_record_vessels = vessel_record_counts.nsmallest(20)\n",
    "\n",
    "# Check if these vessels are in the test set\n",
    "vessels_in_test = lowest_record_vessels.index.isin(vessel_ids_test)\n",
    "\n",
    "# Combine the results into a dataframe for easy viewing\n",
    "vessels_with_low_records = pd.DataFrame({\n",
    "    'vesselId': lowest_record_vessels.index,\n",
    "    'record_count': lowest_record_vessels.values,\n",
    "    'in_test_set': vessels_in_test\n",
    "})\n",
    "\n",
    "# Display the result\n",
    "print(vessels_with_low_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf33d5f-ec86-4377-ae63-91486ffdf1d8",
   "metadata": {},
   "source": [
    "We remove the two vessels with the lowest records count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d26ec89-b8f5-427c-a5e2-54429fbc4a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of vessel IDs to remove\n",
    "vessels_to_remove = ['61e9f3cbb937134a3c4bff09', '61e9f3adb937134a3c4bfe37']\n",
    "\n",
    "# Remove vessels from the dataset\n",
    "baseDataset = baseDataset[~ais_train_df['vesselId'].isin(vessels_to_remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d39dab-5e35-428f-8a93-12c9e7e1979e",
   "metadata": {},
   "source": [
    "We now create a time_diff column and flag large time gaps column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "985d9b90-e21e-43ea-a170-7804081756cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_time_diff_and_gap_flag(df, time_col='time', vessel_col='vesselId', threshold=2):\n",
    "    \"\"\"\n",
    "    Adds time_diff and large_gap_flag to the dataframe based on vessel-specific statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe containing vesselId and timestamp columns.\n",
    "    time_col (str): The name of the timestamp column.\n",
    "    vessel_col (str): The name of the vessel identifier column.\n",
    "    threshold (float): The number of standard deviations above the mean to flag large time gaps.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The original dataframe with 'time_diff' and 'large_gap_flag' added.\n",
    "    \"\"\"\n",
    "    # Ensure the time column is in datetime format\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    \n",
    "    # Calculate time_diff (difference in time between consecutive entries for each vessel)\n",
    "    df['time_diff'] = df.groupby(vessel_col)[time_col].diff().dt.total_seconds().fillna(0)\n",
    "    \n",
    "    # Group by vesselId to calculate the mean and standard deviation of time_diff for each vessel\n",
    "    vessel_stats = df.groupby(vessel_col)['time_diff'].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    # Merge vessel statistics back to the original dataframe\n",
    "    df = df.merge(vessel_stats, on=vessel_col, how='left')\n",
    "    \n",
    "    # Define a large time gap as threshold * standard deviations above the mean for each vessel\n",
    "    df['large_gap_flag'] = (df['time_diff'] > df['mean'] + threshold * df['std']).astype(int)\n",
    "    \n",
    "    # Drop the mean and std columns (optional, you can keep them if needed)\n",
    "    df = df.drop(columns=['mean', 'std'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9ea4d98-e43d-43f9-8957-dd757331415c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522033 entries, 0 to 1522032\n",
      "Data columns (total 23 columns):\n",
      " #   Column          Non-Null Count    Dtype         \n",
      "---  ------          --------------    -----         \n",
      " 0   time            1522033 non-null  datetime64[ns]\n",
      " 1   cog             1516175 non-null  float64       \n",
      " 2   sog             1522033 non-null  float64       \n",
      " 3   rot             1522033 non-null  float64       \n",
      " 4   heading         1517137 non-null  float64       \n",
      " 5   navstat         1522033 non-null  int64         \n",
      " 6   latitude        1522033 non-null  float64       \n",
      " 7   longitude       1522033 non-null  float64       \n",
      " 8   vesselId        1522033 non-null  object        \n",
      " 9   portId          1520418 non-null  object        \n",
      " 10  portLatitude    1520418 non-null  float64       \n",
      " 11  portLongitude   1520418 non-null  float64       \n",
      " 12  shippingLineId  1522033 non-null  object        \n",
      " 13  CEU             1522033 non-null  int64         \n",
      " 14  DWT             1507084 non-null  float64       \n",
      " 15  GT              1522033 non-null  int64         \n",
      " 16  vesselType      1522033 non-null  category      \n",
      " 17  breadth         1507084 non-null  float64       \n",
      " 18  homePort        1522033 non-null  object        \n",
      " 19  length          1522033 non-null  float64       \n",
      " 20  age             1522033 non-null  int64         \n",
      " 21  time_diff       1522033 non-null  float64       \n",
      " 22  large_gap_flag  1522033 non-null  int64         \n",
      "dtypes: category(1), datetime64[ns](1), float64(12), int64(5), object(4)\n",
      "memory usage: 256.9+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset = add_time_diff_and_gap_flag(baseDataset, time_col='time', vessel_col='vesselId', threshold=3)\n",
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb21a05-af79-4d1d-8132-72c6112808ef",
   "metadata": {},
   "source": [
    "# We now generalize the navstat for easier use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df05755e-5f1d-4616-90fe-5d5354ee736d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_navstat_to_movement(df, navstat_col='navstat'):\n",
    "    \"\"\"\n",
    "    Create a new feature indicating whether the vessel is moving, anchored, or moored \n",
    "    based on the navstat column and then remove the original navstat column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe containing the navstat column.\n",
    "    navstat_col (str): The name of the navstat column.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The dataframe with a new 'vessel_status' column and without the original 'navstat' column.\n",
    "    \"\"\"\n",
    "    # Define the mapping of navstat codes to the movement categories\n",
    "    navstat_mapping = {\n",
    "        0: 'moving',      # Underway using engine\n",
    "        1: 'anchored',    # Anchored\n",
    "        2: 'anchored',      # Not under command\n",
    "        3: 'moving',      # Restricted manoeuverability\n",
    "        4: 'moving',      # Constrained by her draught\n",
    "        5: 'moored',      # Moored\n",
    "        6: 'anchored',    # Aground (considered stationary)\n",
    "        7: 'moving',      # Engaged in fishing\n",
    "        8: 'moving',      # Underway sailing\n",
    "        9: 'unknown',      # Reserved for future use\n",
    "        15: 'unknown'      # Undefined\n",
    "    }\n",
    "\n",
    "    # Create a new column based on the mapping\n",
    "    df['vessel_status'] = df[navstat_col].map(navstat_mapping)\n",
    "\n",
    "    # Remove the original navstat column\n",
    "    df = df.drop(columns=[navstat_col])\n",
    "\n",
    "    return df\n",
    "\n",
    "#Apply the function to create the vessel_status feature and remove the navstat column\n",
    "baseDataset = map_navstat_to_movement(baseDataset, navstat_col='navstat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "295f9ad0-4d22-4883-a1ae-14e82c1a08f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522033 entries, 0 to 1522032\n",
      "Data columns (total 23 columns):\n",
      " #   Column          Non-Null Count    Dtype         \n",
      "---  ------          --------------    -----         \n",
      " 0   time            1522033 non-null  datetime64[ns]\n",
      " 1   cog             1516175 non-null  float64       \n",
      " 2   sog             1522033 non-null  float64       \n",
      " 3   rot             1522033 non-null  float64       \n",
      " 4   heading         1517137 non-null  float64       \n",
      " 5   latitude        1522033 non-null  float64       \n",
      " 6   longitude       1522033 non-null  float64       \n",
      " 7   vesselId        1522033 non-null  object        \n",
      " 8   portId          1520418 non-null  object        \n",
      " 9   portLatitude    1520418 non-null  float64       \n",
      " 10  portLongitude   1520418 non-null  float64       \n",
      " 11  shippingLineId  1522033 non-null  object        \n",
      " 12  CEU             1522033 non-null  int64         \n",
      " 13  DWT             1507084 non-null  float64       \n",
      " 14  GT              1522033 non-null  int64         \n",
      " 15  vesselType      1522033 non-null  category      \n",
      " 16  breadth         1507084 non-null  float64       \n",
      " 17  homePort        1522033 non-null  object        \n",
      " 18  length          1522033 non-null  float64       \n",
      " 19  age             1522033 non-null  int64         \n",
      " 20  time_diff       1522033 non-null  float64       \n",
      " 21  large_gap_flag  1522033 non-null  int64         \n",
      " 22  vessel_status   1522029 non-null  object        \n",
      "dtypes: category(1), datetime64[ns](1), float64(12), int64(4), object(5)\n",
      "memory usage: 256.9+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd79a94-1b48-46e5-8997-6e36c55b2a4c",
   "metadata": {},
   "source": [
    "# We now define near land feature which helps with imputing remaining time-dependent values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94b54d-c09f-4b29-bbf8-b10a6d7c2374",
   "metadata": {},
   "source": [
    "#### Near Land feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55a1875f-edc2-45c8-ac5b-e507f85f8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_LAND_PATH = os.path.join(current_dir, '../../original_data/ne_10m_land.zip') # Path to the land zip file\n",
    "MAP_OCEAN_PATH = os.path.join(current_dir, '../../original_data/ne_10m_ocean.zip') # Path to the ocean zip file\n",
    "land_world = gpd.read_file(MAP_LAND_PATH)\n",
    "ocean_world = gpd.read_file(MAP_OCEAN_PATH)\n",
    "\n",
    "def classify_near_land(data: pd.DataFrame):  \n",
    "    \"\"\"\n",
    "    Classify vessels as being near land by checking if they fall within land polygons.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The vessel data containing latitude and longitude.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with a new 'near_land' feature.\n",
    "    \"\"\"\n",
    "    # Create a GeoSeries of points from the vessel's longitude and latitude\n",
    "    _data = data.copy()\n",
    "    _data['geometry'] = gpd.points_from_xy(_data['longitude'], _data['latitude'], crs=\"EPSG:4326\")\n",
    "    gdf = gpd.GeoDataFrame(_data, geometry='geometry')\n",
    "\n",
    "    # Perform spatial join to classify vessels as near land or not\n",
    "    gdf_with_land = gpd.sjoin(gdf, land_world, how='left', predicate='within')\n",
    "    \n",
    "    # If a vessel was classified as being \"on land\", we set \"near_land\" to True\n",
    "    gdf_with_land['near_land'] = gdf_with_land['index_right'].isna() == False\n",
    "\n",
    "    return gdf_with_land\n",
    "\n",
    "# Classify vessels as being near land or not\n",
    "gdf_with_near_land = classify_near_land(baseDataset)\n",
    "\n",
    "# Convert GeoDataFrame to a regular DataFrame (if needed) and drop geometry\n",
    "baseDataset = pd.DataFrame(gdf_with_near_land.drop(columns=['geometry', 'index_right', 'featurecla', 'scalerank', 'min_zoom']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37810955-9cad-45d0-9849-b2ebf42b8a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1522033 entries, 0 to 1522032\n",
      "Data columns (total 24 columns):\n",
      " #   Column          Non-Null Count    Dtype         \n",
      "---  ------          --------------    -----         \n",
      " 0   time            1522033 non-null  datetime64[ns]\n",
      " 1   cog             1516175 non-null  float64       \n",
      " 2   sog             1522033 non-null  float64       \n",
      " 3   rot             1522033 non-null  float64       \n",
      " 4   heading         1517137 non-null  float64       \n",
      " 5   latitude        1522033 non-null  float64       \n",
      " 6   longitude       1522033 non-null  float64       \n",
      " 7   vesselId        1522033 non-null  object        \n",
      " 8   portId          1520418 non-null  object        \n",
      " 9   portLatitude    1520418 non-null  float64       \n",
      " 10  portLongitude   1520418 non-null  float64       \n",
      " 11  shippingLineId  1522033 non-null  object        \n",
      " 12  CEU             1522033 non-null  int64         \n",
      " 13  DWT             1507084 non-null  float64       \n",
      " 14  GT              1522033 non-null  int64         \n",
      " 15  vesselType      1522033 non-null  category      \n",
      " 16  breadth         1507084 non-null  float64       \n",
      " 17  homePort        1522033 non-null  object        \n",
      " 18  length          1522033 non-null  float64       \n",
      " 19  age             1522033 non-null  int64         \n",
      " 20  time_diff       1522033 non-null  float64       \n",
      " 21  large_gap_flag  1522033 non-null  int64         \n",
      " 22  vessel_status   1522029 non-null  object        \n",
      " 23  near_land       1522033 non-null  bool          \n",
      "dtypes: bool(1), category(1), datetime64[ns](1), float64(12), int64(4), object(5)\n",
      "memory usage: 270.0+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94053d58-7d04-46c7-8676-0fca225237e4",
   "metadata": {},
   "source": [
    "### We also create a port distance feature for help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f246cc4f-69d4-4b78-abe6-8f5a06c603c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_np(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Vectorized Haversine formula to calculate distance between two lat/lon points in kilometers.\n",
    "    \"\"\"\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert degrees to radians\n",
    "    lat1_rad, lon1_rad = np.radians(lat1), np.radians(lon1)\n",
    "    lat2_rad, lon2_rad = np.radians(lat2), np.radians(lon2)\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    # Calculate distance\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def calculate_distance_to_port_optimized(df):\n",
    "    \"\"\"\n",
    "    Calculate the distance from the vessel's current location to the port based on existing port latitude and longitude.\n",
    "    Optimized for large datasets using the Haversine formula and handles invalid/missing port information.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe containing vessel data (latitude, longitude, portLatitude, portLongitude, portId).\n",
    "    \n",
    "    Returns:\n",
    "    - df (pd.DataFrame): The modified dataframe with a new 'distance_to_port' feature.\n",
    "    \"\"\"\n",
    "    # Create a mask for valid port entries (i.e., portId is not NaN and port coordinates are valid)\n",
    "    valid_port_mask = df['portId'].notna() & df['portLatitude'].between(-90, 90) & df['portLongitude'].between(-180, 180)\n",
    "    \n",
    "    # Initialize the 'distance_to_port' column with NaN for all rows\n",
    "    df['distance_to_port'] = np.nan\n",
    "    \n",
    "    # Apply Haversine formula on valid rows\n",
    "    valid_rows = df[valid_port_mask]\n",
    "    \n",
    "    df.loc[valid_port_mask, 'distance_to_port'] = haversine_np(\n",
    "        valid_rows['latitude'].values,\n",
    "        valid_rows['longitude'].values,\n",
    "        valid_rows['portLatitude'].values,\n",
    "        valid_rows['portLongitude'].values\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "baseDataset = calculate_distance_to_port_optimized(baseDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c335df1-49cc-4458-adcb-d33195101d53",
   "metadata": {},
   "source": [
    "### Imputing and clarifying vessel status!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0698c63f-d285-497b-b7e1-ffd3de769f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_vessel_status(df, distance_threshold=0.1):\n",
    "    # Ensure vessel_status is a string\n",
    "    df['vessel_status'] = df['vessel_status'].astype(str)\n",
    "\n",
    "    # Step 1: Identify inconsistencies\n",
    "    inconsistent_status = (\n",
    "        ((df['vessel_status'] == 'moored') & (df['sog'] > 0.1)) |\n",
    "        ((df['vessel_status'] == 'moored') & (df['near_land'] == 0)) |\n",
    "        ((df['vessel_status'] == 'anchored') & (df['sog'] > 0.1)) |\n",
    "        ((df['vessel_status'] == 'moving') & (df['sog'] == 0))\n",
    "    )\n",
    "    df.loc[inconsistent_status, 'vessel_status'] = 'unknown'\n",
    "\n",
    "    # Step 2: Impute 'unknown' statuses\n",
    "    mask_unknown = df['vessel_status'] == 'unknown'\n",
    "\n",
    "    # Impute based on proximity and SOG\n",
    "    df.loc[\n",
    "        mask_unknown & (df['sog'] <= 0.1) & (\n",
    "            (df['near_land']) | (df['distance_to_port'] <= distance_threshold)\n",
    "        ),\n",
    "        'vessel_status'\n",
    "    ] = 'moored'\n",
    "\n",
    "    df.loc[\n",
    "        mask_unknown & (df['sog'] != 0),\n",
    "        'vessel_status'\n",
    "    ] = 'moving'\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to your dataset\n",
    "baseDataset = impute_vessel_status(baseDataset)\n",
    "\n",
    "\n",
    "# We use expert knowledge to infer anchored on unknown vessels that have sog = 0\n",
    "def classify_unknown_as_anchored(df, distance_threshold=10, movement_radius=5):\n",
    "    \"\"\"\n",
    "    Classifies 'unknown' vessel statuses as 'anchored' based on vessel behavior.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing vessel data.\n",
    "    - distance_threshold (float): Maximum distance to port (in km) to consider a vessel for anchoring.\n",
    "    - movement_radius (float): Radius within which subsequent rows must be stationary to confirm anchoring.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Updated DataFrame with reclassified vessel statuses.\n",
    "    \"\"\"\n",
    "    # Step 1: Filter unknown statuses with sog = 0 and within specified distance to port\n",
    "    unknown_near_port = df[(df['vessel_status'] == 'unknown') & \n",
    "                           (df['sog'] == 0) & \n",
    "                           (df['distance_to_port'] <= distance_threshold)]\n",
    "    \n",
    "    # Step 2: Iterate through filtered rows to check subsequent behavior\n",
    "    for idx in unknown_near_port.index:\n",
    "        current_row = df.loc[idx]\n",
    "        \n",
    "        # Check the next row (if available) for movement or mooring status\n",
    "        if idx + 1 in df.index:\n",
    "            next_row = df.loc[idx + 1]\n",
    "\n",
    "            # Conditions for reclassifying as 'anchored'\n",
    "            # - If the next row shows movement or a close subsequent stationary position\n",
    "            if ((next_row['sog'] > 0) or  # Vessel starts moving\n",
    "                (next_row['vessel_status'] == 'moored') or  # Vessel moors next\n",
    "                ((next_row['sog'] == 0) and  # Still stationary\n",
    "                 (next_row['distance_to_port'] <= movement_radius))):  # Within close range\n",
    "\n",
    "                # Set current unknown status to 'anchored'\n",
    "                df.at[idx, 'vessel_status'] = 'anchored'\n",
    "\n",
    "    return df\n",
    "\n",
    "baseDataset = classify_unknown_as_anchored(baseDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f81dee-7c26-43d8-b004-2dd9e7969223",
   "metadata": {},
   "source": [
    "# We now find outliers for sog and impute new values on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99a16b55-1241-4974-b81f-1f10a95d09cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134015/1676822380.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('vesselId').apply(interpolate_sog).reset_index(drop=True)\n",
      "/tmp/ipykernel_134015/1676822380.py:48: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('vesselId').apply(backfill_sog).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "def handle_sog_anomalies(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure 'sog' is a float\n",
    "    df['sog'] = df['sog'].astype(float)\n",
    "    \n",
    "    # Calculate vessel-specific time_diff thresholds\n",
    "    vessel_time_stats = df.groupby('vesselId')['time_diff'].agg(['mean', 'std']).reset_index()\n",
    "    vessel_time_stats['threshold'] = vessel_time_stats['mean'] + vessel_time_stats['std']\n",
    "\n",
    "    # Define the moving mask to filter for 'moving' or 'unknown' vessel status\n",
    "    moving_mask = df['vessel_status'].isin(['moving', 'unknown'])\n",
    "\n",
    "    # Compute vessel-specific 'sog' statistics for moving status\n",
    "    vessel_sog_stats = df.loc[moving_mask].groupby('vesselId')['sog'].agg(['median', 'mean', 'std'])\n",
    "    vessel_sog_stats = vessel_sog_stats.rename(columns={'median': 'sog_median', 'mean': 'sog_mean', 'std': 'sog_std'})\n",
    "    \n",
    "    # Merge thresholds and stats back into the main DataFrame\n",
    "    df = df.merge(vessel_time_stats[['vesselId', 'threshold']], on='vesselId', how='left')\n",
    "    df = df.merge(vessel_sog_stats, on='vesselId', how='left')\n",
    "    \n",
    "    # Step 1: Identify anomalies (sog over threshold)\n",
    "    sog_upper_limit = 2* df['sog_mean'] + df['sog_std']\n",
    "    df.loc[df['sog'] > sog_upper_limit, 'sog'] = np.nan  # Set anomalies to NaN\n",
    "    \n",
    "    # Step 2: Interpolate sog where time_diff is within threshold\n",
    "    def interpolate_sog(group):\n",
    "        threshold = group['threshold'].iloc[0]\n",
    "        # Only interpolate where time_diff is within threshold\n",
    "        mask = group['time_diff'] <= threshold\n",
    "        group.loc[mask, 'sog'] = group.loc[mask, 'sog'].interpolate(method='linear', limit_direction='both')\n",
    "        return group\n",
    "    \n",
    "    df = df.groupby('vesselId').apply(interpolate_sog).reset_index(drop=True)\n",
    "    \n",
    "    # Step 3: Backfill sog where appropriate\n",
    "    def backfill_sog(group):\n",
    "        threshold = group['threshold'].iloc[0]\n",
    "        for idx in group.index:\n",
    "            if pd.isnull(group.loc[idx, 'sog']):\n",
    "                # Check if the next row has time_diff within threshold and status is 'moving'\n",
    "                next_time_diff = group['time_diff'].shift(-1)\n",
    "                next_status = group['vessel_status'].shift(-1)\n",
    "                if (next_time_diff[idx] <= threshold) and (next_status[idx] == 'moving'):\n",
    "                    group.loc[idx, 'sog'] = group['sog'].shift(-1)[idx]\n",
    "        return group\n",
    "    \n",
    "    df = df.groupby('vesselId').apply(backfill_sog).reset_index(drop=True)\n",
    "    \n",
    "    # Clean up\n",
    "    df.drop(columns=['threshold', 'sog_median', 'sog_mean', 'sog_std'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "baseDataset = handle_sog_anomalies(baseDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2212a97-4c7d-40d9-8421-60f06d2f9437",
   "metadata": {},
   "source": [
    "# We now fix cog values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7d3a201-daaf-45a4-b17c-511ba7fc7bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134015/4048450493.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('vesselId').apply(interpolate_cog).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "def handle_cog_interpolation(df, cog_col='cog', vessel_status_col='vessel_status'):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure COG is a float type\n",
    "    df[cog_col] = df[cog_col].astype(float)\n",
    "    \n",
    "    # Set COG to 0 for moored vessels (stationary), as course is not meaningful\n",
    "    df.loc[df[vessel_status_col] == 'moored', cog_col] = 0\n",
    "    \n",
    "    # Calculate vessel-specific time_diff thresholds\n",
    "    vessel_time_stats = df.groupby('vesselId')['time_diff'].agg(['mean', 'std']).reset_index()\n",
    "    vessel_time_stats['threshold'] = vessel_time_stats['mean'] + vessel_time_stats['std']\n",
    "    \n",
    "    # Merge thresholds back into the main DataFrame\n",
    "    df = df.merge(vessel_time_stats[['vesselId', 'threshold']], on='vesselId', how='left')\n",
    "    \n",
    "    # Step 1: Interpolate COG where time_diff is within threshold\n",
    "    def interpolate_cog(group):\n",
    "        threshold = group['threshold'].iloc[0]\n",
    "        # Only interpolate where time_diff is within threshold\n",
    "        mask = group['time_diff'] <= threshold\n",
    "        group.loc[mask, cog_col] = group.loc[mask, cog_col].interpolate(method='linear', limit_direction='both')\n",
    "        return group\n",
    "    \n",
    "    df = df.groupby('vesselId').apply(interpolate_cog).reset_index(drop=True)\n",
    "    \n",
    "    #Clean up\n",
    "    df.drop(columns=['threshold'], inplace =True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to your dataset\n",
    "baseDataset = handle_cog_interpolation(baseDataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d10c65f7-db39-4739-9a9a-e8fb7be64841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522033 entries, 0 to 1522032\n",
      "Data columns (total 25 columns):\n",
      " #   Column            Non-Null Count    Dtype         \n",
      "---  ------            --------------    -----         \n",
      " 0   time              1522033 non-null  datetime64[ns]\n",
      " 1   cog               1522029 non-null  float64       \n",
      " 2   sog               1522031 non-null  float64       \n",
      " 3   rot               1522033 non-null  float64       \n",
      " 4   heading           1517137 non-null  float64       \n",
      " 5   latitude          1522033 non-null  float64       \n",
      " 6   longitude         1522033 non-null  float64       \n",
      " 7   vesselId          1522033 non-null  object        \n",
      " 8   portId            1520418 non-null  object        \n",
      " 9   portLatitude      1520418 non-null  float64       \n",
      " 10  portLongitude     1520418 non-null  float64       \n",
      " 11  shippingLineId    1522033 non-null  object        \n",
      " 12  CEU               1522033 non-null  int64         \n",
      " 13  DWT               1507084 non-null  float64       \n",
      " 14  GT                1522033 non-null  int64         \n",
      " 15  vesselType        1522033 non-null  category      \n",
      " 16  breadth           1507084 non-null  float64       \n",
      " 17  homePort          1522033 non-null  object        \n",
      " 18  length            1522033 non-null  float64       \n",
      " 19  age               1522033 non-null  int64         \n",
      " 20  time_diff         1522033 non-null  float64       \n",
      " 21  large_gap_flag    1522033 non-null  int64         \n",
      " 22  vessel_status     1522033 non-null  object        \n",
      " 23  near_land         1522033 non-null  bool          \n",
      " 24  distance_to_port  1520418 non-null  float64       \n",
      "dtypes: bool(1), category(1), datetime64[ns](1), float64(13), int64(4), object(5)\n",
      "memory usage: 270.0+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6342d2-be0b-409c-a8a4-f8166bb2a393",
   "metadata": {},
   "source": [
    "# We now fix heading values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "929aafe4-2d44-4f02-b481-3a7e42c10c01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134015/1379117370.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('vesselId').apply(interpolate_heading).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "def handle_heading_interpolation(df, heading_col='heading', vessel_status_col='vessel_status'):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure heading is a float type\n",
    "    df[heading_col] = df[heading_col].astype(float)\n",
    "    \n",
    "    # Set heading to 0 for moored vessels (stationary), as direction is not meaningful\n",
    "    df.loc[df[vessel_status_col] == 'moored', heading_col] = 0\n",
    "    \n",
    "    # Calculate vessel-specific time_diff thresholds\n",
    "    vessel_time_stats = df.groupby('vesselId')['time_diff'].agg(['mean', 'std']).reset_index()\n",
    "    vessel_time_stats['threshold'] = vessel_time_stats['mean'] + vessel_time_stats['std']\n",
    "    \n",
    "    # Merge thresholds back into the main DataFrame\n",
    "    df = df.merge(vessel_time_stats[['vesselId', 'threshold']], on='vesselId', how='left')\n",
    "    \n",
    "    # Step 1: Interpolate heading where time_diff is within threshold\n",
    "    def interpolate_heading(group):\n",
    "        threshold = group['threshold'].iloc[0]\n",
    "        # Only interpolate where time_diff is within threshold\n",
    "        mask = group['time_diff'] <= threshold\n",
    "        group.loc[mask, heading_col] = group.loc[mask, heading_col].interpolate(method='linear', limit_direction='both')\n",
    "        return group\n",
    "    \n",
    "    df = df.groupby('vesselId').apply(interpolate_heading).reset_index(drop=True)\n",
    "    \n",
    "    #Clean up\n",
    "    df.drop(columns=['threshold'], inplace =True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "baseDataset = handle_heading_interpolation(baseDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d55369c4-0ed6-4d22-955a-e9c49e7b4615",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522033 entries, 0 to 1522032\n",
      "Data columns (total 25 columns):\n",
      " #   Column            Non-Null Count    Dtype         \n",
      "---  ------            --------------    -----         \n",
      " 0   time              1522033 non-null  datetime64[ns]\n",
      " 1   cog               1522029 non-null  float64       \n",
      " 2   sog               1522031 non-null  float64       \n",
      " 3   rot               1522033 non-null  float64       \n",
      " 4   heading           1521545 non-null  float64       \n",
      " 5   latitude          1522033 non-null  float64       \n",
      " 6   longitude         1522033 non-null  float64       \n",
      " 7   vesselId          1522033 non-null  object        \n",
      " 8   portId            1520418 non-null  object        \n",
      " 9   portLatitude      1520418 non-null  float64       \n",
      " 10  portLongitude     1520418 non-null  float64       \n",
      " 11  shippingLineId    1522033 non-null  object        \n",
      " 12  CEU               1522033 non-null  int64         \n",
      " 13  DWT               1507084 non-null  float64       \n",
      " 14  GT                1522033 non-null  int64         \n",
      " 15  vesselType        1522033 non-null  category      \n",
      " 16  breadth           1507084 non-null  float64       \n",
      " 17  homePort          1522033 non-null  object        \n",
      " 18  length            1522033 non-null  float64       \n",
      " 19  age               1522033 non-null  int64         \n",
      " 20  time_diff         1522033 non-null  float64       \n",
      " 21  large_gap_flag    1522033 non-null  int64         \n",
      " 22  vessel_status     1522033 non-null  object        \n",
      " 23  near_land         1522033 non-null  bool          \n",
      " 24  distance_to_port  1520418 non-null  float64       \n",
      "dtypes: bool(1), category(1), datetime64[ns](1), float64(13), int64(4), object(5)\n",
      "memory usage: 270.0+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c89d5-c3a4-4fb7-aa45-917fbb0346a2",
   "metadata": {},
   "source": [
    "## We now fix rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd63971a-f60c-475e-bfde-34c666364b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_rot_for_moored_vessels(df):\n",
    "    \"\"\"\n",
    "    Set ROT (Rate of Turn) to 0 for all moored vessels.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe containing vessel_behaviour and rot columns.\n",
    "    rot_col (str): The name of the ROT column.\n",
    "    behaviour_col (str): The name of the vessel behaviour column (e.g., 'moored', 'moving').\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The dataframe with ROT set to 0 for moored vessels.\n",
    "    \"\"\"\n",
    "    # Set ROT to 0 where vessel_behaviour is 'moored'\n",
    "    df.loc[df['vessel_status'] == 'moored', 'rot'] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "baseDataset = handle_rot_for_moored_vessels(baseDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97d7c2c6-ee72-4f66-b5f5-31859baaa10d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522033 entries, 0 to 1522032\n",
      "Data columns (total 25 columns):\n",
      " #   Column            Non-Null Count    Dtype         \n",
      "---  ------            --------------    -----         \n",
      " 0   time              1522033 non-null  datetime64[ns]\n",
      " 1   cog               1522029 non-null  float64       \n",
      " 2   sog               1522031 non-null  float64       \n",
      " 3   rot               1522033 non-null  float64       \n",
      " 4   heading           1521545 non-null  float64       \n",
      " 5   latitude          1522033 non-null  float64       \n",
      " 6   longitude         1522033 non-null  float64       \n",
      " 7   vesselId          1522033 non-null  object        \n",
      " 8   portId            1520418 non-null  object        \n",
      " 9   portLatitude      1520418 non-null  float64       \n",
      " 10  portLongitude     1520418 non-null  float64       \n",
      " 11  shippingLineId    1522033 non-null  object        \n",
      " 12  CEU               1522033 non-null  int64         \n",
      " 13  DWT               1507084 non-null  float64       \n",
      " 14  GT                1522033 non-null  int64         \n",
      " 15  vesselType        1522033 non-null  category      \n",
      " 16  breadth           1507084 non-null  float64       \n",
      " 17  homePort          1522033 non-null  object        \n",
      " 18  length            1522033 non-null  float64       \n",
      " 19  age               1522033 non-null  int64         \n",
      " 20  time_diff         1522033 non-null  float64       \n",
      " 21  large_gap_flag    1522033 non-null  int64         \n",
      " 22  vessel_status     1522033 non-null  object        \n",
      " 23  near_land         1522033 non-null  bool          \n",
      " 24  distance_to_port  1520418 non-null  float64       \n",
      "dtypes: bool(1), category(1), datetime64[ns](1), float64(13), int64(4), object(5)\n",
      "memory usage: 270.0+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ba0ed-8c1f-42b5-838d-ebd7bc4c799e",
   "metadata": {},
   "source": [
    "# We now move onto feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b5fc32-d4ad-467b-9111-a35a79a8064e",
   "metadata": {},
   "source": [
    "## Spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e574a8d-512f-4f03-a773-90d5c8774b54",
   "metadata": {},
   "source": [
    "#### Clustering common routes feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c766251-5c57-4169-adab-8b3961850c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10% of the data for clustering\n",
    "df_sample = baseDataset.sample(frac=0.1, random_state=42)\n",
    "X_sample = df_sample[['latitude', 'longitude']].values\n",
    "\n",
    "# Standardize and apply DBSCAN to the sample\n",
    "scaler = StandardScaler()\n",
    "X_scaled_sample = scaler.fit_transform(X_sample)\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=40)\n",
    "dbscan.fit(X_scaled_sample)\n",
    "\n",
    "# Get the cluster labels from DBSCAN\n",
    "labels_sample = dbscan.labels_\n",
    "\n",
    "# Check for noise (-1 indicates noise points)\n",
    "n_noise = list(labels_sample).count(-1)\n",
    "print(f'Number of noise points: {n_noise}')\n",
    "\n",
    "# Analyze the resulting clusters\n",
    "n_clusters = len(set(labels_sample)) - (1 if -1 in labels_sample else 0)\n",
    "print(f'Number of clusters found: {n_clusters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b4187-0c1f-4a56-8bf6-4714aea8643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_clusters(df, sample_df, cluster_labels, cluster_centers, eps, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Performs clustering-based feature engineering, including time spent in cluster and cluster transitions.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The full dataframe with vessel data.\n",
    "    - sample_df (pd.DataFrame): The sampled dataframe used for clustering.\n",
    "    - cluster_labels (np.array): Array of cluster labels from DBSCAN for the sampled data.\n",
    "    - cluster_centers (np.array): Array of cluster centroids from DBSCAN.\n",
    "    - eps (float): The epsilon value used in DBSCAN for maximum distance between points to form a cluster.\n",
    "    - batch_size (int): Number of points to process in each batch to avoid memory errors.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pd.DataFrame): The modified dataframe with new features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Assign the cluster labels for the points in the sample\n",
    "    df.loc[sample_df.index, 'cluster_label'] = cluster_labels\n",
    "\n",
    "    # Step 2: Use KDTree for efficient nearest-neighbor search for non-sampled points\n",
    "    tree = KDTree(cluster_centers)  # Build KDTree from cluster centroids\n",
    "\n",
    "    # Step 3: For non-sampled points, assign them to the nearest cluster if within 'eps'\n",
    "    non_sampled_mask = df['cluster_label'].isna()\n",
    "    non_sampled_points = df.loc[non_sampled_mask, ['latitude', 'longitude']].values\n",
    "    \n",
    "    if non_sampled_points.shape[0] > 0:\n",
    "        # Initialize arrays to store results\n",
    "        nearest_centroids = np.full(non_sampled_points.shape[0], -1)\n",
    "        nearest_distances = np.full(non_sampled_points.shape[0], np.inf)\n",
    "        \n",
    "        # Process in batches to avoid memory overload\n",
    "        for i in range(0, non_sampled_points.shape[0], batch_size):\n",
    "            batch = non_sampled_points[i:i+batch_size]\n",
    "            \n",
    "            # Query KDTree for nearest centroids and distances for the current batch\n",
    "            batch_distances, batch_nearest_centroids = tree.query(batch)\n",
    "            \n",
    "            # Assign points to the nearest cluster if the distance is within 'eps'\n",
    "            within_eps = batch_distances <= eps\n",
    "            nearest_centroids[i:i+batch_size][within_eps] = batch_nearest_centroids[within_eps]\n",
    "            nearest_distances[i:i+batch_size][within_eps] = batch_distances[within_eps]\n",
    "        \n",
    "        # Assign the nearest centroids back to the dataframe\n",
    "        df.loc[non_sampled_mask, 'cluster_label'] = nearest_centroids\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "baseDataset = feature_engineering_clusters(baseDataset, df_sample, dbscan.labels_, dbscan.components_, eps=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583cbde-fa7e-4d2b-9aa6-f7af9602f8d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb20a0-0de2-4db2-802d-78facf58dd1c",
   "metadata": {},
   "source": [
    "### Temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb13824-9379-4d85-a4d0-8475a2c5e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df, lag_columns, lags):\n",
    "    \"\"\"\n",
    "    Create lag features for the specified columns and lag values.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe containing spatial and time-dependent data.\n",
    "    - lag_columns (list of str): List of columns for which to create lag features.\n",
    "    - lags (list of int): List of lag intervals to create.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pd.DataFrame): The dataframe with new lag features.\n",
    "    \"\"\"\n",
    "    for col in lag_columns:\n",
    "        for lag in lags:\n",
    "            # Create lagged features, keeping NaNs for missing values\n",
    "            df[f'lag_{lag}_{col}'] = df.groupby('vesselId')[col].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_time_dependent_spatial_features(df):\n",
    "    \"\"\"\n",
    "    Create lag, distance, and bearing features based on spatial and movement-related data.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe containing time-dependent spatial data (latitude, longitude, cog, sog, heading).\n",
    "    \n",
    "    Returns:\n",
    "    - df (pd.DataFrame): The dataframe with new spatial features.\n",
    "    \"\"\"\n",
    "    # Ensure the 'time' column is sorted by vesselId and time\n",
    "    df = df.sort_values(by=['vesselId', 'time'])\n",
    "    \n",
    "    # Lag Feature Creation\n",
    "    df = create_lag_features(df, lag_columns=['latitude', 'longitude'], lags=[1, 2, 3, 4, 5])\n",
    "    df = create_lag_features(df, lag_columns=['cog', 'sog', 'heading'], lags=[1, 2])\n",
    "    \n",
    "    # Calculate the distance between consecutive points using the Haversine formula\n",
    "    def haversine_np(lat1, lon1, lat2, lon2):\n",
    "        R = 6371.0  # Radius of the Earth in kilometers\n",
    "        lat1_rad, lon1_rad = np.radians(lat1), np.radians(lon1)\n",
    "        lat2_rad, lon2_rad = np.radians(lat2), np.radians(lon2)\n",
    "        dlat = lat2_rad - lat1_rad\n",
    "        dlon = lon2_rad - lon1_rad\n",
    "        a = np.sin(dlat / 2) ** 2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2) ** 2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        return R * c\n",
    "    \n",
    "    df['distance_traveled'] = haversine_np(df['lag_1_latitude'], df['lag_1_longitude'], df['latitude'], df['longitude'])\n",
    "    \n",
    "    # Calculate Speed Difference (change in sog between consecutive points)\n",
    "    df['speed_change'] = df['sog'] - df['lag_1_sog']\n",
    "    \n",
    "    # Calculate Bearing between consecutive points\n",
    "    def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "        lat1_rad = np.radians(lat1)\n",
    "        lat2_rad = np.radians(lat2)\n",
    "        dlon_rad = np.radians(lon2 - lon1)\n",
    "        \n",
    "        x = np.sin(dlon_rad) * np.cos(lat2_rad)\n",
    "        y = np.cos(lat1_rad) * np.sin(lat2_rad) - np.sin(lat1_rad) * np.cos(lat2_rad) * np.cos(dlon_rad)\n",
    "        initial_bearing = np.degrees(np.arctan2(x, y))\n",
    "        return (initial_bearing + 360) % 360  # Normalize to [0, 360] degrees\n",
    "    \n",
    "    df['bearing'] = calculate_bearing(df['lag_1_latitude'], df['lag_1_longitude'], df['latitude'], df['longitude'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "baseDataset = create_time_dependent_spatial_features(baseDataset)\n",
    "\n",
    "\n",
    "def create_rolling_features(df, columns, windows):\n",
    "    \"\"\"\n",
    "    Create rolling statistics (mean and standard deviation) for specified columns over given time windows.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe containing spatial and time-dependent data.\n",
    "    - columns (list of str): List of columns to calculate rolling statistics for.\n",
    "    - windows (list of int): List of window sizes for the rolling statistics.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pd.DataFrame): The dataframe with new rolling statistics features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterate over each column and each window size to create rolling features\n",
    "    for col in columns:\n",
    "        for window in windows:\n",
    "            # Create rolling mean feature\n",
    "            df[f'{col}_rolling_mean_{window}'] = (\n",
    "                df.groupby('vesselId')[col]\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .mean()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "            \n",
    "            # Create rolling std feature\n",
    "            df[f'{col}_rolling_std_{window}'] = (\n",
    "                df.groupby('vesselId')[col]\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .std()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "    \n",
    "    return df\n",
    "\n",
    "baseDataset = create_rolling_features(baseDataset, columns=['sog', 'cog', 'heading'], windows=[3, 5])\n",
    "\n",
    "def extract_time_features(df):\n",
    "    \"\"\"\n",
    "    Extract hour, day, and day of the week from a datetime column 'time'.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe containing a datetime 'time' column.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pd.DataFrame): The dataframe with new time-based features.\n",
    "    \"\"\"\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['day'] = df['time'].dt.day\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "    return df\n",
    "\n",
    "baseDataset = extract_time_features(baseDataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be870f-3c50-429c-bb3a-1a80fa60f15f",
   "metadata": {},
   "source": [
    "#### Port features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70871f88-43c0-4fc7-b16f-82cb6c49353a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_port_frequency(df):\n",
    "    \"\"\"\n",
    "    Calculate how frequently each vessel visits a specific port based on portId.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe containing vessel data with portId.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pd.DataFrame): The modified dataframe with a new 'port_frequency' feature.\n",
    "    \"\"\"\n",
    "    # Step 1: Group by vesselId and portId, and calculate the frequency of each port visit\n",
    "    port_frequency = df.groupby(['vesselId', 'portId']).size().reset_index(name='port_frequency')\n",
    "\n",
    "    # Step 2: Merge the port frequency back to the original dataframe\n",
    "    df = df.merge(port_frequency, on=['vesselId', 'portId'], how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Calculate port frequency\n",
    "baseDataset = calculate_port_frequency(baseDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d151016-df31-4a01-83bc-9f5087a3622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_previous_port_features_optimized(df):\n",
    "    # Sort by vesselId and time to ensure sequential order within each vessel\n",
    "    df = df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "    # Shift port info by one row to get the \"previous\" port details\n",
    "    df['previous_portId'] = df.groupby('vesselId')['portId'].shift(1)\n",
    "    df['previous_port_lat'] = df.groupby('vesselId')['portLatitude'].shift(1)\n",
    "    df['previous_port_lon'] = df.groupby('vesselId')['portLongitude'].shift(1)\n",
    "\n",
    "    # Identify rows where the portId has changed (within each vessel)\n",
    "    change_mask = df.groupby('vesselId')['portId'].apply(lambda x: x != x.shift(1))\n",
    "\n",
    "    # Only retain previous port info where there was a change in portId\n",
    "    df['previous_portId'] = np.where(change_mask, df['previous_portId'], np.nan)\n",
    "    df['previous_port_lat'] = np.where(change_mask, df['previous_port_lat'], np.nan)\n",
    "    df['previous_port_lon'] = np.where(change_mask, df['previous_port_lon'], np.nan)\n",
    "\n",
    "    # Forward fill previous port details across rows to maintain last known previous port until a new one is found\n",
    "    df[['previous_portId', 'previous_port_lat', 'previous_port_lon']] = df.groupby('vesselId')[['previous_portId', 'previous_port_lat', 'previous_port_lon']].ffill()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "baseDataset = add_previous_port_features_optimized(baseDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f676b0b-65c0-4746-9c85-9f8e13b21dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a5898f-8e30-4883-a4a4-cfd4bfa8f2a6",
   "metadata": {},
   "source": [
    "# We try training indivudal tabular predictor models: LightGBM, XGBoost and CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55694c64-3b0e-4cf8-ba14-e56c44b50ee7",
   "metadata": {},
   "source": [
    "### Data type conversion: These models only work for specific types, so we fix that first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc262aa-f3ef-416b-8b72-78b2430e6e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_dataframe_types(df: pd.DataFrame, encode_categorical: bool = False) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Converts DataFrame columns to appropriate data types for XGBoost and CatBoost models.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with original data types.\n",
    "    - encode_categorical (bool): If True, perform label encoding on categorical features.\n",
    "\n",
    "    Returns:\n",
    "    - df_converted (pd.DataFrame): DataFrame with converted data types.\n",
    "    - categorical_features (list): List of column names that are categorical features (empty if encoded).\n",
    "    \"\"\"\n",
    "    df_converted = df.copy()\n",
    "\n",
    "    # 1. Convert datetime columns to numerical features\n",
    "    datetime_cols = df_converted.select_dtypes(include=['datetime64[ns]']).columns.tolist()\n",
    "    for col in datetime_cols:\n",
    "        # Convert datetime to int\n",
    "        df_converted[f'{col}'] = df_converted[col].astype(np.int64) // 10**9  # Convert to seconds\n",
    "\n",
    "    # 2. Convert object columns to category dtype\n",
    "    object_cols = df_converted.select_dtypes(include=['object']).columns.tolist()\n",
    "    for col in object_cols:\n",
    "        df_converted[col] = df_converted[col].astype('category')\n",
    "\n",
    "    # 3. Ensure existing 'category' columns are correctly typed\n",
    "    category_cols = df_converted.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "    # 4. Convert boolean columns to integers\n",
    "    bool_cols = df_converted.select_dtypes(include=['bool']).columns.tolist()\n",
    "    for col in bool_cols:\n",
    "        df_converted[col] = df_converted[col].astype(int)\n",
    "\n",
    "    # 5. Handle categorical features\n",
    "    if encode_categorical:\n",
    "        # Perform label encoding\n",
    "        for col in category_cols:\n",
    "            le = LabelEncoder()\n",
    "            df_converted[col] = le.fit_transform(df_converted[col].astype(str))\n",
    "        categorical_features = []  # No categorical features after encoding\n",
    "    else:\n",
    "        categorical_features = category_cols.copy()\n",
    "\n",
    "    # 6. Verify data types\n",
    "    acceptable_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    for col in df_converted.columns:\n",
    "        if df_converted[col].dtype.name not in acceptable_dtypes and col not in categorical_features:\n",
    "            print(f\"Warning: Column {col} has dtype {df_converted[col].dtype}, which may not be accepted by XGBoost.\")\n",
    "\n",
    "    return df_converted, categorical_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477c75b-1405-4c30-a8ce-4587db2a0989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert training data types\n",
    "X_train_converted, categorical_features = convert_dataframe_types(baseDataset, encode_categorical=True)\n",
    "X_train_converted.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab159b-52a7-446a-9b63-106e3ab88f44",
   "metadata": {},
   "source": [
    "#### Kaggle metric:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c196d-2860-41a9-aa32-ee18f8a9b380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_weighted_geodesic_distance(row) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the weighted geodesic distance between actual and predicted lat/long points.\n",
    "\n",
    "    Parameters:\n",
    "    - row: A row from the merged DataFrame containing 'latitude', 'longitude', 'latitude_predicted',\n",
    "           'longitude_predicted', and 'scaling_factor'.\n",
    "\n",
    "    Returns:\n",
    "    - Weighted geodesic distance in meters.\n",
    "    \"\"\"\n",
    "    if pd.isna(row['latitude']) or pd.isna(row['latitude_predicted']):\n",
    "        return np.nan\n",
    "\n",
    "    # Calculate the geodesic distance in meters between actual and predicted coordinates\n",
    "    distance = geodesic((row['latitude'], row['longitude']),\n",
    "                        (row['latitude_predicted'], row['longitude_predicted'])).meters\n",
    "    # Multiply by the scaling factor to get the weighted distance\n",
    "    weighted_distance = distance * row['scaling_factor']\n",
    "    return weighted_distance\n",
    "\n",
    "def compute_score(solution: pd.DataFrame, prediction: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Computes the mean weighted geodesic distance for model tuning.\n",
    "\n",
    "    Parameters:\n",
    "    - solution: DataFrame with columns ['vesselId', 'time', 'latitude', 'longitude', 'scaling_factor'] (ground truth).\n",
    "    - prediction: DataFrame with columns ['vesselId', 'time', 'latitude_predicted', 'longitude_predicted'] (model predictions).\n",
    "\n",
    "    Returns:\n",
    "    - Mean weighted geodesic distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Merge solution and prediction DataFrames on 'vesselId' and 'time'\n",
    "    merged = pd.merge(solution, prediction, on=['vesselId', 'time'], how='inner')\n",
    "\n",
    "    # Apply the weighted distance calculation row by row\n",
    "    merged['weighted_distance'] = merged.apply(calculate_weighted_geodesic_distance, axis=1)\n",
    "\n",
    "    # Return the mean weighted distance in kilometers\n",
    "    return merged['weighted_distance'].mean() / 1000.0  # Convert meters to kilometers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71625703-163d-427e-aaef-11125ab56783",
   "metadata": {},
   "source": [
    "# Training with MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2fbab-4114-4273-9cca-4095fcbb17a7",
   "metadata": {},
   "source": [
    "### Create Time-Based Folds Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69c9ec-26b1-494d-b72a-cc5d001907b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "def create_time_based_folds(df: pd.DataFrame, num_folds: int = 3, days_per_fold: int = 5) -> List[Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Splits the data into sequential time-based folds for time-based cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the full dataset.\n",
    "    - num_folds: Number of folds for cross-validation.\n",
    "    - days_per_fold: Number of days per validation set.\n",
    "\n",
    "    Returns:\n",
    "    - List of dictionaries with 'train', 'val', 'val_start_time', and 'val_end_time' for each fold.\n",
    "    \"\"\"\n",
    "    # Ensure data is sorted by integer 'time'\n",
    "    df = df.sort_values(by='time')\n",
    "    folds = []\n",
    "    \n",
    "    # Compute the range of time in integer format for fold construction\n",
    "    max_time = df['time'].max()\n",
    "    fold_duration = days_per_fold * 24 * 3600  # days_per_fold converted to seconds\n",
    "    \n",
    "    # Generate folds by subtracting `fold_duration` for each new fold's validation start\n",
    "    for i in range(num_folds):\n",
    "        val_end_time = max_time - i * fold_duration\n",
    "        val_start_time = val_end_time - fold_duration\n",
    "\n",
    "        val_data = df[(df['time'] < val_end_time) & (df['time'] >= val_start_time)]\n",
    "        train_data = df[df['time'] < val_start_time]\n",
    "\n",
    "        if not train_data.empty and not val_data.empty:\n",
    "            folds.append({\n",
    "                'train': train_data,\n",
    "                'val': val_data,\n",
    "                'val_start_time': val_start_time,\n",
    "                'val_end_time': val_end_time\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Fold {i + 1} skipped due to empty train or validation set.\")\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cdf74c-b1bc-4368-b1ac-d6188517db58",
   "metadata": {},
   "source": [
    "# Recursive Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a4ecea-e6bc-452c-8ff9-37a5b4388d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "def recursive_prediction(\n",
    "    model,\n",
    "    train_data: pd.DataFrame,\n",
    "    val_data: pd.DataFrame,\n",
    "    static_features: list,\n",
    "    time_dependent_features: list,\n",
    "    lag_features: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform recursive predictions on validation data using the trained model,\n",
    "    updating lag features and other necessary features.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model for predictions\n",
    "    - train_data: Training data DataFrame\n",
    "    - val_data: Validation data DataFrame with 'vesselId' and 'time' columns, other features set to NaN\n",
    "    - static_features: List of static feature names\n",
    "    - time_dependent_features: List of time-dependent feature names set to NaN in validation\n",
    "    - lag_features: Dict with keys 'lat_lon' (latitude/longitude lags) and 'other' (e.g., `cog`, `sog`, `heading` lags)\n",
    "\n",
    "    Returns:\n",
    "    - predictions: DataFrame with predicted 'latitude' and 'longitude' for the validation data\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    val_data = val_data.sort_values(['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "    # Define the full feature set for model training and prediction\n",
    "    model_features = list(\n",
    "        static_features +\n",
    "        time_dependent_features +\n",
    "        lag_features['lat_lon'] +\n",
    "        lag_features['other'] +\n",
    "        [\"hour\", \"day\", \"day_of_week\", \"time_diff\"]\n",
    "    )\n",
    "\n",
    "    for vessel_id in val_data['vesselId'].unique():\n",
    "        val_vessel = val_data[val_data['vesselId'] == vessel_id].copy().reset_index(drop=True)\n",
    "        train_vessel = train_data[train_data['vesselId'] == vessel_id].copy()\n",
    "        if train_vessel.empty:\n",
    "            continue\n",
    "\n",
    "        # Cache static features\n",
    "        static_values = train_vessel.iloc[-1][static_features].to_dict()\n",
    "\n",
    "        # Initialize features DataFrame\n",
    "        features_df = val_vessel[['time']].copy()\n",
    "\n",
    "        # Convert 'time' to datetime within the function for feature calculations\n",
    "        features_df['time_datetime'] = pd.to_datetime(features_df['time'], unit='s')\n",
    "        train_vessel['time_datetime'] = pd.to_datetime(train_vessel['time'], unit='s')\n",
    "\n",
    "        # Compute time features\n",
    "        features_df['hour'] = features_df['time_datetime'].dt.hour\n",
    "        features_df['day'] = features_df['time_datetime'].dt.day\n",
    "        features_df['day_of_week'] = features_df['time_datetime'].dt.dayofweek\n",
    "\n",
    "        # Compute time_diff\n",
    "        # For the initial time, we need the last time from train_vessel\n",
    "        last_train_time = train_vessel['time_datetime'].iloc[-1]\n",
    "        times = pd.concat([pd.Series([last_train_time]), features_df['time_datetime']], ignore_index=True)\n",
    "        features_df['time_diff'] = times.diff().dt.total_seconds().iloc[1:].values\n",
    "\n",
    "        # Add static features\n",
    "        for feature in static_features:\n",
    "            features_df[feature] = static_values.get(feature, np.nan)\n",
    "\n",
    "        # Initialize lag features with NaNs\n",
    "        for feature in lag_features['lat_lon'] + lag_features['other']:\n",
    "            features_df[feature] = np.nan\n",
    "\n",
    "        # Set time-dependent features to NaN\n",
    "        for feature in time_dependent_features:\n",
    "            features_df[feature] = np.nan\n",
    "\n",
    "        # Initialize lag deques with last known values from training data\n",
    "        lat_lon_rolling = {\n",
    "            feature: deque(train_vessel[feature].dropna().tail(5), maxlen=5)\n",
    "            for feature in lag_features['lat_lon']\n",
    "        }\n",
    "        other_lags_rolling = {\n",
    "            feature: deque(train_vessel[feature].dropna().tail(2), maxlen=2)\n",
    "            for feature in lag_features['other']\n",
    "        }\n",
    "\n",
    "        # Process time steps recursively\n",
    "        for idx in features_df.index:\n",
    "            # Update lag features\n",
    "            for feature in lag_features['lat_lon']:\n",
    "                if len(lat_lon_rolling[feature]) > 0:\n",
    "                    features_df.at[idx, feature] = lat_lon_rolling[feature][-1]\n",
    "                else:\n",
    "                    features_df.at[idx, feature] = np.nan\n",
    "\n",
    "            for feature in lag_features['other']:\n",
    "                if len(other_lags_rolling[feature]) > 0:\n",
    "                    features_df.at[idx, feature] = other_lags_rolling[feature][-1]\n",
    "                else:\n",
    "                    features_df.at[idx, feature] = np.nan\n",
    "\n",
    "            # Prepare features for prediction\n",
    "            X_pred = features_df.loc[[idx], model_features]\n",
    "            # Remove 'time_datetime' from features if present\n",
    "            if 'time_datetime' in X_pred.columns:\n",
    "                X_pred = X_pred.drop(columns=['time_datetime'])\n",
    "\n",
    "            # Ensure that all features are acceptable to the model (e.g., numeric types)\n",
    "            # Convert any datetime columns back to integer timestamps if necessary\n",
    "            # For example, if 'time' is included as a feature, ensure it's integer\n",
    "            if 'time' in X_pred.columns:\n",
    "                X_pred['time'] = features_df.loc[idx, 'time']\n",
    "\n",
    "            # Predict\n",
    "            pred = model.predict(X_pred)\n",
    "            pred_latitude, pred_longitude = pred[0]\n",
    "\n",
    "            # Append predictions\n",
    "            predictions.append({\n",
    "                'vesselId': vessel_id,\n",
    "                'time': features_df.at[idx, 'time'],  # Keep 'time' as integer\n",
    "                'latitude_predicted': pred_latitude,\n",
    "                'longitude_predicted': pred_longitude\n",
    "            })\n",
    "\n",
    "            # Update lag deques with predicted values\n",
    "            for feature in lag_features['lat_lon']:\n",
    "                if 'latitude' in feature:\n",
    "                    lat_lon_rolling[feature].append(pred_latitude)\n",
    "                elif 'longitude' in feature:\n",
    "                    lat_lon_rolling[feature].append(pred_longitude)\n",
    "\n",
    "            # Update other lag features if applicable\n",
    "            # For this example, we assume they remain NaN or are not updated\n",
    "\n",
    "    return pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3ebd3-1f17-48fa-b1bc-f3cda21cb3e0",
   "metadata": {},
   "source": [
    "# Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c9ed5-0f8f-446b-a0fb-d6a505d6e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "def get_model(model_name: str, model_params: dict):\n",
    "    if model_name == 'xgboost':\n",
    "        from xgboost import XGBRegressor\n",
    "        base_model = XGBRegressor(**model_params)\n",
    "    elif model_name == 'lightgbm':\n",
    "        from lightgbm import LGBMRegressor\n",
    "        base_model = LGBMRegressor(**model_params)\n",
    "    elif model_name == 'catboost':\n",
    "        from catboost import CatBoostRegressor\n",
    "        base_model = CatBoostRegressor(**model_params, verbose=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name.\")\n",
    "\n",
    "    # Wrap the model with MultiOutputRegressor\n",
    "    model = MultiOutputRegressor(base_model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7e9a6-f4e2-4c56-aa95-fc2788f94d55",
   "metadata": {},
   "source": [
    "# Cross-Validation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d17635-757b-4999-8e96-515709eb3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(\n",
    "    df: pd.DataFrame,\n",
    "    num_folds: int,\n",
    "    days_per_fold: int,\n",
    "    static_features: list,\n",
    "    dynamic_features: list,\n",
    "    lag_features: dict,  # Expecting a dictionary here\n",
    "    model_name: str,\n",
    "    model_params: dict,\n",
    "    verbosity: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Cross-validate the model using time-based folds and recursive prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Full dataset DataFrame\n",
    "    - num_folds: Number of folds\n",
    "    - days_per_fold: Number of days per validation set\n",
    "    - static_features, dynamic_features, lag_features: Lists of feature names\n",
    "    - model_name: Name of the model to use ('xgboost', 'lightgbm', 'catboost')\n",
    "    - model_params: Dictionary of hyperparameters for the model\n",
    "    - verbosity: Verbosity level\n",
    "\n",
    "    Returns:\n",
    "    - cv_scores: List of scores for each fold\n",
    "    - models: List of trained models for each fold\n",
    "    \"\"\"\n",
    "    # Create time-based folds\n",
    "    folds = create_time_based_folds(df, num_folds, days_per_fold)\n",
    "    cv_scores = []\n",
    "    models = []\n",
    "\n",
    "    for fold_idx, fold in enumerate(folds):\n",
    "        if verbosity > 0:\n",
    "            print(f\"Processing Fold {fold_idx + 1}/{num_folds}\")\n",
    "        \n",
    "        train_data = fold['train'].copy()\n",
    "        val_data = fold['val'].copy()\n",
    "        val_start_time = fold['val_start_time']\n",
    "\n",
    "        # 'time' remains as integer 'time' in 'train_data' and 'val_data'\n",
    "\n",
    "        # Prepare training features and target\n",
    "        lag_feature_names = lag_features['lat_lon'] + lag_features['other']\n",
    "        model_features = static_features + dynamic_features + lag_feature_names + [\"hour\", \"day\", \"day_of_week\", \"time_diff\"]\n",
    "        X_train = train_data[model_features]\n",
    "        y_train = train_data[['latitude', 'longitude']]\n",
    "\n",
    "        # Prepare the model\n",
    "        model = get_model(model_name, model_params)\n",
    "\n",
    "        # Fit the model with early stopping\n",
    "        model.fit(X_train, y_train,)\n",
    "\n",
    "        # Prepare validation data for recursive prediction\n",
    "        val_data_recursive = val_data[['vesselId', 'time']].copy()\n",
    "        val_data_recursive = val_data_recursive.reindex(columns=['vesselId', 'time'] + model_features)\n",
    "        val_data_recursive[model_features] = np.nan\n",
    "\n",
    "        # Perform recursive prediction\n",
    "        predictions = recursive_prediction(\n",
    "            model, \n",
    "            train_data, \n",
    "            val_data_recursive, \n",
    "            static_features, \n",
    "            dynamic_features,\n",
    "            lag_features\n",
    "        )\n",
    "\n",
    "        # Compute scaling factors\n",
    "        val_start_datetime = pd.to_datetime(val_start_time, unit='s')\n",
    "        val_data['time_datetime'] = pd.to_datetime(val_data['time'], unit='s')\n",
    "        val_data['day_number'] = (val_data['time_datetime'] - val_start_datetime).dt.days + 1\n",
    "\n",
    "        scaling_factors = {1: 0.3, 2: 0.25, 3: 0.2, 4: 0.15, 5: 0.1}\n",
    "        val_data['scaling_factor'] = val_data['day_number'].map(scaling_factors)\n",
    "\n",
    "        # Prepare solution and prediction DataFrames\n",
    "        solution = val_data[['vesselId', 'time', 'latitude', 'longitude', 'scaling_factor']].reset_index(drop=True)\n",
    "        prediction = predictions[['vesselId', 'time', 'latitude_predicted', 'longitude_predicted']].copy()\n",
    "        prediction = prediction.reset_index(drop=True)\n",
    "\n",
    "        # Compute score using your custom metric\n",
    "        score = compute_score(\n",
    "            solution=solution,\n",
    "            prediction=prediction\n",
    "        )\n",
    "\n",
    "        if verbosity > 0:\n",
    "            print(f\"Fold {fold_idx + 1} Score: {score:.4f}\")\n",
    "        cv_scores.append(score)\n",
    "        models.append(model)\n",
    "\n",
    "    return cv_scores, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b46418-7516-49b0-bf75-57fc8726bd66",
   "metadata": {},
   "source": [
    "# Define generic objective function with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebefd8-956e-4c3e-baf3-9acd50962dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from functools import partial\n",
    "\n",
    "# Note that we should change paramters based on how long we want to run!!!\n",
    "# For final hyper paramter tests we should have large ranges to find the best possible!!!\n",
    "\n",
    "def objective(trial, df, num_folds, days_per_fold, static_features, dynamic_features, lag_features, model_name, verbosity=1):\n",
    "    # Suggest hyperparameters based on the model\n",
    "    if model_name == 'xgboost':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 3),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 3),\n",
    "            'random_state': 42,\n",
    "            'tree_method': 'hist',  # Faster tree construction\n",
    "            'verbosity': 0\n",
    "        }\n",
    "    elif model_name == 'lightgbm':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 15, 31),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 10, 20),\n",
    "                'random_state': 42\n",
    "            }\n",
    "\n",
    "    elif model_name == 'catboost':\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 500),\n",
    "            'depth': trial.suggest_int('depth', 3, 8),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 3, 8),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.5, 1.0),\n",
    "            'random_strength': trial.suggest_float('random_strength', 0.5, 1.5),\n",
    "            'random_seed': 42,\n",
    "            'verbose': 0\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name.\")\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_scores, models = cross_validate_model(\n",
    "        df=df,\n",
    "        num_folds=num_folds,\n",
    "        days_per_fold=days_per_fold,\n",
    "        static_features=static_features,\n",
    "        dynamic_features=dynamic_features,\n",
    "        lag_features=lag_features,\n",
    "        model_name=model_name,\n",
    "        model_params=params,\n",
    "        verbosity=verbosity\n",
    "    )\n",
    "\n",
    "    # Return the average score over the folds\n",
    "    avg_score = sum(cv_scores) / len(cv_scores)\n",
    "\n",
    "    return avg_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2d165f-653f-4f94-97f4-962678f998f3",
   "metadata": {},
   "source": [
    "# Perform Hyper paramter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc57a51-d471-44fb-a731-f5f0e3f1f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(df, num_folds, days_per_fold, static_features, dynamic_features, lag_features, model_name, n_trials=50, verbosity=1):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "\n",
    "    # Define the objective function with partial to fix the additional parameters\n",
    "    objective_func = partial(\n",
    "        objective,\n",
    "        df=df,\n",
    "        num_folds=num_folds,\n",
    "        days_per_fold=days_per_fold,\n",
    "        static_features=static_features,\n",
    "        dynamic_features=dynamic_features,\n",
    "        lag_features=lag_features,\n",
    "        model_name=model_name,\n",
    "        verbosity=verbosity\n",
    "    )\n",
    "\n",
    "    study.optimize(objective_func, n_trials=n_trials)\n",
    "\n",
    "    if verbosity > 0:\n",
    "        print('Best trial score:', study.best_value)\n",
    "        print('Best hyperparameters:', study.best_params)\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88fca4-ae49-4371-b205-1f4541f78308",
   "metadata": {},
   "source": [
    "# Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d98592-67b4-415a-9c68-dbe33ec3c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define static features (these do not change over time)\n",
    "static_features = [\n",
    "    'shippingLineId', 'CEU',\n",
    "    'DWT', 'GT', 'vesselType', 'breadth', 'homePort', 'length', 'age'\n",
    "]\n",
    "\n",
    "# Define dynamic features (exclude lags)\n",
    "dynamic_features = [\n",
    "    # List dynamic features here (e.g., 'cog', 'sog', 'rot', 'heading')\n",
    "    'cog', 'sog', 'rot', 'heading', 'portId', 'portLatitude', 'portLongitude',\n",
    "    'large_gap_flag', 'vessel_status', 'near_land', 'distance_to_port',\n",
    "    'cluster_label', 'distance_traveled', 'speed_change', 'bearing', \n",
    "    'sog_rolling_mean_3', 'sog_rolling_std_3', 'sog_rolling_mean_5', 'sog_rolling_std_5',\n",
    "    'cog_rolling_mean_3', 'cog_rolling_std_3', 'cog_rolling_mean_5', 'cog_rolling_std_5',\n",
    "    'heading_rolling_mean_3', 'heading_rolling_std_3', 'heading_rolling_mean_5', 'heading_rolling_std_5',\n",
    "    'port_frequency', 'previous_portId', 'previous_port_lat', 'previous_port_lon'\n",
    "]\n",
    "\n",
    "# Define lag features\n",
    "lag_features = {\n",
    "    'lat_lon': [\n",
    "        f'lag_{i}_latitude' for i in range(1, 6)\n",
    "    ] + [\n",
    "        f'lag_{i}_longitude' for i in range(1, 6)\n",
    "    ],\n",
    "    'other': [\n",
    "        f'lag_{i}_cog' for i in range(1, 3)\n",
    "    ] + [\n",
    "        f'lag_{i}_sog' for i in range(1, 3)\n",
    "    ] + [\n",
    "        f'lag_{i}_heading' for i in range(1, 3)\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30029f9e-c5b5-466f-bb28-838ebba18cc9",
   "metadata": {},
   "source": [
    "# Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa3959-59f2-4ecc-85d6-f0b866a94c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = X_train_converted\n",
    "\n",
    "# Perform hyperparameter tuning for the desired model\n",
    "model_name = 'xgboost'  # or 'lightgbm' or 'catboost'\n",
    "\n",
    "study = hyperparameter_tuning(\n",
    "    df,\n",
    "    num_folds=4,\n",
    "    days_per_fold=2,\n",
    "    static_features=static_features,\n",
    "    dynamic_features=dynamic_features,\n",
    "    lag_features=lag_features,\n",
    "    model_name=model_name,\n",
    "    n_trials=50,  # Adjust n_trials based on your computational resources\n",
    "    verbosity=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c63f9-d1da-47fd-aa4e-19263b39d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Prepare the full training data (use the entire dataset)\n",
    "full_train_data = df.copy()\n",
    "full_train_data['time'] = full_train_data['time'].astype(int)\n",
    "\n",
    "# Prepare training features and target\n",
    "lag_feature_names = lag_features['lat_lon'] + lag_features['other']\n",
    "model_features = static_features + dynamic_features + lag_feature_names\n",
    "X_full_train = full_train_data[model_features]\n",
    "y_full_train = full_train_data[['latitude', 'longitude']]\n",
    "\n",
    "# Create the final model\n",
    "final_model = get_model(model_name, best_params)\n",
    "\n",
    "# Fit the final model\n",
    "print(\"Training the final model on the full dataset...\")\n",
    "final_model.fit(X_full_train, y_full_train)\n",
    "print(\"Final model training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f585bf-e0fb-4ddc-b1bb-42d9e3115ee5",
   "metadata": {},
   "source": [
    "# Prepare test set for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54243340-0751-479f-97d0-91d6c56a071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test set\n",
    "test_df = ais_test_df\n",
    "\n",
    "# Sort test_df based on 'vesselId' and 'time'\n",
    "test_df = test_df.sort_values(['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "# Store the IDs and remove the 'ID' column\n",
    "test_ids = test_df[['ID']].copy()\n",
    "test_df = test_df.drop(columns=['ID'])\n",
    "\n",
    "# Add the model features and set them to NaN\n",
    "test_df = test_df.reindex(columns=['vesselId', 'time'] + model_features + ['latitude', 'longitude', 'time_diff', 'hour', 'day', 'day_of_week'])\n",
    "test_df[model_features] = np.nan\n",
    "\n",
    "# Fix category varible vesselId\n",
    "le = LabelEncoder()\n",
    "test_df['vesselId'] = le.fit_transform(test_df['vesselId'].astype(str))\n",
    "\n",
    "# Step 1: Convert 'time' column from object to datetime\n",
    "test_df['time'] = pd.to_datetime(test_df['time'], errors='coerce')\n",
    "# Step 2: Convert datetime to Unix timestamp in seconds\n",
    "test_df['time'] = test_df['time'].view(np.int64) // 10**9  # Unix timestamp in seconds\n",
    "test_df = test_df[full_train_data.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa281e22-ca2d-4f0f-a55d-de0cf1e060f5",
   "metadata": {},
   "source": [
    "# Run recursive prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aa5050-0d3e-4d07-b88e-cc358b6498fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform recursive prediction\n",
    "print(\"Performing recursive prediction on the test set...\")\n",
    "test_predictions = recursive_prediction(\n",
    "    final_model,\n",
    "    full_train_data,\n",
    "    test_df,\n",
    "    static_features,\n",
    "    dynamic_features,\n",
    "    lag_features\n",
    ")\n",
    "print(\"Recursive prediction complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a526a-9dd7-430c-becc-6d24ae293769",
   "metadata": {},
   "source": [
    "# Prepare and save submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229ce73-5566-4ee1-90f6-cbca6d800bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the predictions with the IDs\n",
    "test_predictions = test_predictions.sort_values(['vesselId', 'time']).reset_index(drop=True)\n",
    "submission = pd.concat([test_ids, test_predictions[['latitude_predicted', 'longitude_predicted']]], axis=1)\n",
    "\n",
    "# Rename columns as per Kaggle submission requirements\n",
    "submission.rename(columns={'latitude_predicted': 'latitude', 'longitude_predicted': 'longitude'}, inplace=True)\n",
    "\n",
    "# Sort the submission DataFrame based on 'ID'\n",
    "submission = submission.sort_values('ID').reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file 'submission.csv' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7fb2f-3c32-4617-b090-c80dba6dafa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters found by Optuna:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "    \n",
    "import json\n",
    "\n",
    "with open(\"best_params.json\", \"w\") as file:\n",
    "    json.dump(best_params, file)\n",
    "\n",
    "print(\"Best hyperparameters saved to best_params.json\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
