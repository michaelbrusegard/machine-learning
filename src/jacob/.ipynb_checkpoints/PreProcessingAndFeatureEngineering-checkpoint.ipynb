{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a586a6-b58a-49e0-afd5-070d2e0470ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calculations\n",
    "import math\n",
    "from haversine import haversine, Unit\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "#Cloud storage\n",
    "from google.cloud import storage\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42559ad2-7e00-40d7-880c-8f4092155732",
   "metadata": {},
   "source": [
    "We load the necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e02f6da-78cd-4dd8-a8f3-418e14ec7026",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522065 entries, 0 to 1522064\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count    Dtype  \n",
      "---  ------     --------------    -----  \n",
      " 0   time       1522065 non-null  object \n",
      " 1   cog        1522065 non-null  float64\n",
      " 2   sog        1522065 non-null  float64\n",
      " 3   rot        1522065 non-null  int64  \n",
      " 4   heading    1522065 non-null  int64  \n",
      " 5   navstat    1522065 non-null  int64  \n",
      " 6   etaRaw     1522065 non-null  object \n",
      " 7   latitude   1522065 non-null  float64\n",
      " 8   longitude  1522065 non-null  float64\n",
      " 9   vesselId   1522065 non-null  object \n",
      " 10  portId     1520450 non-null  object \n",
      "dtypes: float64(4), int64(3), object(4)\n",
      "memory usage: 127.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = '../../original_data/ais/ais_train.csv'\n",
    "\n",
    "# Load the file into a pandas dataframe\n",
    "ais_train_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "# Display the dataframe\n",
    "ais_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c2ef9df-6a2d-43d1-915c-7726fd32a221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>portId</th>\n",
       "      <th>name</th>\n",
       "      <th>portLocation</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>UN_LOCODE</th>\n",
       "      <th>countryName</th>\n",
       "      <th>ISO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61d36ed80a1807568ff9a064</td>\n",
       "      <td>Port of Algiers</td>\n",
       "      <td>Algiers</td>\n",
       "      <td>3.067222</td>\n",
       "      <td>36.773611</td>\n",
       "      <td>DZALG</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61d36ed80a1807568ff9a065</td>\n",
       "      <td>Port of Annaba</td>\n",
       "      <td>Annaba</td>\n",
       "      <td>7.772500</td>\n",
       "      <td>36.900556</td>\n",
       "      <td>DZAAE</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61d36edf0a1807568ff9a070</td>\n",
       "      <td>Port of Oran</td>\n",
       "      <td>Oran</td>\n",
       "      <td>-0.639722</td>\n",
       "      <td>35.712222</td>\n",
       "      <td>DZORN</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61d36ee00a1807568ff9a072</td>\n",
       "      <td>Port of Skikda</td>\n",
       "      <td>Skikda</td>\n",
       "      <td>6.905833</td>\n",
       "      <td>36.887500</td>\n",
       "      <td>DZSKI</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61d36ee10a1807568ff9a074</td>\n",
       "      <td>Port of Pago-Pago</td>\n",
       "      <td>Pago-Pago</td>\n",
       "      <td>-170.690556</td>\n",
       "      <td>-14.274167</td>\n",
       "      <td>ASPPG</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     portId               name portLocation   longitude  \\\n",
       "0  61d36ed80a1807568ff9a064    Port of Algiers      Algiers    3.067222   \n",
       "1  61d36ed80a1807568ff9a065     Port of Annaba       Annaba    7.772500   \n",
       "2  61d36edf0a1807568ff9a070       Port of Oran         Oran   -0.639722   \n",
       "3  61d36ee00a1807568ff9a072     Port of Skikda       Skikda    6.905833   \n",
       "4  61d36ee10a1807568ff9a074  Port of Pago-Pago    Pago-Pago -170.690556   \n",
       "\n",
       "    latitude UN_LOCODE     countryName ISO  \n",
       "0  36.773611     DZALG         Algeria  DZ  \n",
       "1  36.900556     DZAAE         Algeria  DZ  \n",
       "2  35.712222     DZORN         Algeria  DZ  \n",
       "3  36.887500     DZSKI         Algeria  DZ  \n",
       "4 -14.274167     ASPPG  American Samoa  AS  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = '../../original_data/ports/ports.csv'\n",
    "\n",
    "#Load the file into a pandas dataframe\n",
    "ports_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "ports_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7892f075-fe16-4095-a5ab-8ba91ba3c159",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vesselId</th>\n",
       "      <th>shippingLineId</th>\n",
       "      <th>shippingLineName</th>\n",
       "      <th>arrivalDate</th>\n",
       "      <th>sailingDate</th>\n",
       "      <th>portName</th>\n",
       "      <th>portId</th>\n",
       "      <th>portLatitude</th>\n",
       "      <th>portLongitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61e9f3b1b937134a3c4bfe53</td>\n",
       "      <td>61a8e672f9cba188601e84ac</td>\n",
       "      <td>Wallenius Wilhelmsen Ocean</td>\n",
       "      <td>2023-10-02 00:00:00+00:00</td>\n",
       "      <td>2023-10-03 00:00:00+00:00</td>\n",
       "      <td>Port of Brunswick</td>\n",
       "      <td>61d38499b7b7526e1adf3d54</td>\n",
       "      <td>31.140556</td>\n",
       "      <td>-81.496667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61e9f3b1b937134a3c4bfe53</td>\n",
       "      <td>61a8e672f9cba188601e84ac</td>\n",
       "      <td>Wallenius Wilhelmsen Ocean</td>\n",
       "      <td>2023-10-27 00:00:00+00:00</td>\n",
       "      <td>2023-10-27 00:00:00+00:00</td>\n",
       "      <td>Port of Southampton</td>\n",
       "      <td>61d3832bb7b7526e1adf3b63</td>\n",
       "      <td>50.902500</td>\n",
       "      <td>-1.428889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61e9f3b1b937134a3c4bfe53</td>\n",
       "      <td>61a8e672f9cba188601e84ac</td>\n",
       "      <td>Wallenius Wilhelmsen Ocean</td>\n",
       "      <td>2023-10-19 00:00:00+00:00</td>\n",
       "      <td>2023-10-20 00:00:00+00:00</td>\n",
       "      <td>Port of Bremerhaven</td>\n",
       "      <td>61d375e793c6feb83e5eb3e2</td>\n",
       "      <td>53.563611</td>\n",
       "      <td>8.554722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61e9f3b1b937134a3c4bfe53</td>\n",
       "      <td>61a8e672f9cba188601e84ac</td>\n",
       "      <td>Wallenius Wilhelmsen Ocean</td>\n",
       "      <td>2023-10-09 00:00:00+00:00</td>\n",
       "      <td>2023-10-10 00:00:00+00:00</td>\n",
       "      <td>Port of New York</td>\n",
       "      <td>61d38481b7b7526e1adf3d23</td>\n",
       "      <td>40.688333</td>\n",
       "      <td>-74.028611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61e9f3b1b937134a3c4bfe53</td>\n",
       "      <td>61a8e672f9cba188601e84ac</td>\n",
       "      <td>Wallenius Wilhelmsen Ocean</td>\n",
       "      <td>2023-09-25 00:00:00+00:00</td>\n",
       "      <td>2023-09-26 00:00:00+00:00</td>\n",
       "      <td>Manzanillo International Terminal</td>\n",
       "      <td>61d37d0199db2ccf7339eee1</td>\n",
       "      <td>9.372370</td>\n",
       "      <td>-79.879790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   vesselId            shippingLineId  \\\n",
       "0  61e9f3b1b937134a3c4bfe53  61a8e672f9cba188601e84ac   \n",
       "1  61e9f3b1b937134a3c4bfe53  61a8e672f9cba188601e84ac   \n",
       "2  61e9f3b1b937134a3c4bfe53  61a8e672f9cba188601e84ac   \n",
       "3  61e9f3b1b937134a3c4bfe53  61a8e672f9cba188601e84ac   \n",
       "4  61e9f3b1b937134a3c4bfe53  61a8e672f9cba188601e84ac   \n",
       "\n",
       "             shippingLineName                arrivalDate  \\\n",
       "0  Wallenius Wilhelmsen Ocean  2023-10-02 00:00:00+00:00   \n",
       "1  Wallenius Wilhelmsen Ocean  2023-10-27 00:00:00+00:00   \n",
       "2  Wallenius Wilhelmsen Ocean  2023-10-19 00:00:00+00:00   \n",
       "3  Wallenius Wilhelmsen Ocean  2023-10-09 00:00:00+00:00   \n",
       "4  Wallenius Wilhelmsen Ocean  2023-09-25 00:00:00+00:00   \n",
       "\n",
       "                 sailingDate                           portName  \\\n",
       "0  2023-10-03 00:00:00+00:00                  Port of Brunswick   \n",
       "1  2023-10-27 00:00:00+00:00                Port of Southampton   \n",
       "2  2023-10-20 00:00:00+00:00                Port of Bremerhaven   \n",
       "3  2023-10-10 00:00:00+00:00                   Port of New York   \n",
       "4  2023-09-26 00:00:00+00:00  Manzanillo International Terminal   \n",
       "\n",
       "                     portId  portLatitude  portLongitude  \n",
       "0  61d38499b7b7526e1adf3d54     31.140556     -81.496667  \n",
       "1  61d3832bb7b7526e1adf3b63     50.902500      -1.428889  \n",
       "2  61d375e793c6feb83e5eb3e2     53.563611       8.554722  \n",
       "3  61d38481b7b7526e1adf3d23     40.688333     -74.028611  \n",
       "4  61d37d0199db2ccf7339eee1      9.372370     -79.879790  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = '../../original_data/ais/schedules_to_may_2024.csv'\n",
    "\n",
    "#Load the file into a pandas dataframe\n",
    "schedules_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "schedules_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac7e81a-fb28-4c9a-9525-48f40a9442c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shippingLineId</th>\n",
       "      <th>vesselId</th>\n",
       "      <th>CEU</th>\n",
       "      <th>DWT</th>\n",
       "      <th>GT</th>\n",
       "      <th>NT</th>\n",
       "      <th>vesselType</th>\n",
       "      <th>breadth</th>\n",
       "      <th>depth</th>\n",
       "      <th>draft</th>\n",
       "      <th>enginePower</th>\n",
       "      <th>freshWater</th>\n",
       "      <th>fuel</th>\n",
       "      <th>homePort</th>\n",
       "      <th>length</th>\n",
       "      <th>maxHeight</th>\n",
       "      <th>maxSpeed</th>\n",
       "      <th>maxWidth</th>\n",
       "      <th>rampCapacity</th>\n",
       "      <th>yearBuilt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61a8e672f9cba188601e84ab</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "      <td>6500</td>\n",
       "      <td>21200.0</td>\n",
       "      <td>58684</td>\n",
       "      <td>17606.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>22.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OSLO</td>\n",
       "      <td>199.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>15.2</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61ec94f1a8cafc0e93f0e92a</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8d</td>\n",
       "      <td>4902</td>\n",
       "      <td>12325.0</td>\n",
       "      <td>46800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14220.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MONROVIA</td>\n",
       "      <td>182.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61e213d5d612676a0f0fb755</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8f</td>\n",
       "      <td>5000</td>\n",
       "      <td>13059.0</td>\n",
       "      <td>46800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14220.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SAINT JOHN'S</td>\n",
       "      <td>182.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61be24574ea00ae59d0fe388</td>\n",
       "      <td>61e9f38eb937134a3c4bfd91</td>\n",
       "      <td>4200</td>\n",
       "      <td>12588.0</td>\n",
       "      <td>39362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11060.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61a8e673f9cba188601e84ae</td>\n",
       "      <td>61e9f390b937134a3c4bfd93</td>\n",
       "      <td>7450</td>\n",
       "      <td>21052.0</td>\n",
       "      <td>75528</td>\n",
       "      <td>24391.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>22.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13140.0</td>\n",
       "      <td>491.47</td>\n",
       "      <td>3236.78</td>\n",
       "      <td>Panama</td>\n",
       "      <td>199.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             shippingLineId                  vesselId   CEU      DWT     GT  \\\n",
       "0  61a8e672f9cba188601e84ab  61e9f38eb937134a3c4bfd8b  6500  21200.0  58684   \n",
       "1  61ec94f1a8cafc0e93f0e92a  61e9f38eb937134a3c4bfd8d  4902  12325.0  46800   \n",
       "2  61e213d5d612676a0f0fb755  61e9f38eb937134a3c4bfd8f  5000  13059.0  46800   \n",
       "3  61be24574ea00ae59d0fe388  61e9f38eb937134a3c4bfd91  4200  12588.0  39362   \n",
       "4  61a8e673f9cba188601e84ae  61e9f390b937134a3c4bfd93  7450  21052.0  75528   \n",
       "\n",
       "        NT  vesselType  breadth  depth  draft  enginePower  freshWater  \\\n",
       "0  17606.0        83.0     32.0  22.20    NaN          0.0         NaN   \n",
       "1      NaN        83.0     31.0    NaN    NaN      14220.0         NaN   \n",
       "2      NaN        83.0     31.0    NaN    NaN      14220.0         NaN   \n",
       "3      NaN        83.0     28.0    NaN    NaN      11060.0         NaN   \n",
       "4  24391.0        83.0     37.2  22.23    NaN      13140.0      491.47   \n",
       "\n",
       "      fuel      homePort  length  maxHeight  maxSpeed  maxWidth  rampCapacity  \\\n",
       "0      NaN          OSLO  199.00        5.0      18.6      15.2         150.0   \n",
       "1      NaN      MONROVIA  182.00        NaN       NaN       NaN           NaN   \n",
       "2      NaN  SAINT JOHN'S  182.00        NaN       NaN       NaN           NaN   \n",
       "3      NaN           NaN  167.00        NaN       NaN       NaN           NaN   \n",
       "4  3236.78        Panama  199.98        NaN       NaN       NaN           NaN   \n",
       "\n",
       "   yearBuilt  \n",
       "0       2000  \n",
       "1       2006  \n",
       "2       2010  \n",
       "3       2011  \n",
       "4       2018  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = '../../original_data/vessels/vessels.csv'\n",
    "\n",
    "#Load the file into a pandas dataframe\n",
    "vessels_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "vessels_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f548182-0679-45c5-95cc-94110953667a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MAP_LAND_PATH = \"gs://jacobsbucketformlproject/ML Competition/ne_10m_land.zip\" # Path to the land zip file\n",
    "#MAP_OCEAN_PATH = \"gs://jacobsbucketformlproject/ML Competition/ne_10m_ocean.zip\" # Path to the ocean zip file\n",
    "#land_world = gpd.read_file(MAP_LAND_PATH)\n",
    "#ocean_world = gpd.read_file(MAP_OCEAN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843db903-f622-4a52-9654-de36953e644b",
   "metadata": {},
   "source": [
    "Convert all timestamps to datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd6b7ca0-6987-45e9-a5d3-3db9e6c93395",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066/408382314.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  ais_train_df['navstat_desc'].fillna('Unknown', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522065 entries, 0 to 1522064\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count    Dtype         \n",
      "---  ------        --------------    -----         \n",
      " 0   time          1522065 non-null  datetime64[ns]\n",
      " 1   cog           1516207 non-null  float64       \n",
      " 2   sog           1522065 non-null  float64       \n",
      " 3   rot           1522065 non-null  float64       \n",
      " 4   heading       1517169 non-null  float64       \n",
      " 5   navstat       1522065 non-null  int64         \n",
      " 6   etaRaw        1506820 non-null  datetime64[ns]\n",
      " 7   latitude      1522065 non-null  float64       \n",
      " 8   longitude     1522065 non-null  float64       \n",
      " 9   vesselId      1522065 non-null  object        \n",
      " 10  portId        1520450 non-null  object        \n",
      " 11  navstat_desc  1522065 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(6), int64(1), object(3)\n",
      "memory usage: 139.3+ MB\n"
     ]
    }
   ],
   "source": [
    "def preprocess_ais_train(ais_train_df):\n",
    "    \"\"\"\n",
    "    Preprocess the ais_train_df by converting columns, handling missing or invalid values, \n",
    "    and mapping NAVSTAT codes to descriptions.\n",
    "\n",
    "    Parameters:\n",
    "    - ais_train_df: DataFrame containing the raw AIS train data.\n",
    "\n",
    "    Returns:\n",
    "    - ais_train_df_cleaned: A cleaned and preprocessed version of ais_train_df.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert 'time' and 'etaRaw' to datetime\n",
    "    ais_train_df['time'] = pd.to_datetime(ais_train_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    ais_train_df['etaRaw'] = pd.to_datetime(ais_train_df['etaRaw'], errors='coerce', format='%m-%d %H:%M')\n",
    "    \n",
    "    # Step 2: Add the year 2024 to 'etaRaw' column\n",
    "    ais_train_df['etaRaw'] = ais_train_df['etaRaw'].apply(lambda x: x.replace(year=2024) if pd.notna(x) else pd.NaT)\n",
    "    \n",
    "    # Step 3: Convert relevant columns to float\n",
    "    ais_train_df['cog'] = ais_train_df['cog'].astype(float)\n",
    "    ais_train_df['sog'] = ais_train_df['sog'].astype(float)\n",
    "    ais_train_df['rot'] = ais_train_df['rot'].astype(float)\n",
    "    ais_train_df['heading'] = ais_train_df['heading'].astype(float)\n",
    "    ais_train_df['latitude'] = ais_train_df['latitude'].astype(float)\n",
    "    ais_train_df['longitude'] = ais_train_df['longitude'].astype(float)\n",
    "    \n",
    "    # Step 4: Replace invalid or default values with NaN\n",
    "    ais_train_df['cog'] = np.where((ais_train_df['cog'] == 360) | (ais_train_df['cog'] > 360), np.nan, ais_train_df['cog'])\n",
    "    ais_train_df['sog'] = np.where((ais_train_df['sog'] == 1023), np.nan, ais_train_df['sog'])\n",
    "    ais_train_df['rot'] = np.where((ais_train_df['rot'] == -128), np.nan, ais_train_df['rot'])\n",
    "    ais_train_df['heading'] = np.where((ais_train_df['heading'] > 360) | (ais_train_df['heading'] == 511), np.nan, ais_train_df['heading'])\n",
    "    \n",
    "    # Step 5: Map NAVSTAT codes to descriptive statuses\n",
    "    navstat_mapping = {\n",
    "        0: 'Under way using engine',\n",
    "        1: 'At anchor',\n",
    "        2: 'Not under command',\n",
    "        3: 'Restricted manoeuvrability',\n",
    "        4: 'Constrained by her draught',\n",
    "        5: 'Moored',\n",
    "        6: 'Aground',\n",
    "        7: 'Engaged in fishing',\n",
    "        8: 'Under way sailing',\n",
    "        15: 'Undefined'\n",
    "    }\n",
    "    ais_train_df['navstat_desc'] = ais_train_df['navstat'].map(navstat_mapping)\n",
    "    ais_train_df['navstat_desc'].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    ais_train_df = ais_train_df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "    return ais_train_df\n",
    "\n",
    "ais_train_df = preprocess_ais_train(ais_train_df)\n",
    "\n",
    "ais_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf785408-2d26-46aa-b5f2-a37c33d0f607",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 711 entries, 0 to 710\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   shippingLineId  711 non-null    object \n",
      " 1   vesselId        711 non-null    object \n",
      " 2   CEU             711 non-null    int64  \n",
      " 3   DWT             703 non-null    float64\n",
      " 4   GT              711 non-null    int64  \n",
      " 5   vesselType      699 non-null    float64\n",
      " 6   breadth         703 non-null    float64\n",
      " 7   enginePower     691 non-null    float64\n",
      " 8   homePort        573 non-null    object \n",
      " 9   length          711 non-null    float64\n",
      " 10  maxSpeed        213 non-null    float64\n",
      " 11  age             711 non-null    int64  \n",
      "dtypes: float64(6), int64(3), object(3)\n",
      "memory usage: 66.8+ KB\n"
     ]
    }
   ],
   "source": [
    "def preprocess_vessels(vessels_df):\n",
    "    \"\"\"\n",
    "    Preprocess the vessels_df by converting 'yearBuilt' to 'age', \n",
    "    handling missing values, and dropping unnecessary columns.\n",
    "\n",
    "    Parameters:\n",
    "    - vessels_df: DataFrame containing the raw vessels data.\n",
    "\n",
    "    Returns:\n",
    "    - vessels_df_cleaned: A cleaned and preprocessed version of vessels_df.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate the current year and create 'age' column\n",
    "    current_year = 2024\n",
    "    vessels_df['age'] = vessels_df['yearBuilt'].apply(lambda x: current_year - x if pd.notna(x) else np.nan)\n",
    "\n",
    "    # Step 2: Drop the 'yearBuilt' column\n",
    "    vessels_df = vessels_df.drop(columns=['yearBuilt'])\n",
    "\n",
    "    # Step 3: Drop columns with high missing values and low predictive power\n",
    "    columns_to_drop = ['NT', 'depth', 'draft', 'freshWater', 'fuel', 'maxHeight', 'maxWidth', 'rampCapacity']\n",
    "    vessels_df = vessels_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    return vessels_df\n",
    "\n",
    "vessels_df = preprocess_vessels(vessels_df)\n",
    "\n",
    "vessels_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "387e8389-0c16-4b0a-8eeb-ef17f42c420f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1329 entries, 0 to 1328\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   portId     1329 non-null   object \n",
      " 1   longitude  1329 non-null   float64\n",
      " 2   latitude   1329 non-null   float64\n",
      " 3   ISO        1329 non-null   object \n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 41.7+ KB\n"
     ]
    }
   ],
   "source": [
    "def preprocess_ports(ports_df):\n",
    "    \"\"\"\n",
    "    Preprocess the ports_df by dropping unnecessary columns based on redundancy.\n",
    "\n",
    "    Parameters:\n",
    "    - ports_df: DataFrame containing the raw ports data.\n",
    "\n",
    "    Returns:\n",
    "    - ports_df_cleaned: A cleaned and preprocessed version of ports_df.\n",
    "    \"\"\"\n",
    "    # Step 1: Drop columns based on redundancy\n",
    "    columns_to_drop = ['name','portLocation', 'UN_LOCODE', 'countryName']\n",
    "    ports_df = ports_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    return ports_df\n",
    "\n",
    "ports_df = preprocess_ports(ports_df)\n",
    "\n",
    "ports_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d57473-3888-4ba4-9f8c-0db8f37d8c37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32944 entries, 0 to 32943\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   vesselId       32944 non-null  object        \n",
      " 1   arrivalDate    32944 non-null  datetime64[ns]\n",
      " 2   sailingDate    32944 non-null  datetime64[ns]\n",
      " 3   portId         32944 non-null  object        \n",
      " 4   portLatitude   32944 non-null  float64       \n",
      " 5   portLongitude  32944 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(2), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "def preprocess_schedules(schedules_df):\n",
    "    \"\"\"\n",
    "    Preprocess the schedules_df by converting timestamps, handling invalid rows,\n",
    "    removing duplicates, and filtering out conflicting port information.\n",
    "\n",
    "    Parameters:\n",
    "    - schedules_df: DataFrame containing the raw schedules data.\n",
    "\n",
    "    Returns:\n",
    "    - schedules_df_cleaned: A cleaned and preprocessed version of schedules_df.\n",
    "    \"\"\"\n",
    "    # Make an explicit copy of the DataFrame to avoid working with a slice\n",
    "    schedules_df = schedules_df.copy()\n",
    "\n",
    "    # Step 1: Remove duplicates\n",
    "    schedules_df = schedules_df.drop_duplicates()\n",
    "\n",
    "    # Step 2: Convert 'arrivalDate' and 'sailingDate' to datetime format\n",
    "    schedules_df['arrivalDate'] = pd.to_datetime(schedules_df['arrivalDate'], errors='coerce')\n",
    "    schedules_df['sailingDate'] = pd.to_datetime(schedules_df['sailingDate'], errors='coerce')\n",
    "\n",
    "    # Step 3: Remove timezone info to match the format of 'ais_train'\n",
    "    schedules_df['arrivalDate'] = schedules_df['arrivalDate'].dt.tz_localize(None)\n",
    "    schedules_df['sailingDate'] = schedules_df['sailingDate'].dt.tz_localize(None)\n",
    "\n",
    "    # Step 4: Drop rows with NaN values for vesselID, portId\n",
    "    schedules_df = schedules_df.dropna(subset=['vesselId', 'portId'])\n",
    "\n",
    "    # Step 5: Remove redundant columns ('portName', 'shippingLineName')\n",
    "    columns_to_drop = ['portName','shippingLineId', 'shippingLineName']\n",
    "    schedules_df = schedules_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Step 6: Remove rows where 'sailingDate' is before 'arrivalDate'\n",
    "    schedules_df = schedules_df[schedules_df['sailingDate'] >= schedules_df['arrivalDate']].copy()\n",
    "\n",
    "    # Step 7: Handle conflicting port information\n",
    "    # Drop duplicates based on 'vesselId', 'arrivalDate', 'sailingDate', and 'portId'\n",
    "    schedules_no_port_duplicates = schedules_df.drop_duplicates(subset=['vesselId', 'arrivalDate', 'sailingDate', 'portId'])\n",
    "\n",
    "    # Identify conflicting entries with the same 'vesselId', 'arrivalDate', 'sailingDate' but different 'portId'\n",
    "    vessel_date_duplicates = schedules_no_port_duplicates[schedules_no_port_duplicates.duplicated(subset=['vesselId', 'arrivalDate', 'sailingDate'], keep=False)]\n",
    "\n",
    "    # Remove all conflicting rows from the original DataFrame\n",
    "    schedules_df_cleaned = schedules_df[~schedules_df.index.isin(vessel_date_duplicates.index)]\n",
    "\n",
    "    # Step 8: Remove exact duplicates (same 'vesselId', 'arrivalDate', 'sailingDate', 'portId')\n",
    "    schedules_df_cleaned = schedules_df_cleaned.drop_duplicates(subset=['vesselId', 'arrivalDate', 'sailingDate', 'portId'])\n",
    "\n",
    "    # Step 9: Sort the cleaned DataFrame by 'vesselId' and 'arrivalDate' to ensure proper ordering\n",
    "    schedules_df_cleaned = schedules_df_cleaned.sort_values(by=['vesselId', 'arrivalDate']).reset_index(drop=True)\n",
    "\n",
    "    return schedules_df_cleaned\n",
    "\n",
    "schedules_df = preprocess_schedules(schedules_df)\n",
    "\n",
    "schedules_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a215526-2833-465f-b8c5-7182e3181d3f",
   "metadata": {},
   "source": [
    "# We now merge the datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c60cf6-d8ff-4538-a468-f48712fbadbb",
   "metadata": {},
   "source": [
    "Before mergin we must check that all the data we have behaves as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bae3d49e-3cdd-4b11-8e29-b453ce652f39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No timestamp anomalies found. All timestamps are in ascending order for each vessel.\n"
     ]
    }
   ],
   "source": [
    "def check_timestamp_anomalies(ais_train_df):\n",
    "    \"\"\"\n",
    "    Check if the 'time' column for each vessel in the ais_train_df is in ascending order.\n",
    "    Identify any vessels with timestamp anomalies (non-ascending order).\n",
    "    \n",
    "    Parameters:\n",
    "    - ais_train_df: The AIS data containing vessel information and timestamps.\n",
    "\n",
    "    Returns:\n",
    "    - A list of vessel IDs with timestamp anomalies and the corresponding rows with issues.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store vessel IDs with anomalies\n",
    "    vessels_with_anomalies = []\n",
    "\n",
    "    # Group by 'vesselId' and check if the 'time' column is sorted for each group\n",
    "    for vessel_id, vessel_data in ais_train_df.groupby('vesselId'):\n",
    "        # Sort the data by 'time' to ensure correct order\n",
    "        sorted_data = vessel_data.sort_values('time').reset_index(drop=True)\n",
    "        \n",
    "        # Check if the 'time' column is strictly increasing\n",
    "        if not sorted_data['time'].is_monotonic_increasing:\n",
    "            print(f\"Anomaly detected in vessel ID: {vessel_id}\")\n",
    "            \n",
    "            # Add vessel ID to the list of anomalies\n",
    "            vessels_with_anomalies.append(vessel_id)\n",
    "            \n",
    "            # Print the rows where anomalies exist (timestamps are not in ascending order)\n",
    "            incorrect_rows = sorted_data[sorted_data['time'].diff().dt.total_seconds() < 0]\n",
    "            print(f\"Problematic rows for vessel ID {vessel_id}:\\n\", incorrect_rows[['time', 'latitude', 'longitude']])\n",
    "\n",
    "    # Final report\n",
    "    if vessels_with_anomalies:\n",
    "        print(f\"\\nNumber of vessels with timestamp anomalies: {len(vessels_with_anomalies)}\")\n",
    "    else:\n",
    "        print(\"\\nNo timestamp anomalies found. All timestamps are in ascending order for each vessel.\")\n",
    "    \n",
    "    return vessels_with_anomalies\n",
    "\n",
    "\n",
    "# Test the function on ais_train_df\n",
    "vessels_with_timestamp_anomalies = check_timestamp_anomalies(ais_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eac27557-a614-4730-8337-10dec7ce2932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate rows found.\n"
     ]
    }
   ],
   "source": [
    "def check_duplicate_rows(schedules_df):\n",
    "    \"\"\"\n",
    "    Check for duplicate rows in the schedules_cleaned_df based on 'vesselId', 'arrivalDate', 'sailingDate' and 'portId'.\n",
    "\n",
    "    Parameters:\n",
    "    - schedules_cleaned_df: The schedules data containing vessel information and timestamps.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame of duplicate rows.\n",
    "    \"\"\"\n",
    "    # Check for duplicates based on 'vesselId', 'arrivalDate', 'sailingDate' and 'PortId\n",
    "    duplicate_rows = schedules_df[schedules_df.duplicated(subset=['vesselId', 'arrivalDate', 'sailingDate','portId'], keep=False)]\n",
    "\n",
    "    if not duplicate_rows.empty:\n",
    "        print(f\"Number of duplicate rows found: {len(duplicate_rows)}\")\n",
    "        print(duplicate_rows[['vesselId', 'arrivalDate', 'sailingDate', 'portId']])\n",
    "    else:\n",
    "        print(\"No duplicate rows found.\")\n",
    "\n",
    "    return duplicate_rows\n",
    "\n",
    "\n",
    "# Run the duplicate check\n",
    "duplicate_anomalies = check_duplicate_rows(schedules_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d2e8c89-be67-48a2-bdad-22e88ccb811c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No anomalies found. All timestamps are in ascending order and valid.\n"
     ]
    }
   ],
   "source": [
    "def check_schedule_timestamp_anomalies_complete(schedules_df):\n",
    "    \"\"\"\n",
    "    Check if 'arrivalDate' is in ascending order for each vessel and if 'sailingDate' is before 'arrivalDate'.\n",
    "    Allow cases where 'sailingDate' is the same as 'arrivalDate'.\n",
    "\n",
    "    Parameters:\n",
    "    - schedules_cleaned_df: The schedules data containing vessel information and timestamps.\n",
    "\n",
    "    Returns:\n",
    "    - A list of vessel IDs with anomalies and a DataFrame of rows with issues.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store vessel IDs with anomalies\n",
    "    vessels_with_anomalies = []\n",
    "    \n",
    "    # Initialize a DataFrame to store rows with issues\n",
    "    anomaly_rows = pd.DataFrame()\n",
    "\n",
    "    # Group by 'vesselId' and check for ascending 'arrivalDate' and valid 'sailingDate'\n",
    "    for vessel_id, vessel_data in schedules_df.groupby('vesselId'):\n",
    "        # Check if 'arrivalDate' is in ascending order\n",
    "        if not vessel_data['arrivalDate'].is_monotonic_increasing:\n",
    "            print(f\"ArrivalDate anomaly detected in vessel ID: {vessel_id}\")\n",
    "            vessels_with_anomalies.append(vessel_id)\n",
    "            \n",
    "            # Identify rows where 'arrivalDate' is not in ascending order\n",
    "            arrival_anomalies = vessel_data[vessel_data['arrivalDate'].diff().dt.total_seconds() < 0]\n",
    "            anomaly_rows = pd.concat([anomaly_rows, arrival_anomalies])\n",
    "\n",
    "        # Check if 'sailingDate' is before 'arrivalDate' (not equal)\n",
    "        sailing_anomalies = vessel_data[vessel_data['sailingDate'] < vessel_data['arrivalDate']]\n",
    "        if not sailing_anomalies.empty:\n",
    "            print(f\"SailingDate anomaly detected in vessel ID: {vessel_id}\")\n",
    "            vessels_with_anomalies.append(vessel_id)\n",
    "            anomaly_rows = pd.concat([anomaly_rows, sailing_anomalies])\n",
    "    \n",
    "    # Final report\n",
    "    if not anomaly_rows.empty:\n",
    "        print(f\"\\nNumber of vessels with timestamp anomalies: {len(vessels_with_anomalies)}\")\n",
    "        print(anomaly_rows[['vesselId', 'arrivalDate', 'sailingDate', 'portId']])\n",
    "    else:\n",
    "        print(\"\\nNo anomalies found. All timestamps are in ascending order and valid.\")\n",
    "    \n",
    "    return vessels_with_anomalies, anomaly_rows\n",
    "\n",
    "# Run the updated anomaly check on schedules_df\n",
    "vessels_with_anomalies, anomaly_rows = check_schedule_timestamp_anomalies_complete(schedules_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a1f24d-2b96-4b7d-9901-749c8d2bc237",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies found where sailingDate is greater than the next row's arrivalDate:\n",
      "                       vesselId         sailingDate    next_arrivalDate  \\\n",
      "0      61e9f38eb937134a3c4bfd8b 2023-08-06 17:00:00 2023-08-06 15:56:00   \n",
      "1      61e9f38eb937134a3c4bfd8b 2023-08-09 21:00:00 2023-08-08 15:56:00   \n",
      "2      61e9f38eb937134a3c4bfd8b 2023-08-18 00:00:00 2023-08-15 00:00:00   \n",
      "3      61e9f38eb937134a3c4bfd8b 2023-08-17 00:00:00 2023-08-16 00:00:00   \n",
      "4      61e9f38eb937134a3c4bfd8b 2023-08-18 21:00:00 2023-08-18 15:56:00   \n",
      "...                         ...                 ...                 ...   \n",
      "11256  631fc2a1fe2331c1cf4131e0 2024-05-05 23:59:00 2024-05-05 07:00:00   \n",
      "11257  631fc2a1fe2331c1cf4131e0 2024-05-14 14:00:00 2024-05-13 20:00:00   \n",
      "11258  6326fcf5c46d6a20d22ca323 2023-09-21 00:00:00 2023-09-20 00:00:00   \n",
      "11259  6326fcf5c46d6a20d22ca323 2023-12-29 00:00:00 2023-12-28 00:00:00   \n",
      "11260  6326fcf5c46d6a20d22ca323 2024-01-31 00:00:00 2024-01-26 00:00:00   \n",
      "\n",
      "       index_current  index_next  \n",
      "0                 11          12  \n",
      "1                 13          14  \n",
      "2                 15          16  \n",
      "3                 16          17  \n",
      "4                 19          20  \n",
      "...              ...         ...  \n",
      "11256            325         326  \n",
      "11257            329         330  \n",
      "11258             61          62  \n",
      "11259             90          91  \n",
      "11260             98          99  \n",
      "\n",
      "[11261 rows x 5 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vesselId</th>\n",
       "      <th>arrivalDate</th>\n",
       "      <th>sailingDate</th>\n",
       "      <th>portId</th>\n",
       "      <th>portLatitude</th>\n",
       "      <th>portLongitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32939</th>\n",
       "      <td>6326fcf5c46d6a20d22ca323</td>\n",
       "      <td>2024-06-08</td>\n",
       "      <td>2024-06-08</td>\n",
       "      <td>61d375e793c6feb83e5eb3e2</td>\n",
       "      <td>53.563611</td>\n",
       "      <td>8.554722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32940</th>\n",
       "      <td>6326fcf5c46d6a20d22ca323</td>\n",
       "      <td>2024-06-14</td>\n",
       "      <td>2024-06-14</td>\n",
       "      <td>61d373b83aeaecc07011a62b</td>\n",
       "      <td>60.437778</td>\n",
       "      <td>22.216389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32941</th>\n",
       "      <td>6326fcf5c46d6a20d22ca323</td>\n",
       "      <td>2024-06-17</td>\n",
       "      <td>2024-06-17</td>\n",
       "      <td>61d375e793c6feb83e5eb3e2</td>\n",
       "      <td>53.563611</td>\n",
       "      <td>8.554722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32942</th>\n",
       "      <td>6326fcf5c46d6a20d22ca323</td>\n",
       "      <td>2024-06-24</td>\n",
       "      <td>2024-06-24</td>\n",
       "      <td>61d373b83aeaecc07011a62b</td>\n",
       "      <td>60.437778</td>\n",
       "      <td>22.216389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32943</th>\n",
       "      <td>6326fcf5c46d6a20d22ca323</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>61d375e793c6feb83e5eb3e2</td>\n",
       "      <td>53.563611</td>\n",
       "      <td>8.554722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       vesselId arrivalDate sailingDate  \\\n",
       "32939  6326fcf5c46d6a20d22ca323  2024-06-08  2024-06-08   \n",
       "32940  6326fcf5c46d6a20d22ca323  2024-06-14  2024-06-14   \n",
       "32941  6326fcf5c46d6a20d22ca323  2024-06-17  2024-06-17   \n",
       "32942  6326fcf5c46d6a20d22ca323  2024-06-24  2024-06-24   \n",
       "32943  6326fcf5c46d6a20d22ca323  2024-06-27  2024-06-27   \n",
       "\n",
       "                         portId  portLatitude  portLongitude  \n",
       "32939  61d375e793c6feb83e5eb3e2     53.563611       8.554722  \n",
       "32940  61d373b83aeaecc07011a62b     60.437778      22.216389  \n",
       "32941  61d375e793c6feb83e5eb3e2     53.563611       8.554722  \n",
       "32942  61d373b83aeaecc07011a62b     60.437778      22.216389  \n",
       "32943  61d375e793c6feb83e5eb3e2     53.563611       8.554722  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_sailing_vs_arrival_anomalies(schedules_df):\n",
    "    \"\"\"\n",
    "    Sorts schedules for each vessel by 'arrivalDate' and checks if the 'sailingDate' \n",
    "    of one row is greater than the 'arrivalDate' of the next row.\n",
    "\n",
    "    Parameters:\n",
    "    - schedules_df: DataFrame containing schedules data.\n",
    "\n",
    "    Returns:\n",
    "    - anomalies_df: DataFrame containing rows where 'sailingDate' is greater \n",
    "      than the 'arrivalDate' of the next row.\n",
    "    \"\"\"\n",
    "    # Create an empty list to store anomalies\n",
    "    anomalies = []\n",
    "\n",
    "    # Get unique vessel IDs\n",
    "    vessel_ids = schedules_df['vesselId'].unique()\n",
    "\n",
    "    # Loop through each vessel and perform the check\n",
    "    for vessel_id in vessel_ids:\n",
    "        # Filter schedules for this vessel and sort by arrivalDate\n",
    "        vessel_schedules_df = schedules_df[schedules_df['vesselId'] == vessel_id].sort_values('arrivalDate').reset_index(drop=True)\n",
    "        \n",
    "        # Iterate through rows to check if sailingDate is greater than the arrivalDate of the next row\n",
    "        for i in range(len(vessel_schedules_df) - 1):\n",
    "            current_sailing_date = vessel_schedules_df.loc[i, 'sailingDate']\n",
    "            next_arrival_date = vessel_schedules_df.loc[i + 1, 'arrivalDate']\n",
    "            \n",
    "            # If the current row's sailingDate is greater than the next row's arrivalDate, record it\n",
    "            if current_sailing_date > next_arrival_date:\n",
    "                anomalies.append({\n",
    "                    'vesselId': vessel_id,\n",
    "                    'sailingDate': current_sailing_date,\n",
    "                    'next_arrivalDate': next_arrival_date,\n",
    "                    'index_current': i,\n",
    "                    'index_next': i + 1\n",
    "                })\n",
    "\n",
    "    # Convert the list of anomalies to a DataFrame for inspection\n",
    "    anomalies_df = pd.DataFrame(anomalies)\n",
    "\n",
    "    return anomalies_df\n",
    "\n",
    "# Assuming schedules_df_cleaned is the cleaned schedules DataFrame\n",
    "anomalies_df = check_sailing_vs_arrival_anomalies(schedules_df)\n",
    "\n",
    "# Print anomalies if any exist\n",
    "if not anomalies_df.empty:\n",
    "    print(\"Anomalies found where sailingDate is greater than the next row's arrivalDate:\")\n",
    "    print(anomalies_df)\n",
    "else:\n",
    "    print(\"No anomalies found.\")\n",
    "    \n",
    "schedules_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fae88ec2-1af1-434d-94b3-536beb24a931",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No anomalies found.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_schedules_final(schedules_df):\n",
    "    \"\"\"\n",
    "    Preprocess schedules to handle infeasible arrival dates and small movements at the same port.\n",
    "\n",
    "    Parameters:\n",
    "    - schedules_df: DataFrame with schedules data.\n",
    "\n",
    "    Returns:\n",
    "    - schedules_df: Updated schedules DataFrame with added flags for small movements and skipped ports.\n",
    "    \"\"\"\n",
    "    schedules_df = schedules_df.copy()\n",
    "\n",
    "    # Initialize new features\n",
    "    schedules_df['small_movement_flag'] = 0\n",
    "    schedules_df['skipped_port_flag'] = 0\n",
    "\n",
    "    # Sort by 'vesselId' and 'arrivalDate'\n",
    "    schedules_df = schedules_df.sort_values(by=['vesselId', 'arrivalDate']).reset_index(drop=True)\n",
    "\n",
    "    # Keep track of rows to drop\n",
    "    rows_to_drop = set()\n",
    "\n",
    "    # Process each vessel separately\n",
    "    for vessel_id in schedules_df['vesselId'].unique():\n",
    "        vessel_indices = schedules_df[schedules_df['vesselId'] == vessel_id].index.tolist()\n",
    "        i = 0\n",
    "        while i < len(vessel_indices) - 1:\n",
    "            current_idx = vessel_indices[i]\n",
    "            next_idx = vessel_indices[i + 1]\n",
    "            current_row = schedules_df.loc[current_idx]\n",
    "            next_row = schedules_df.loc[next_idx]\n",
    "\n",
    "            # Handle same ports\n",
    "            if current_row['portId'] == next_row['portId']:\n",
    "                # Update sailingDate and set flag\n",
    "                schedules_df.at[current_idx, 'sailingDate'] = next_row['sailingDate']\n",
    "                schedules_df.at[current_idx, 'small_movement_flag'] = 1\n",
    "                rows_to_drop.add(next_idx)  # Mark next row for removal\n",
    "                vessel_indices.pop(i + 1)   # Remove next index from list\n",
    "                # Do not increment i to check for further consecutive same ports\n",
    "                continue\n",
    "\n",
    "            # Handle different ports with invalid or zero time difference arrivalDate\n",
    "            elif next_row['arrivalDate'] <= current_row['sailingDate']:\n",
    "                # Mark next row for removal\n",
    "                rows_to_drop.add(next_idx)\n",
    "                # Set skipped port flag\n",
    "                schedules_df.at[current_idx, 'skipped_port_flag'] = 1\n",
    "                vessel_indices.pop(i + 1)   # Remove next index from list\n",
    "                # Do not increment i to check the next row against the current one\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                # Valid movement, proceed to next\n",
    "                i += 1\n",
    "\n",
    "    # Remove rows marked for dropping\n",
    "    schedules_df = schedules_df.drop(index=list(rows_to_drop)).reset_index(drop=True)\n",
    "\n",
    "    return schedules_df\n",
    "\n",
    "\n",
    "# Re-run the anomaly detection\n",
    "# Preprocess the schedules\n",
    "schedules_df_final = preprocess_schedules_final(schedules_df)\n",
    "anomalies_df_final = check_sailing_vs_arrival_anomalies(schedules_df_final)\n",
    "\n",
    "# Print anomalies if any exist\n",
    "if not anomalies_df_final.empty:\n",
    "    print(\"Anomalies found where sailingDate is greater than the next row's arrivalDate:\")\n",
    "    print(anomalies_df_final)\n",
    "else:\n",
    "    print(\"No anomalies found.\")\n",
    "    \n",
    "anomalies_df_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da8d942f-0fbd-4065-ac2f-5fa4bd625841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522065 entries, 0 to 1522064\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count    Dtype         \n",
      "---  ------        --------------    -----         \n",
      " 0   time          1522065 non-null  datetime64[ns]\n",
      " 1   cog           1516207 non-null  float64       \n",
      " 2   sog           1522065 non-null  float64       \n",
      " 3   rot           1522065 non-null  float64       \n",
      " 4   heading       1517169 non-null  float64       \n",
      " 5   navstat       1522065 non-null  int64         \n",
      " 6   etaRaw        1506820 non-null  datetime64[ns]\n",
      " 7   latitude      1522065 non-null  float64       \n",
      " 8   longitude     1522065 non-null  float64       \n",
      " 9   vesselId      1522065 non-null  object        \n",
      " 10  portId        1520450 non-null  object        \n",
      " 11  navstat_desc  1522065 non-null  object        \n",
      "dtypes: datetime64[ns](2), float64(6), int64(1), object(3)\n",
      "memory usage: 139.3+ MB\n"
     ]
    }
   ],
   "source": [
    "ais_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "682f98cf-ecdc-4545-b86f-f243eedafc39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 711 entries, 0 to 710\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   shippingLineId  711 non-null    object \n",
      " 1   vesselId        711 non-null    object \n",
      " 2   CEU             711 non-null    int64  \n",
      " 3   DWT             703 non-null    float64\n",
      " 4   GT              711 non-null    int64  \n",
      " 5   vesselType      699 non-null    float64\n",
      " 6   breadth         703 non-null    float64\n",
      " 7   enginePower     691 non-null    float64\n",
      " 8   homePort        573 non-null    object \n",
      " 9   length          711 non-null    float64\n",
      " 10  maxSpeed        213 non-null    float64\n",
      " 11  age             711 non-null    int64  \n",
      "dtypes: float64(6), int64(3), object(3)\n",
      "memory usage: 66.8+ KB\n"
     ]
    }
   ],
   "source": [
    "vessels_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "607fec47-901b-4bc1-a825-2b1021ab94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1329 entries, 0 to 1328\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   portId     1329 non-null   object \n",
      " 1   longitude  1329 non-null   float64\n",
      " 2   latitude   1329 non-null   float64\n",
      " 3   ISO        1329 non-null   object \n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 41.7+ KB\n"
     ]
    }
   ],
   "source": [
    "ports_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98c04a0b-caea-4dc4-a3f9-678c4ad66c48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12724 entries, 0 to 12723\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   vesselId             12724 non-null  object        \n",
      " 1   arrivalDate          12724 non-null  datetime64[ns]\n",
      " 2   sailingDate          12724 non-null  datetime64[ns]\n",
      " 3   portId               12724 non-null  object        \n",
      " 4   portLatitude         12724 non-null  float64       \n",
      " 5   portLongitude        12724 non-null  float64       \n",
      " 6   small_movement_flag  12724 non-null  int64         \n",
      " 7   skipped_port_flag    12724 non-null  int64         \n",
      "dtypes: datetime64[ns](2), float64(2), int64(2), object(2)\n",
      "memory usage: 795.4+ KB\n"
     ]
    }
   ],
   "source": [
    "schedules_df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db363d8-6b8d-49fe-9ae0-88b3ff08ff2b",
   "metadata": {},
   "source": [
    "### Merge code to datasets in batches due to memory constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7888b824-d67d-477d-9766-f516bfe8d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_in_batches(\n",
    "    ais_train_df,\n",
    "    vessels_df,\n",
    "    schedules_df,\n",
    "    ports_df,\n",
    "    batch_size=200,\n",
    "    gcs_bucket_path=\"gs://jacobsbucketformlproject/ML Competition/\"\n",
    "):\n",
    "    # Get unique vessel IDs\n",
    "    vessel_ids = ais_train_df['vesselId'].unique()\n",
    "\n",
    "    # Split vessel IDs into batches\n",
    "    vessel_id_batches = [vessel_ids[i:i + batch_size] for i in range(0, len(vessel_ids), batch_size)]\n",
    "\n",
    "    # Process each batch\n",
    "    for batch_num, vessel_id_batch in enumerate(vessel_id_batches, start=1):\n",
    "        print(f'Processing batch {batch_num}/{len(vessel_id_batches)}')\n",
    "\n",
    "        # Filter AIS data for the current batch\n",
    "        ais_batch_df = ais_train_df[ais_train_df['vesselId'].isin(vessel_id_batch)]\n",
    "\n",
    "        # Filter schedules data for the current batch\n",
    "        schedules_batch_df = schedules_df[schedules_df['vesselId'].isin(vessel_id_batch)]\n",
    "\n",
    "        # Merge schedules data with ports data (left join)\n",
    "        schedules_batch_df = schedules_batch_df.merge(\n",
    "            ports_df,\n",
    "            on='portId', how='left'\n",
    "        )\n",
    "\n",
    "        # Create a list to collect merged dataframes per vessel\n",
    "        merged_dfs = []\n",
    "\n",
    "        # Process each vessel individually to manage memory usage and ensure time order\n",
    "        for vessel_id in vessel_id_batch:\n",
    "            # Filter data for this vessel\n",
    "            ais_vessel_df = ais_batch_df[ais_batch_df['vesselId'] == vessel_id].sort_values('time').reset_index(drop=True)\n",
    "            schedules_vessel_df = schedules_batch_df[schedules_batch_df['vesselId'] == vessel_id].sort_values('arrivalDate').reset_index(drop=True)\n",
    "\n",
    "            # Rename AIS portId to ais_portId\n",
    "            if 'portId' in ais_vessel_df.columns:\n",
    "                ais_vessel_df.rename(columns={'portId': 'ais_portId'}, inplace=True)\n",
    "\n",
    "            # Initialize schedule-related columns with NaN\n",
    "            schedule_cols = [\n",
    "                'schedule_arrivalDate', 'schedule_sailingDate', 'schedule_moored_portId',\n",
    "                'schedule_moored_portLatitude', 'schedule_moored_portLongitude',\n",
    "                'schedule_voyage_end',\n",
    "                'schedule_destination_portId',\n",
    "                'schedule_destination_portLatitude',\n",
    "                'schedule_destination_portLongtitude',\n",
    "                'schedule_small_movement_flag', 'schedule_skipped_port_flag'\n",
    "            ]\n",
    "\n",
    "            if schedules_vessel_df.empty:\n",
    "                # Assign NaN for schedule-related columns\n",
    "                for col in schedule_cols:\n",
    "                    ais_vessel_df[col] = pd.NA\n",
    "                merged_vessel_df = ais_vessel_df\n",
    "            else:\n",
    "                # Create events (port stays and voyages)\n",
    "                events = []\n",
    "\n",
    "                # Add events\n",
    "                for idx in range(len(schedules_vessel_df) - 1):\n",
    "                    arrivalDate = schedules_vessel_df.loc[idx, 'arrivalDate']\n",
    "                    voyage_start = schedules_vessel_df.loc[idx, 'sailingDate']\n",
    "                    voyage_end = schedules_vessel_df.loc[idx + 1, 'arrivalDate']\n",
    "                    voyage_start_portId = schedules_vessel_df.loc[idx, 'portId']\n",
    "                    mooredPortLongitude = schedules_vessel_df.loc[idx, 'portLongitude']\n",
    "                    mooredPortLatitude = schedules_vessel_df.loc[idx, 'portLatitude']\n",
    "                    voyage_end_portId = schedules_vessel_df.loc[idx + 1, 'portId']\n",
    "                    destinationLatitude = schedules_vessel_df.loc[idx + 1, 'portLatitude'] \n",
    "                    destinationLongitude = schedules_vessel_df.loc[idx + 1, 'portLongitude']\n",
    "                    small_movement_flag = schedules_vessel_df.loc[idx, 'small_movement_flag']\n",
    "                    skipped_port_flag = schedules_vessel_df.loc[idx,'skipped_port_flag']\n",
    "                    event = {\n",
    "                        'start_time': arrivalDate,\n",
    "                        'end_time': voyage_end,\n",
    "                        'schedule_arrivalDate': arrivalDate,\n",
    "                        'schedule_sailingDate': voyage_start,\n",
    "                        'schedule_moored_portId': voyage_start_portId,\n",
    "                        'schedule_moored_portLatitude': mooredPortLatitude,\n",
    "                        'schedule_moored_portLongitude': mooredPortLongitude,\n",
    "                        'schedule_voyage_end': voyage_end,\n",
    "                        'schedule_destination_portId': voyage_end_portId,\n",
    "                        'schedule_destination_portLatitude': destinationLatitude,\n",
    "                        'schedule_destination_portLongtitude': destinationLongitude,\n",
    "                        'schedule_small_movement_flag': small_movement_flag,\n",
    "                        'schedule_skipped_port_flag': skipped_port_flag\n",
    "                    }\n",
    "                    events.append(event)\n",
    "\n",
    "                # Handle the last voyage after the last port stay\n",
    "                last_idx = len(schedules_vessel_df)-1\n",
    "                arrivalDate = schedules_vessel_df.loc[last_idx, 'arrivalDate']\n",
    "                voyage_start = schedules_vessel_df.loc[last_idx, 'sailingDate']\n",
    "                voyage_start_portId = schedules_vessel_df.loc[last_idx, 'portId']\n",
    "                mooredPortLongitude = schedules_vessel_df.loc[last_idx, 'portLongitude']\n",
    "                mooredPortLatitude = schedules_vessel_df.loc[last_idx, 'portLatitude']\n",
    "                small_movement_flag = schedules_vessel_df.loc[last_idx, 'small_movement_flag']\n",
    "                skipped_port_flag = schedules_vessel_df.loc[last_idx,'skipped_port_flag']\n",
    "                voyage_end = pd.NaT # No known destination time\n",
    "                voyage_end_portId = pd.NA  # No known destination port\n",
    "                destinationLatitude = pd.NA\n",
    "                destinationLongitude = pd.NA\n",
    "                event = {\n",
    "                        'start_time': arrivalDate,\n",
    "                        'end_time': voyage_start,\n",
    "                        'schedule_arrivalDate': arrivalDate,\n",
    "                        'schedule_sailingDate': voyage_start,\n",
    "                        'schedule_moored_portId': voyage_start_portId,\n",
    "                        'schedule_moored_portLatitude': mooredPortLatitude,\n",
    "                        'schedule_moored_portLongitude': mooredPortLongitude,\n",
    "                        'schedule_voyage_end': voyage_end,\n",
    "                        'schedule_destination_portId': voyage_end_portId,\n",
    "                        'schedule_destination_portLatitude': destinationLatitude,\n",
    "                        'schedule_destination_portLongtitude': destinationLongitude,\n",
    "                        'schedule_small_movement_flag': small_movement_flag,\n",
    "                        'schedule_skipped_port_flag': skipped_port_flag\n",
    "                }\n",
    "                events.append(event)\n",
    "\n",
    "                # Convert events to DataFrame\n",
    "                events_df = pd.DataFrame(events)\n",
    "\n",
    "                # Now process each event\n",
    "                data_points = []\n",
    "\n",
    "                for idx, event in events_df.iterrows():\n",
    "                    start_time = event['start_time']\n",
    "                    end_time = event['end_time']\n",
    "\n",
    "                    # Get AIS data points within this event period\n",
    "                    ais_event_df = ais_vessel_df[(ais_vessel_df['time'] >= start_time) & (ais_vessel_df['time'] <= end_time)].copy()\n",
    "\n",
    "                    if not ais_event_df.empty:\n",
    "                        # Assign event data to AIS data points\n",
    "                        for col in schedule_cols:\n",
    "                            ais_event_df[col] = event.get(col, pd.NA)\n",
    "                        data_points.append(ais_event_df)\n",
    "                    else:\n",
    "                        # Create synthetic data point with NaN AIS data and event data\n",
    "                        synthetic_point = {col: pd.NA for col in ais_vessel_df.columns if col != 'vesselId'}\n",
    "                        synthetic_point['vesselId'] = vessel_id  # Ensure vesselId is set\n",
    "                        synthetic_point['time'] = pd.NaT  # Set time to NaT\n",
    "\n",
    "                        # Assign event data to synthetic point\n",
    "                        for col in schedule_cols:\n",
    "                            synthetic_point[col] = event.get(col, pd.NA)\n",
    "\n",
    "                        synthetic_df = pd.DataFrame([synthetic_point])\n",
    "                        data_points.append(synthetic_df)\n",
    "\n",
    "                # Include any AIS data points not covered by events\n",
    "                # Collect all event intervals\n",
    "                event_intervals = [(event['start_time'], event['end_time']) for _, event in events_df.iterrows()]\n",
    "                # Function to check if a time is within any event interval\n",
    "                def is_in_event(time):\n",
    "                    return any((time >= start) and (time <= end) for (start, end) in event_intervals)\n",
    "\n",
    "                # Identify AIS data points not in any event\n",
    "                ais_outside_events_df = ais_vessel_df[~ais_vessel_df['time'].apply(is_in_event)].copy()\n",
    "                if not ais_outside_events_df.empty:\n",
    "                    # Assign NaNs to schedule columns\n",
    "                    for col in schedule_cols:\n",
    "                        ais_outside_events_df[col] = pd.NA\n",
    "                    data_points.append(ais_outside_events_df)\n",
    "\n",
    "                # Collect all data points\n",
    "                merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
    "\n",
    "                # Create a sort key for merged_vessel_df\n",
    "                # For rows with valid 'time', use 'time' as sort key\n",
    "                # For synthetic data points with 'time' as NaT, use 'arrivalDate' or 'voyage_start'\n",
    "                merged_vessel_df['sort_time'] = merged_vessel_df.apply(\n",
    "                    lambda row: row['time'] if pd.notnull(row['time']) else (\n",
    "                        row['schedule_arrivalDate'] if pd.notnull(row['schedule_arrivalDate']) else pd.Timestamp.max\n",
    "                    ),\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                # Now sort by 'sort_time'\n",
    "                merged_vessel_df.sort_values('sort_time', inplace=True)\n",
    "                merged_vessel_df.drop(columns=['sort_time'], inplace=True)\n",
    "                merged_vessel_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Merge with vessel specifications to ensure vessel data is present\n",
    "            merged_vessel_df = merged_vessel_df.merge(vessels_df, on='vesselId', how='left')\n",
    "\n",
    "            # Append to the list of merged dataframes\n",
    "            merged_dfs.append(merged_vessel_df)\n",
    "\n",
    "        # Concatenate merged dataframes for the batch\n",
    "        if merged_dfs:\n",
    "            merged_batch_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "\n",
    "            # Save the merged batch to Google Cloud Storage\n",
    "            output_path = f\"{gcs_bucket_path}merged_data_batch_{batch_num}.csv\"\n",
    "            merged_batch_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        else:\n",
    "            print(f'No data to merge for batch {batch_num}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b9d52-e185-4251-8b8d-29a49c39a51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
      "/tmp/ipykernel_4066/1098879623.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_vessel_df = pd.concat(data_points, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "merge_data_in_batches(\n",
    "    ais_train_df=ais_train_df,\n",
    "    vessels_df=vessels_df,\n",
    "    schedules_df=schedules_df_final,\n",
    "    ports_df=ports_df,\n",
    "    batch_size=200,\n",
    "    gcs_bucket_path=\"gs://jacobsbucketformlproject/ML Competition/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e32e0f0-1bea-41d4-b364-a11cc186f74d",
   "metadata": {},
   "source": [
    "First we must create a function that gives voyage info so our merging with schedules becomes more useful!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8c709-029f-4f4a-964c-b5c5121a18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clear_dataframes(*dfs):\n",
    "    \"\"\"\n",
    "    Remove specified dataframes from memory and trigger garbage collection.\n",
    "\n",
    "    Parameters:\n",
    "    - dfs: A list of dataframe variables to be removed.\n",
    "    \"\"\"\n",
    "    for df in dfs:\n",
    "        del df\n",
    "    # Force garbage collection to free up memory\n",
    "    gc.collect()\n",
    "\n",
    "# Example usage:\n",
    "# Assume ais_train_df, vessels_df, schedules_df, and ports_df were used for merging\n",
    "clear_dataframes(ais_train_df, vessels_df, schedules_df, ports_df)\n",
    "\n",
    "# After calling this, the dataframes will be removed from memory, and memory will be freed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316ba0a-c48e-41eb-a8b0-21f6ba8399fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the first batch in your GCS bucket\n",
    "first_batch_path = 'gs://jacobsbucketformlproject/ML Competition/merged_data_batch_1.csv'\n",
    "\n",
    "# Load the first batch into a pandas DataFrame\n",
    "first_batch_df = pd.read_csv(first_batch_path, delimiter=',', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the DataFrame to inspect\n",
    "first_batch_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4692d-09f0-4b17-a448-fb655a9897eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing values in the merged dataframe\n",
    "\n",
    "missing_values = first_batch_df.isna().sum()\n",
    "\n",
    "# Display columns with missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values[missing_values > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8a470-0b1d-44c7-9ce1-00cdffacaef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_batch_df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384438a-616b-4ff3-b490-da11ea8eba41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_batch_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07900ae-5320-43d6-8e6d-3fe57c982be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_batch_path = 'gs://jacobsbucketformlproject/ML Competition/merged_data_batch_2.csv'\n",
    "# Load the first batch into a pandas DataFrame\n",
    "second_batch_df = pd.read_csv(second_batch_path, delimiter=',', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the DataFrame to inspect\n",
    "second_batch_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d7f47-ccca-4d52-ab7e-099e3c662a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing values in the merged dataframe\n",
    "\n",
    "missing_values1 = second_batch_df.isna().sum()\n",
    "\n",
    "# Display columns with missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values1[missing_values1 > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f825efe-6c67-41a6-9f45-bbf4dbcff768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_batch_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83595e4b-fb05-45aa-bd4a-91223ed8520c",
   "metadata": {},
   "source": [
    "# Summary!!!\n",
    "\n",
    "Merge looks good! Now we must use the batches to resample time points and feature engineer, before model development. Will use different ways of resampling and feature engineering based on model. So this is the base dataset I now will work on."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
