{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a586a6-b58a-49e0-afd5-070d2e0470ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Data visualization\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calculations\n",
    "import math\n",
    "from haversine import haversine, Unit\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "#Cloud storage\n",
    "from google.cloud import storage\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42559ad2-7e00-40d7-880c-8f4092155732",
   "metadata": {},
   "source": [
    "We load the necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e02f6da-78cd-4dd8-a8f3-418e14ec7026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = '../../original_data/ais/ais_train.csv'\n",
    "\n",
    "# Load the file into a pandas dataframe\n",
    "ais_train_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "# Display the dataframe\n",
    "ais_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ef9df-6a2d-43d1-915c-7726fd32a221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = '../../original_data/ports/ports.csv'\n",
    "\n",
    "#Load the file into a pandas dataframe\n",
    "ports_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "ports_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892f075-fe16-4095-a5ab-8ba91ba3c159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = '../../original_data/ais/schedules_to_may_2024.csv'\n",
    "\n",
    "#Load the file into a pandas dataframe\n",
    "schedules_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "schedules_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7e81a-fb28-4c9a-9525-48f40a9442c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = '../../original_data/vessels/vessels.csv'\n",
    "\n",
    "#Load the file into a pandas dataframe\n",
    "vessels_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "vessels_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f548182-0679-45c5-95cc-94110953667a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MAP_LAND_PATH = \"gs://jacobsbucketformlproject/ML Competition/ne_10m_land.zip\" # Path to the land zip file\n",
    "#MAP_OCEAN_PATH = \"gs://jacobsbucketformlproject/ML Competition/ne_10m_ocean.zip\" # Path to the ocean zip file\n",
    "#land_world = gpd.read_file(MAP_LAND_PATH)\n",
    "#ocean_world = gpd.read_file(MAP_OCEAN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843db903-f622-4a52-9654-de36953e644b",
   "metadata": {},
   "source": [
    "Convert all timestamps to datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b7ca0-6987-45e9-a5d3-3db9e6c93395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_ais_train(ais_train_df):\n",
    "    \"\"\"\n",
    "    Preprocess the ais_train_df by converting columns, handling missing or invalid values, \n",
    "    and mapping NAVSTAT codes to descriptions.\n",
    "\n",
    "    Parameters:\n",
    "    - ais_train_df: DataFrame containing the raw AIS train data.\n",
    "\n",
    "    Returns:\n",
    "    - ais_train_df_cleaned: A cleaned and preprocessed version of ais_train_df.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert 'time' and 'etaRaw' to datetime\n",
    "    ais_train_df['time'] = pd.to_datetime(ais_train_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    ais_train_df['etaRaw'] = pd.to_datetime(ais_train_df['etaRaw'], errors='coerce', format='%m-%d %H:%M')\n",
    "    \n",
    "    # Step 2: Add the year 2024 to 'etaRaw' column\n",
    "    ais_train_df['etaRaw'] = ais_train_df['etaRaw'].apply(lambda x: x.replace(year=2024) if pd.notna(x) else pd.NaT)\n",
    "    \n",
    "    # Step 3: Convert relevant columns to float\n",
    "    ais_train_df['cog'] = ais_train_df['cog'].astype(float)\n",
    "    ais_train_df['sog'] = ais_train_df['sog'].astype(float)\n",
    "    ais_train_df['rot'] = ais_train_df['rot'].astype(float)\n",
    "    ais_train_df['heading'] = ais_train_df['heading'].astype(float)\n",
    "    ais_train_df['latitude'] = ais_train_df['latitude'].astype(float)\n",
    "    ais_train_df['longitude'] = ais_train_df['longitude'].astype(float)\n",
    "    \n",
    "    # Step 4: Replace invalid or default values with NaN\n",
    "    ais_train_df['cog'] = np.where((ais_train_df['cog'] == 360) | (ais_train_df['cog'] > 360), np.nan, ais_train_df['cog'])\n",
    "    ais_train_df['sog'] = np.where((ais_train_df['sog'] == 1023), np.nan, ais_train_df['sog'])\n",
    "    ais_train_df['rot'] = np.where((ais_train_df['rot'] == -128), np.nan, ais_train_df['rot'])\n",
    "    ais_train_df['heading'] = np.where((ais_train_df['heading'] > 360) | (ais_train_df['heading'] == 511), np.nan, ais_train_df['heading'])\n",
    "    \n",
    "    # Step 5: Map NAVSTAT codes to descriptive statuses\n",
    "    navstat_mapping = {\n",
    "        0: 'Under way using engine',\n",
    "        1: 'At anchor',\n",
    "        2: 'Not under command',\n",
    "        3: 'Restricted manoeuvrability',\n",
    "        4: 'Constrained by her draught',\n",
    "        5: 'Moored',\n",
    "        6: 'Aground',\n",
    "        7: 'Engaged in fishing',\n",
    "        8: 'Under way sailing',\n",
    "        15: 'Undefined'\n",
    "    }\n",
    "    ais_train_df['navstat_desc'] = ais_train_df['navstat'].map(navstat_mapping)\n",
    "    ais_train_df['navstat_desc'].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    ais_train_df = ais_train_df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "    return ais_train_df\n",
    "\n",
    "ais_train_df = preprocess_ais_train(ais_train_df)\n",
    "\n",
    "ais_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf785408-2d26-46aa-b5f2-a37c33d0f607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_vessels(vessels_df):\n",
    "    \"\"\"\n",
    "    Preprocess the vessels_df by converting 'yearBuilt' to 'age', \n",
    "    handling missing values, and dropping unnecessary columns.\n",
    "\n",
    "    Parameters:\n",
    "    - vessels_df: DataFrame containing the raw vessels data.\n",
    "\n",
    "    Returns:\n",
    "    - vessels_df_cleaned: A cleaned and preprocessed version of vessels_df.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate the current year and create 'age' column\n",
    "    current_year = 2024\n",
    "    vessels_df['age'] = vessels_df['yearBuilt'].apply(lambda x: current_year - x if pd.notna(x) else np.nan)\n",
    "\n",
    "    # Step 2: Drop the 'yearBuilt' column\n",
    "    vessels_df = vessels_df.drop(columns=['yearBuilt'])\n",
    "\n",
    "    # Step 3: Drop columns with high missing values and low predictive power\n",
    "    columns_to_drop = ['NT', 'depth', 'draft', 'freshWater', 'fuel', 'maxHeight', 'maxWidth', 'rampCapacity']\n",
    "    vessels_df = vessels_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    return vessels_df\n",
    "\n",
    "vessels_df = preprocess_vessels(vessels_df)\n",
    "\n",
    "vessels_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e8389-0c16-4b0a-8eeb-ef17f42c420f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_ports(ports_df):\n",
    "    \"\"\"\n",
    "    Preprocess the ports_df by dropping unnecessary columns based on redundancy.\n",
    "\n",
    "    Parameters:\n",
    "    - ports_df: DataFrame containing the raw ports data.\n",
    "\n",
    "    Returns:\n",
    "    - ports_df_cleaned: A cleaned and preprocessed version of ports_df.\n",
    "    \"\"\"\n",
    "    # Step 1: Drop columns based on redundancy\n",
    "    columns_to_drop = ['name','portLocation', 'UN_LOCODE', 'countryName']\n",
    "    ports_df = ports_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    return ports_df\n",
    "\n",
    "ports_df = preprocess_ports(ports_df)\n",
    "\n",
    "ports_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d57473-3888-4ba4-9f8c-0db8f37d8c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_schedules(schedules_df):\n",
    "    \"\"\"\n",
    "    Preprocess the schedules_df by converting timestamps, handling invalid rows,\n",
    "    removing duplicates, and filtering out conflicting port information.\n",
    "\n",
    "    Parameters:\n",
    "    - schedules_df: DataFrame containing the raw schedules data.\n",
    "\n",
    "    Returns:\n",
    "    - schedules_df_cleaned: A cleaned and preprocessed version of schedules_df.\n",
    "    \"\"\"\n",
    "    # Make an explicit copy of the DataFrame to avoid working with a slice\n",
    "    schedules_df = schedules_df.copy()\n",
    "\n",
    "    # Step 1: Remove duplicates\n",
    "    schedules_df = schedules_df.drop_duplicates()\n",
    "\n",
    "    # Step 2: Convert 'arrivalDate' and 'sailingDate' to datetime format\n",
    "    schedules_df['arrivalDate'] = pd.to_datetime(schedules_df['arrivalDate'], errors='coerce')\n",
    "    schedules_df['sailingDate'] = pd.to_datetime(schedules_df['sailingDate'], errors='coerce')\n",
    "\n",
    "    # Step 3: Remove timezone info to match the format of 'ais_train'\n",
    "    schedules_df['arrivalDate'] = schedules_df['arrivalDate'].dt.tz_localize(None)\n",
    "    schedules_df['sailingDate'] = schedules_df['sailingDate'].dt.tz_localize(None)\n",
    "\n",
    "    # Step 4: Drop rows with NaN values for vesselID, portId\n",
    "    schedules_df = schedules_df.dropna(subset=['vesselId', 'portId'])\n",
    "\n",
    "    # Step 5: Remove redundant columns ('portName', 'shippingLineName')\n",
    "    columns_to_drop = ['portName','shippingLineId', 'shippingLineName']\n",
    "    schedules_df = schedules_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Step 6: Remove rows where 'sailingDate' is before 'arrivalDate'\n",
    "    schedules_df = schedules_df[schedules_df['sailingDate'] >= schedules_df['arrivalDate']].copy()\n",
    "\n",
    "    # Step 7: Handle conflicting port information\n",
    "    # Drop duplicates based on 'vesselId', 'arrivalDate', 'sailingDate', and 'portId'\n",
    "    schedules_no_port_duplicates = schedules_df.drop_duplicates(subset=['vesselId', 'arrivalDate', 'sailingDate', 'portId'])\n",
    "\n",
    "    # Identify conflicting entries with the same 'vesselId', 'arrivalDate', 'sailingDate' but different 'portId'\n",
    "    vessel_date_duplicates = schedules_no_port_duplicates[schedules_no_port_duplicates.duplicated(subset=['vesselId', 'arrivalDate', 'sailingDate'], keep=False)]\n",
    "\n",
    "    # Remove all conflicting rows from the original DataFrame\n",
    "    schedules_df_cleaned = schedules_df[~schedules_df.index.isin(vessel_date_duplicates.index)]\n",
    "\n",
    "    # Step 8: Remove exact duplicates (same 'vesselId', 'arrivalDate', 'sailingDate', 'portId')\n",
    "    schedules_df_cleaned = schedules_df_cleaned.drop_duplicates(subset=['vesselId', 'arrivalDate', 'sailingDate', 'portId'])\n",
    "\n",
    "    # Step 9: Sort the cleaned DataFrame by 'vesselId' and 'arrivalDate' to ensure proper ordering\n",
    "    schedules_df_cleaned = schedules_df_cleaned.sort_values(by=['vesselId', 'arrivalDate']).reset_index(drop=True)\n",
    "\n",
    "    return schedules_df_cleaned\n",
    "\n",
    "schedules_df = preprocess_schedules(schedules_df)\n",
    "\n",
    "schedules_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a215526-2833-465f-b8c5-7182e3181d3f",
   "metadata": {},
   "source": [
    "# We now merge the datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c60cf6-d8ff-4538-a468-f48712fbadbb",
   "metadata": {},
   "source": [
    "Before mergin we must check that all the data we have behaves as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3d49e-3cdd-4b11-8e29-b453ce652f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_timestamp_anomalies(ais_train_df):\n",
    "    \"\"\"\n",
    "    Check if the 'time' column for each vessel in the ais_train_df is in ascending order.\n",
    "    Identify any vessels with timestamp anomalies (non-ascending order).\n",
    "    \n",
    "    Parameters:\n",
    "    - ais_train_df: The AIS data containing vessel information and timestamps.\n",
    "\n",
    "    Returns:\n",
    "    - A list of vessel IDs with timestamp anomalies and the corresponding rows with issues.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store vessel IDs with anomalies\n",
    "    vessels_with_anomalies = []\n",
    "\n",
    "    # Group by 'vesselId' and check if the 'time' column is sorted for each group\n",
    "    for vessel_id, vessel_data in ais_train_df.groupby('vesselId'):\n",
    "        # Sort the data by 'time' to ensure correct order\n",
    "        sorted_data = vessel_data.sort_values('time').reset_index(drop=True)\n",
    "        \n",
    "        # Check if the 'time' column is strictly increasing\n",
    "        if not sorted_data['time'].is_monotonic_increasing:\n",
    "            print(f\"Anomaly detected in vessel ID: {vessel_id}\")\n",
    "            \n",
    "            # Add vessel ID to the list of anomalies\n",
    "            vessels_with_anomalies.append(vessel_id)\n",
    "            \n",
    "            # Print the rows where anomalies exist (timestamps are not in ascending order)\n",
    "            incorrect_rows = sorted_data[sorted_data['time'].diff().dt.total_seconds() < 0]\n",
    "            print(f\"Problematic rows for vessel ID {vessel_id}:\\n\", incorrect_rows[['time', 'latitude', 'longitude']])\n",
    "\n",
    "    # Final report\n",
    "    if vessels_with_anomalies:\n",
    "        print(f\"\\nNumber of vessels with timestamp anomalies: {len(vessels_with_anomalies)}\")\n",
    "    else:\n",
    "        print(\"\\nNo timestamp anomalies found. All timestamps are in ascending order for each vessel.\")\n",
    "    \n",
    "    return vessels_with_anomalies\n",
    "\n",
    "\n",
    "# Test the function on ais_train_df\n",
    "vessels_with_timestamp_anomalies = check_timestamp_anomalies(ais_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac27557-a614-4730-8337-10dec7ce2932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_duplicate_rows(schedules_df):\n",
    "    \"\"\"\n",
    "    Check for duplicate rows in the schedules_cleaned_df based on 'vesselId', 'arrivalDate', 'sailingDate' and 'portId'.\n",
    "\n",
    "    Parameters:\n",
    "    - schedules_cleaned_df: The schedules data containing vessel information and timestamps.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame of duplicate rows.\n",
    "    \"\"\"\n",
    "    # Check for duplicates based on 'vesselId', 'arrivalDate', 'sailingDate' and 'PortId\n",
    "    duplicate_rows = schedules_df[schedules_df.duplicated(subset=['vesselId', 'arrivalDate', 'sailingDate','portId'], keep=False)]\n",
    "\n",
    "    if not duplicate_rows.empty:\n",
    "        print(f\"Number of duplicate rows found: {len(duplicate_rows)}\")\n",
    "        print(duplicate_rows[['vesselId', 'arrivalDate', 'sailingDate', 'portId']])\n",
    "    else:\n",
    "        print(\"No duplicate rows found.\")\n",
    "\n",
    "    return duplicate_rows\n",
    "\n",
    "\n",
    "# Run the duplicate check\n",
    "duplicate_anomalies = check_duplicate_rows(schedules_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e8c89-be67-48a2-bdad-22e88ccb811c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_schedule_timestamp_anomalies_complete(schedules_df):\n",
    "    \"\"\"\n",
    "    Check if 'arrivalDate' is in ascending order for each vessel and if 'sailingDate' is before 'arrivalDate'.\n",
    "    Allow cases where 'sailingDate' is the same as 'arrivalDate'.\n",
    "\n",
    "    Parameters:\n",
    "    - schedules_cleaned_df: The schedules data containing vessel information and timestamps.\n",
    "\n",
    "    Returns:\n",
    "    - A list of vessel IDs with anomalies and a DataFrame of rows with issues.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store vessel IDs with anomalies\n",
    "    vessels_with_anomalies = []\n",
    "    \n",
    "    # Initialize a DataFrame to store rows with issues\n",
    "    anomaly_rows = pd.DataFrame()\n",
    "\n",
    "    # Group by 'vesselId' and check for ascending 'arrivalDate' and valid 'sailingDate'\n",
    "    for vessel_id, vessel_data in schedules_df.groupby('vesselId'):\n",
    "        # Check if 'arrivalDate' is in ascending order\n",
    "        if not vessel_data['arrivalDate'].is_monotonic_increasing:\n",
    "            print(f\"ArrivalDate anomaly detected in vessel ID: {vessel_id}\")\n",
    "            vessels_with_anomalies.append(vessel_id)\n",
    "            \n",
    "            # Identify rows where 'arrivalDate' is not in ascending order\n",
    "            arrival_anomalies = vessel_data[vessel_data['arrivalDate'].diff().dt.total_seconds() < 0]\n",
    "            anomaly_rows = pd.concat([anomaly_rows, arrival_anomalies])\n",
    "\n",
    "        # Check if 'sailingDate' is before 'arrivalDate' (not equal)\n",
    "        sailing_anomalies = vessel_data[vessel_data['sailingDate'] < vessel_data['arrivalDate']]\n",
    "        if not sailing_anomalies.empty:\n",
    "            print(f\"SailingDate anomaly detected in vessel ID: {vessel_id}\")\n",
    "            vessels_with_anomalies.append(vessel_id)\n",
    "            anomaly_rows = pd.concat([anomaly_rows, sailing_anomalies])\n",
    "    \n",
    "    # Final report\n",
    "    if not anomaly_rows.empty:\n",
    "        print(f\"\\nNumber of vessels with timestamp anomalies: {len(vessels_with_anomalies)}\")\n",
    "        print(anomaly_rows[['vesselId', 'arrivalDate', 'sailingDate', 'portId']])\n",
    "    else:\n",
    "        print(\"\\nNo anomalies found. All timestamps are in ascending order and valid.\")\n",
    "    \n",
    "    return vessels_with_anomalies, anomaly_rows\n",
    "\n",
    "# Run the updated anomaly check on schedules_df\n",
    "vessels_with_anomalies, anomaly_rows = check_schedule_timestamp_anomalies_complete(schedules_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1f24d-2b96-4b7d-9901-749c8d2bc237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_sailing_vs_arrival_anomalies(schedules_df):\n",
    "    \"\"\"\n",
    "    Sorts schedules for each vessel by 'arrivalDate' and checks if the 'sailingDate' \n",
    "    of one row is greater than the 'arrivalDate' of the next row.\n",
    "\n",
    "    Parameters:\n",
    "    - schedules_df: DataFrame containing schedules data.\n",
    "\n",
    "    Returns:\n",
    "    - anomalies_df: DataFrame containing rows where 'sailingDate' is greater \n",
    "      than the 'arrivalDate' of the next row.\n",
    "    \"\"\"\n",
    "    # Create an empty list to store anomalies\n",
    "    anomalies = []\n",
    "\n",
    "    # Get unique vessel IDs\n",
    "    vessel_ids = schedules_df['vesselId'].unique()\n",
    "\n",
    "    # Loop through each vessel and perform the check\n",
    "    for vessel_id in vessel_ids:\n",
    "        # Filter schedules for this vessel and sort by arrivalDate\n",
    "        vessel_schedules_df = schedules_df[schedules_df['vesselId'] == vessel_id].sort_values('arrivalDate').reset_index(drop=True)\n",
    "        \n",
    "        # Iterate through rows to check if sailingDate is greater than the arrivalDate of the next row\n",
    "        for i in range(len(vessel_schedules_df) - 1):\n",
    "            current_sailing_date = vessel_schedules_df.loc[i, 'sailingDate']\n",
    "            next_arrival_date = vessel_schedules_df.loc[i + 1, 'arrivalDate']\n",
    "            \n",
    "            # If the current row's sailingDate is greater than the next row's arrivalDate, record it\n",
    "            if current_sailing_date > next_arrival_date:\n",
    "                anomalies.append({\n",
    "                    'vesselId': vessel_id,\n",
    "                    'sailingDate': current_sailing_date,\n",
    "                    'next_arrivalDate': next_arrival_date,\n",
    "                    'index_current': i,\n",
    "                    'index_next': i + 1\n",
    "                })\n",
    "\n",
    "    # Convert the list of anomalies to a DataFrame for inspection\n",
    "    anomalies_df = pd.DataFrame(anomalies)\n",
    "\n",
    "    return anomalies_df\n",
    "\n",
    "# Assuming schedules_df_cleaned is the cleaned schedules DataFrame\n",
    "anomalies_df = check_sailing_vs_arrival_anomalies(schedules_df)\n",
    "\n",
    "# Print anomalies if any exist\n",
    "if not anomalies_df.empty:\n",
    "    print(\"Anomalies found where sailingDate is greater than the next row's arrivalDate:\")\n",
    "    print(anomalies_df)\n",
    "else:\n",
    "    print(\"No anomalies found.\")\n",
    "    \n",
    "schedules_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae88ec2-1af1-434d-94b3-536beb24a931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_schedules_final(schedules_df):\n",
    "    \"\"\"\n",
    "    Preprocess schedules to handle infeasible arrival dates and small movements at the same port.\n",
    "\n",
    "    Parameters:\n",
    "    - schedules_df: DataFrame with schedules data.\n",
    "\n",
    "    Returns:\n",
    "    - schedules_df: Updated schedules DataFrame with added flags for small movements and skipped ports.\n",
    "    \"\"\"\n",
    "    schedules_df = schedules_df.copy()\n",
    "\n",
    "    # Initialize new features\n",
    "    schedules_df['small_movement_flag'] = 0\n",
    "    schedules_df['skipped_port_flag'] = 0\n",
    "\n",
    "    # Sort by 'vesselId' and 'arrivalDate'\n",
    "    schedules_df = schedules_df.sort_values(by=['vesselId', 'arrivalDate']).reset_index(drop=True)\n",
    "\n",
    "    # Keep track of rows to drop\n",
    "    rows_to_drop = set()\n",
    "\n",
    "    # Process each vessel separately\n",
    "    for vessel_id in schedules_df['vesselId'].unique():\n",
    "        vessel_indices = schedules_df[schedules_df['vesselId'] == vessel_id].index.tolist()\n",
    "        i = 0\n",
    "        while i < len(vessel_indices) - 1:\n",
    "            current_idx = vessel_indices[i]\n",
    "            next_idx = vessel_indices[i + 1]\n",
    "            current_row = schedules_df.loc[current_idx]\n",
    "            next_row = schedules_df.loc[next_idx]\n",
    "\n",
    "            # Handle same ports\n",
    "            if current_row['portId'] == next_row['portId']:\n",
    "                # Update sailingDate and set flag\n",
    "                schedules_df.at[current_idx, 'sailingDate'] = next_row['sailingDate']\n",
    "                schedules_df.at[current_idx, 'small_movement_flag'] = 1\n",
    "                rows_to_drop.add(next_idx)  # Mark next row for removal\n",
    "                vessel_indices.pop(i + 1)   # Remove next index from list\n",
    "                # Do not increment i to check for further consecutive same ports\n",
    "                continue\n",
    "\n",
    "            # Handle different ports with invalid or zero time difference arrivalDate\n",
    "            elif next_row['arrivalDate'] <= current_row['sailingDate']:\n",
    "                # Mark next row for removal\n",
    "                rows_to_drop.add(next_idx)\n",
    "                # Set skipped port flag\n",
    "                schedules_df.at[current_idx, 'skipped_port_flag'] = 1\n",
    "                vessel_indices.pop(i + 1)   # Remove next index from list\n",
    "                # Do not increment i to check the next row against the current one\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                # Valid movement, proceed to next\n",
    "                i += 1\n",
    "\n",
    "    # Remove rows marked for dropping\n",
    "    schedules_df = schedules_df.drop(index=list(rows_to_drop)).reset_index(drop=True)\n",
    "\n",
    "    return schedules_df\n",
    "\n",
    "\n",
    "# Re-run the anomaly detection\n",
    "# Preprocess the schedules\n",
    "schedules_df_final = preprocess_schedules_final(schedules_df)\n",
    "anomalies_df_final = check_sailing_vs_arrival_anomalies(schedules_df_final)\n",
    "\n",
    "# Print anomalies if any exist\n",
    "if not anomalies_df_final.empty:\n",
    "    print(\"Anomalies found where sailingDate is greater than the next row's arrivalDate:\")\n",
    "    print(anomalies_df_final)\n",
    "else:\n",
    "    print(\"No anomalies found.\")\n",
    "    \n",
    "anomalies_df_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d942f-0fbd-4065-ac2f-5fa4bd625841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ais_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682f98cf-ecdc-4545-b86f-f243eedafc39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vessels_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607fec47-901b-4bc1-a825-2b1021ab94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ports_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c04a0b-caea-4dc4-a3f9-678c4ad66c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schedules_df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db363d8-6b8d-49fe-9ae0-88b3ff08ff2b",
   "metadata": {},
   "source": [
    "### Merge code to datasets in batches due to memory constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888b824-d67d-477d-9766-f516bfe8d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_in_batches(\n",
    "    ais_train_df,\n",
    "    vessels_df,\n",
    "    schedules_df,\n",
    "    ports_df,\n",
    "    batch_size=200,\n",
    "    gcs_bucket_path=\"gs://jacobsbucketformlproject/ML Competition/\"\n",
    "):\n",
    "    # Get unique vessel IDs\n",
    "    vessel_ids = ais_train_df['vesselId'].unique()\n",
    "\n",
    "    # Split vessel IDs into batches\n",
    "    vessel_id_batches = [vessel_ids[i:i + batch_size] for i in range(0, len(vessel_ids), batch_size)]\n",
    "\n",
    "    # Process each batch\n",
    "    for batch_num, vessel_id_batch in enumerate(vessel_id_batches, start=1):\n",
    "        print(f'Processing batch {batch_num}/{len(vessel_id_batches)}')\n",
    "\n",
    "        # Filter AIS data for the current batch\n",
    "        ais_batch_df = ais_train_df[ais_train_df['vesselId'].isin(vessel_id_batch)]\n",
    "\n",
    "        # Filter schedules data for the current batch\n",
    "        schedules_batch_df = schedules_df[schedules_df['vesselId'].isin(vessel_id_batch)]\n",
    "\n",
    "        # Merge schedules data with ports data (left join)\n",
    "        schedules_batch_df = schedules_batch_df.merge(\n",
    "            ports_df,\n",
    "            on='portId', how='left'\n",
    "        )\n",
    "\n",
    "        # Create a list to collect merged dataframes per vessel\n",
    "        merged_dfs = []\n",
    "\n",
    "        # Process each vessel individually to manage memory usage and ensure time order\n",
    "        for vessel_id in vessel_id_batch:\n",
    "            # Filter data for this vessel\n",
    "            ais_vessel_df = ais_batch_df[ais_batch_df['vesselId'] == vessel_id].sort_values('time').reset_index(drop=True)\n",
    "            schedules_vessel_df = schedules_batch_df[schedules_batch_df['vesselId'] == vessel_id].sort_values('arrivalDate').reset_index(drop=True)\n",
    "\n",
    "            # Rename AIS portId to ais_portId\n",
    "            if 'portId' in ais_vessel_df.columns:\n",
    "                ais_vessel_df.rename(columns={'portId': 'ais_portId'}, inplace=True)\n",
    "\n",
    "            # Initialize schedule-related columns with NaN\n",
    "            schedule_cols = [\n",
    "                'schedule_arrivalDate', 'schedule_sailingDate', 'schedule_moored_portId',\n",
    "                'schedule_moored_portLatitude', 'schedule_moored_portLongitude',\n",
    "                'schedule_voyage_end',\n",
    "                'schedule_destination_portId',\n",
    "                'schedule_destination_portLatitude',\n",
    "                'schedule_destination_portLongtitude',\n",
    "                'schedule_small_movement_flag', 'schedule_skipped_port_flag'\n",
    "            ]\n",
    "\n",
    "            if schedules_vessel_df.empty:\n",
    "                # Assign NaN for schedule-related columns\n",
    "                for col in schedule_cols:\n",
    "                    ais_vessel_df[col] = pd.NA\n",
    "                merged_vessel_df = ais_vessel_df\n",
    "            else:\n",
    "                # Create events (port stays and voyages)\n",
    "                events = []\n",
    "\n",
    "                # Add events\n",
    "                for idx in range(len(schedules_vessel_df) - 1):\n",
    "                    arrivalDate = schedules_vessel_df.loc[idx, 'arrivalDate']\n",
    "                    voyage_start = schedules_vessel_df.loc[idx, 'sailingDate']\n",
    "                    voyage_end = schedules_vessel_df.loc[idx + 1, 'arrivalDate']\n",
    "                    voyage_start_portId = schedules_vessel_df.loc[idx, 'portId']\n",
    "                    mooredPortLongitude = schedules_vessel_df.loc[idx, 'portLongitude']\n",
    "                    mooredPortLatitude = schedules_vessel_df.loc[idx, 'portLatitude']\n",
    "                    voyage_end_portId = schedules_vessel_df.loc[idx + 1, 'portId']\n",
    "                    destinationLatitude = schedules_vessel_df.loc[idx + 1, 'portLatitude'] \n",
    "                    destinationLongitude = schedules_vessel_df.loc[idx + 1, 'portLongitude']\n",
    "                    small_movement_flag = schedules_vessel_df.loc[idx, 'small_movement_flag']\n",
    "                    skipped_port_flag = schedules_vessel_df.loc[idx,'skipped_port_flag']\n",
    "                    event = {\n",
    "                        'start_time': arrivalDate,\n",
    "                        'end_time': voyage_end,\n",
    "                        'schedule_arrivalDate': arrivalDate,\n",
    "                        'schedule_sailingDate': voyage_start,\n",
    "                        'schedule_moored_portId': voyage_start_portId,\n",
    "                        'schedule_moored_portLatitude': mooredPortLatitude,\n",
    "                        'schedule_moored_portLongitude': mooredPortLongitude,\n",
    "                        'schedule_voyage_end': voyage_end,\n",
    "                        'schedule_destination_portId': voyage_end_portId,\n",
    "                        'schedule_destination_portLatitude': destinationLatitude,\n",
    "                        'schedule_destination_portLongtitude': destinationLongitude,\n",
    "                        'schedule_small_movement_flag': small_movement_flag,\n",
    "                        'schedule_skipped_port_flag': skipped_port_flag\n",
    "                    }\n",
    "                    events.append(event)\n",
    "\n",
    "                # Handle the last voyage after the last port stay\n",
    "                last_idx = len(schedules_vessel_df)-1\n",
    "                arrivalDate = schedules_vessel_df.loc[last_idx, 'arrivalDate']\n",
    "                voyage_start = schedules_vessel_df.loc[last_idx, 'sailingDate']\n",
    "                voyage_start_portId = schedules_vessel_df.loc[last_idx, 'portId']\n",
    "                mooredPortLongitude = schedules_vessel_df.loc[last_idx, 'portLongitude']\n",
    "                mooredPortLatitude = schedules_vessel_df.loc[last_idx, 'portLatitude']\n",
    "                small_movement_flag = schedules_vessel_df.loc[last_idx, 'small_movement_flag']\n",
    "                skipped_port_flag = schedules_vessel_df.loc[last_idx,'skipped_port_flag']\n",
    "                voyage_end = pd.NaT # No known destination time\n",
    "                voyage_end_portId = pd.NA  # No known destination port\n",
    "                destinationLatitude = pd.NA\n",
    "                destinationLongitude = pd.NA\n",
    "                event = {\n",
    "                        'start_time': arrivalDate,\n",
    "                        'end_time': voyage_start,\n",
    "                        'schedule_arrivalDate': arrivalDate,\n",
    "                        'schedule_sailingDate': voyage_start,\n",
    "                        'schedule_moored_portId': voyage_start_portId,\n",
    "                        'schedule_moored_portLatitude': mooredPortLatitude,\n",
    "                        'schedule_moored_portLongitude': mooredPortLongitude,\n",
    "                        'schedule_voyage_end': voyage_end,\n",
    "                        'schedule_destination_portId': voyage_end_portId,\n",
    "                        'schedule_destination_portLatitude': destinationLatitude,\n",
    "                        'schedule_destination_portLongtitude': destinationLongitude,\n",
    "                        'schedule_small_movement_flag': small_movement_flag,\n",
    "                        'schedule_skipped_port_flag': skipped_port_flag\n",
    "                }\n",
    "                events.append(event)\n",
    "\n",
    "                # Convert events to DataFrame\n",
    "                events_df = pd.DataFrame(events)\n",
    "\n",
    "                # Now process each event\n",
    "                data_points = []\n",
    "\n",
    "                for idx, event in events_df.iterrows():\n",
    "                    start_time = event['start_time']\n",
    "                    end_time = event['end_time']\n",
    "\n",
    "                    # Get AIS data points within this event period\n",
    "                    ais_event_df = ais_vessel_df[(ais_vessel_df['time'] >= start_time) & (ais_vessel_df['time'] <= end_time)].copy()\n",
    "\n",
    "                    if not ais_event_df.empty:\n",
    "                        # Assign event data to AIS data points\n",
    "                        for col in schedule_cols:\n",
    "                            ais_event_df[col] = event.get(col, pd.NA)\n",
    "                        data_points.append(ais_event_df)\n",
    "                    else:\n",
    "                        # Create synthetic data point with NaN AIS data and event data\n",
    "                        synthetic_point = {col: pd.NA for col in ais_vessel_df.columns if col != 'vesselId'}\n",
    "                        synthetic_point['vesselId'] = vessel_id  # Ensure vesselId is set\n",
    "                        synthetic_point['time'] = pd.NaT  # Set time to NaT\n",
    "\n",
    "                        # Assign event data to synthetic point\n",
    "                        for col in schedule_cols:\n",
    "                            synthetic_point[col] = event.get(col, pd.NA)\n",
    "\n",
    "                        synthetic_df = pd.DataFrame([synthetic_point])\n",
    "                        data_points.append(synthetic_df)\n",
    "\n",
    "                # Include any AIS data points not covered by events\n",
    "                # Collect all event intervals\n",
    "                event_intervals = [(event['start_time'], event['end_time']) for _, event in events_df.iterrows()]\n",
    "                # Function to check if a time is within any event interval\n",
    "                def is_in_event(time):\n",
    "                    return any((time >= start) and (time <= end) for (start, end) in event_intervals)\n",
    "\n",
    "                # Identify AIS data points not in any event\n",
    "                ais_outside_events_df = ais_vessel_df[~ais_vessel_df['time'].apply(is_in_event)].copy()\n",
    "                if not ais_outside_events_df.empty:\n",
    "                    # Assign NaNs to schedule columns\n",
    "                    for col in schedule_cols:\n",
    "                        ais_outside_events_df[col] = pd.NA\n",
    "                    data_points.append(ais_outside_events_df)\n",
    "\n",
    "                # Collect all data points\n",
    "                merged_vessel_df = pd.concat(data_points, ignore_index=True)\n",
    "\n",
    "                # Create a sort key for merged_vessel_df\n",
    "                # For rows with valid 'time', use 'time' as sort key\n",
    "                # For synthetic data points with 'time' as NaT, use 'arrivalDate' or 'voyage_start'\n",
    "                merged_vessel_df['sort_time'] = merged_vessel_df.apply(\n",
    "                    lambda row: row['time'] if pd.notnull(row['time']) else (\n",
    "                        row['schedule_arrivalDate'] if pd.notnull(row['schedule_arrivalDate']) else pd.Timestamp.max\n",
    "                    ),\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                # Now sort by 'sort_time'\n",
    "                merged_vessel_df.sort_values('sort_time', inplace=True)\n",
    "                merged_vessel_df.drop(columns=['sort_time'], inplace=True)\n",
    "                merged_vessel_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Merge with vessel specifications to ensure vessel data is present\n",
    "            merged_vessel_df = merged_vessel_df.merge(vessels_df, on='vesselId', how='left')\n",
    "\n",
    "            # Append to the list of merged dataframes\n",
    "            merged_dfs.append(merged_vessel_df)\n",
    "\n",
    "        # Concatenate merged dataframes for the batch\n",
    "        if merged_dfs:\n",
    "            merged_batch_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "\n",
    "            # Save the merged batch to Google Cloud Storage\n",
    "            output_path = f\"{gcs_bucket_path}merged_data_batch_{batch_num}.csv\"\n",
    "            merged_batch_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        else:\n",
    "            print(f'No data to merge for batch {batch_num}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b9d52-e185-4251-8b8d-29a49c39a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data_in_batches(\n",
    "    ais_train_df=ais_train_df,\n",
    "    vessels_df=vessels_df,\n",
    "    schedules_df=schedules_df_final,\n",
    "    ports_df=ports_df,\n",
    "    batch_size=200,\n",
    "    gcs_bucket_path=\"gs://jacobsbucketformlproject/ML Competition/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e32e0f0-1bea-41d4-b364-a11cc186f74d",
   "metadata": {},
   "source": [
    "First we must create a function that gives voyage info so our merging with schedules becomes more useful!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8c709-029f-4f4a-964c-b5c5121a18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clear_dataframes(*dfs):\n",
    "    \"\"\"\n",
    "    Remove specified dataframes from memory and trigger garbage collection.\n",
    "\n",
    "    Parameters:\n",
    "    - dfs: A list of dataframe variables to be removed.\n",
    "    \"\"\"\n",
    "    for df in dfs:\n",
    "        del df\n",
    "    # Force garbage collection to free up memory\n",
    "    gc.collect()\n",
    "\n",
    "# Example usage:\n",
    "# Assume ais_train_df, vessels_df, schedules_df, and ports_df were used for merging\n",
    "clear_dataframes(ais_train_df, vessels_df, schedules_df, ports_df)\n",
    "\n",
    "# After calling this, the dataframes will be removed from memory, and memory will be freed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316ba0a-c48e-41eb-a8b0-21f6ba8399fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the first batch in your GCS bucket\n",
    "first_batch_path = 'gs://jacobsbucketformlproject/ML Competition/merged_data_batch_1.csv'\n",
    "\n",
    "# Load the first batch into a pandas DataFrame\n",
    "first_batch_df = pd.read_csv(first_batch_path, delimiter=',', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the DataFrame to inspect\n",
    "first_batch_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4692d-09f0-4b17-a448-fb655a9897eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing values in the merged dataframe\n",
    "\n",
    "missing_values = first_batch_df.isna().sum()\n",
    "\n",
    "# Display columns with missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values[missing_values > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8a470-0b1d-44c7-9ce1-00cdffacaef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_batch_df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384438a-616b-4ff3-b490-da11ea8eba41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_batch_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07900ae-5320-43d6-8e6d-3fe57c982be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_batch_path = 'gs://jacobsbucketformlproject/ML Competition/merged_data_batch_2.csv'\n",
    "# Load the first batch into a pandas DataFrame\n",
    "second_batch_df = pd.read_csv(second_batch_path, delimiter=',', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the DataFrame to inspect\n",
    "second_batch_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d7f47-ccca-4d52-ab7e-099e3c662a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing values in the merged dataframe\n",
    "\n",
    "missing_values1 = second_batch_df.isna().sum()\n",
    "\n",
    "# Display columns with missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values1[missing_values1 > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f825efe-6c67-41a6-9f45-bbf4dbcff768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_batch_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83595e4b-fb05-45aa-bd4a-91223ed8520c",
   "metadata": {},
   "source": [
    "# Summary!!!\n",
    "\n",
    "Merge looks good! Now we must use the batches to resample time points and feature engineer, before model development. Will use different ways of resampling and feature engineering based on model. So this is the base dataset I now will work on."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
