{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# shortNotebook for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from datetime import timedelta  # Added import for timedelta\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522065 entries, 0 to 1522064\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count    Dtype  \n",
      "---  ------     --------------    -----  \n",
      " 0   time       1522065 non-null  object \n",
      " 1   cog        1522065 non-null  float64\n",
      " 2   sog        1522065 non-null  float64\n",
      " 3   rot        1522065 non-null  int64  \n",
      " 4   heading    1522065 non-null  int64  \n",
      " 5   navstat    1522065 non-null  int64  \n",
      " 6   etaRaw     1522065 non-null  object \n",
      " 7   latitude   1522065 non-null  float64\n",
      " 8   longitude  1522065 non-null  float64\n",
      " 9   vesselId   1522065 non-null  object \n",
      " 10  portId     1520450 non-null  object \n",
      "dtypes: float64(4), int64(3), object(4)\n",
      "memory usage: 127.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your file in the bucket\n",
    "file_path = '../../original_data/ais_train.csv'\n",
    "\n",
    "# Load the file into a pandas dataframe\n",
    "ais_train_df = pd.read_csv(file_path, delimiter= '|', encoding= 'utf-8')\n",
    "\n",
    "# Display the dataframe\n",
    "ais_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51739 entries, 0 to 51738\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   ID              51739 non-null  int64  \n",
      " 1   vesselId        51739 non-null  object \n",
      " 2   time            51739 non-null  object \n",
      " 3   scaling_factor  51739 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "ais_test_df = pd.read_csv('../../original_data/ais_test.csv')\n",
    "ais_test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522065 entries, 0 to 1522064\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count    Dtype         \n",
      "---  ------     --------------    -----         \n",
      " 0   time       1522065 non-null  datetime64[ns]\n",
      " 1   cog        1516207 non-null  float64       \n",
      " 2   sog        1522065 non-null  float64       \n",
      " 3   heading    1517169 non-null  float64       \n",
      " 4   navstat    1522065 non-null  int64         \n",
      " 5   latitude   1522065 non-null  float64       \n",
      " 6   longitude  1522065 non-null  float64       \n",
      " 7   vesselId   1522065 non-null  object        \n",
      " 8   portId     1520450 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(5), int64(1), object(2)\n",
      "memory usage: 104.5+ MB\n"
     ]
    }
   ],
   "source": [
    "def preprocess_ais_train(ais_train_df):\n",
    "    \"\"\"\n",
    "    Preprocess the ais_train_df by converting columns, handling missing or invalid values, \n",
    "    merging port information, and mapping NAVSTAT codes to descriptions.\n",
    "\n",
    "    Parameters:\n",
    "    - ais_train_df: DataFrame containing the raw AIS train data.\n",
    "    - ports_df: DataFrame containing port information with portId, latitude, and longitude.\n",
    "\n",
    "    Returns:\n",
    "    - ais_train_df_cleaned: A cleaned and preprocessed version of ais_train_df.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert 'time' to datetime and drop 'etaRaw'\n",
    "    ais_train_df['time'] = pd.to_datetime(ais_train_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    ais_train_df.drop('etaRaw', axis=1, inplace=True)\n",
    "\n",
    "    # Step 4: Convert relevant columns to float\n",
    "    ais_train_df['cog'] = ais_train_df['cog'].astype(float)\n",
    "    ais_train_df['sog'] = ais_train_df['sog'].astype(float)\n",
    "    ais_train_df['rot'] = ais_train_df['rot'].astype(float)\n",
    "    ais_train_df['heading'] = ais_train_df['heading'].astype(float)\n",
    "    ais_train_df['latitude'] = ais_train_df['latitude'].astype(float)\n",
    "    ais_train_df['longitude'] = ais_train_df['longitude'].astype(float)\n",
    "    \n",
    "    # Step 5: Replace invalid or default values with NaN\n",
    "    ais_train_df['cog'] = np.where((ais_train_df['cog'] == 360) | (ais_train_df['cog'] > 360) | (ais_train_df['cog'] < 0), np.nan, ais_train_df['cog'])\n",
    "    ais_train_df['sog'] = np.where((ais_train_df['sog'] == 1023) | (ais_train_df['sog'] < 0), np.nan, ais_train_df['sog'])\n",
    "    ais_train_df['heading'] = np.where((ais_train_df['heading'] > 360) | (ais_train_df['heading'] == 511) | (ais_train_df['heading'] < 0), np.nan, ais_train_df['heading'])\n",
    "    \n",
    "    # Step 6: Drop 'rot' as it is difficult to find outliers and thus is bring more noise than information\n",
    "    ais_train_df.drop(columns=['rot'], inplace=True)\n",
    "\n",
    "    # Step 7: Sort by vesselId and time\n",
    "    ais_train_df = ais_train_df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "    return ais_train_df\n",
    "\n",
    "baseDataset = preprocess_ais_train(ais_train_df)\n",
    "\n",
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier detection and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    vesselId  record_count  in_test_set\n",
      "0   61e9f3cbb937134a3c4bff09             1        False\n",
      "1   61e9f3adb937134a3c4bfe37            31        False\n",
      "2   61e9f3c6b937134a3c4bfed5           160        False\n",
      "3   61e9f42cb937134a3c4c00f9           191        False\n",
      "4   61e9f45cb937134a3c4c022b           196        False\n",
      "5   61e9f39ab937134a3c4bfdb9           197        False\n",
      "6   61e9f45eb937134a3c4c0235           250        False\n",
      "7   61e9f3bcb937134a3c4bfe91           328         True\n",
      "8   61e9f418b937134a3c4c0077           332        False\n",
      "9   61e9f408b937134a3c4c0023           355        False\n",
      "10  61e9f460b937134a3c4c0243           361        False\n",
      "11  61e9f3f7b937134a3c4bffc5           373        False\n",
      "12  61e9f409b937134a3c4c0027           391        False\n",
      "13  620bf33a718775aca4a81900           401        False\n",
      "14  61e9f423b937134a3c4c00c7           402        False\n",
      "15  61e9f38eb937134a3c4bfd8b           402        False\n",
      "16  61e9f456b937134a3c4c0203           408        False\n",
      "17  61e9f3afb937134a3c4bfe47           416        False\n",
      "18  61e9f3bab937134a3c4bfe8b           451        False\n",
      "19  6326eed6c46d6a20d22ca319           451         True\n"
     ]
    }
   ],
   "source": [
    "# Get the unique vesselIds from the test set\n",
    "vessel_ids_test = set(ais_test_df['vesselId'].unique())\n",
    "\n",
    "# Get the count of records per vesselId in the training set\n",
    "vessel_record_counts = ais_train_df['vesselId'].value_counts()\n",
    "\n",
    "# Get the 10 vessels with the lowest number of records\n",
    "lowest_record_vessels = vessel_record_counts.nsmallest(20)\n",
    "\n",
    "# Check if these vessels are in the test set\n",
    "vessels_in_test = lowest_record_vessels.index.isin(vessel_ids_test)\n",
    "\n",
    "# Combine the results into a dataframe for easy viewing\n",
    "vessels_with_low_records = pd.DataFrame({\n",
    "    'vesselId': lowest_record_vessels.index,\n",
    "    'record_count': lowest_record_vessels.values,\n",
    "    'in_test_set': vessels_in_test\n",
    "})\n",
    "\n",
    "# Display the result\n",
    "print(vessels_with_low_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of vessel IDs to remove\n",
    "vessels_to_remove = ['61e9f3cbb937134a3c4bff09', '61e9f3adb937134a3c4bfe37']\n",
    "\n",
    "# Remove vessels from the dataset\n",
    "baseDataset = baseDataset[~ais_train_df['vesselId'].isin(vessels_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_intervals(data, min_time_diff_minutes=2):\n",
    "    \"\"\"\n",
    "    Filters out records with time differences less than the specified threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing 'vesselId' and 'time' columns.\n",
    "    - min_time_diff_minutes (int): Minimum time difference in minutes to keep records.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame with records having time differences >= min_time_diff_minutes.\n",
    "    \"\"\"\n",
    "    # Ensure time column is in datetime format\n",
    "    data['time'] = pd.to_datetime(data['time'])\n",
    "    \n",
    "    # Sort by vesselId and time to ensure proper order\n",
    "    data = data.sort_values(['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "    # Calculate time differences in minutes\n",
    "    data['time_diff'] = data.groupby('vesselId')['time'].diff().dt.total_seconds() / 60  # in minutes\n",
    "\n",
    "    # Filter out records with time differences less than the specified threshold\n",
    "    filtered_data = data[(data['time_diff'].isna()) | (data['time_diff'] >= min_time_diff_minutes)].copy()\n",
    "\n",
    "    # Drop the time_diff column after filtering\n",
    "    filtered_data = filtered_data.drop(columns=['time_diff']).reset_index(drop=True)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "baseDataset = filter_short_intervals(baseDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Haversine formula to calculate the great-circle distance between two points\n",
    "    R = 3440.065  # Radius of Earth in nautical miles\n",
    "    lat1_rad = np.radians(lat1.astype(float))\n",
    "    lon1_rad = np.radians(lon1.astype(float))\n",
    "    lat2_rad = np.radians(lat2.astype(float))\n",
    "    lon2_rad = np.radians(lon2.astype(float))\n",
    "    \n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "def heuristic_clean(data, vthreshold=30):\n",
    "    \"\"\"\n",
    "    Cleans the trajectory data by removing points with unrealistic speeds.\n",
    "    \n",
    "    :param data: DataFrame containing raw trajectory data\n",
    "    :param vthreshold: Speed threshold in knots\n",
    "    :return: Cleaned trajectory DataFrame\n",
    "    \"\"\"\n",
    "    # Drop duplicates and reset index\n",
    "    data = data.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Sort by vessel ID and time to ensure correct order\n",
    "    data = data.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "    \n",
    "    # Shift the columns to align with the previous record within each vessel\n",
    "    data['prev_latitude'] = data.groupby('vesselId')['latitude'].shift(1)\n",
    "    data['prev_longitude'] = data.groupby('vesselId')['longitude'].shift(1)\n",
    "    data['prev_time'] = data.groupby('vesselId')['time'].shift(1)\n",
    "    \n",
    "    # Calculate time differences in hours\n",
    "    data['delta_time'] = (data['time'] - data['prev_time']).dt.total_seconds() / 3600.0  # in hours\n",
    "    \n",
    "    # Calculate distances using the haversine function where we have valid previous points\n",
    "    valid_rows = data['prev_latitude'].notna() & data['prev_longitude'].notna()\n",
    "    data.loc[valid_rows, 'delta_distance'] = haversine(\n",
    "        data.loc[valid_rows, 'prev_latitude'].to_numpy(),\n",
    "        data.loc[valid_rows, 'prev_longitude'].to_numpy(),\n",
    "        data.loc[valid_rows, 'latitude'].to_numpy(),\n",
    "        data.loc[valid_rows, 'longitude'].to_numpy()\n",
    "    )\n",
    "    \n",
    "    # Calculate speed (distance/time) and filter by threshold\n",
    "    data['speed'] = data['delta_distance'] / data['delta_time']\n",
    "    \n",
    "    # Filter out rows where speed exceeds the threshold\n",
    "    cleaned_data = data[(data['speed'] <= vthreshold) | data['speed'].isna()].copy()\n",
    "    \n",
    "    # Drop intermediate calculation columns\n",
    "    cleaned_data = cleaned_data.drop(columns=['prev_latitude', 'prev_longitude', 'prev_time', \n",
    "                                              'delta_time', 'delta_distance', 'speed'])\n",
    "    \n",
    "    return cleaned_data.reset_index(drop=True)\n",
    "\n",
    "baseDataset = heuristic_clean(baseDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Movement Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_historical_movement_features(df):\n",
    "    \"\"\"\n",
    "    Calculates historical movement features for vessel data:\n",
    "    delta_latitude, delta_longitude, time_diff, movement_intensity, and directional_stability.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing sorted vessel data with columns:\n",
    "                       'vesselId', 'time', 'latitude', and 'longitude'.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with new features added.\n",
    "    \"\"\"\n",
    "    # Calculate deltas in latitude, longitude, and time\n",
    "    df['delta_latitude'] = df.groupby('vesselId')['latitude'].diff()\n",
    "    df['delta_longitude'] = df.groupby('vesselId')['longitude'].diff()\n",
    "    df['time_diff'] = df.groupby('vesselId')['time'].diff().dt.total_seconds() / 3600  # Time difference in hours\n",
    "\n",
    "    # Calculate speed as distance over time\n",
    "    df['speed'] = np.sqrt(df['delta_latitude']**2 + df['delta_longitude']**2) / df['time_diff']\n",
    "\n",
    "    # Calculate movement intensity as a 5-step exponential moving average of speed\n",
    "    df['movement_intensity'] = df.groupby('vesselId')['speed'].transform(lambda x: x.ewm(span=5, adjust=False).mean())\n",
    "\n",
    "    # Calculate bearing as the direction of movement\n",
    "    df['bearing'] = np.arctan2(df['delta_longitude'], df['delta_latitude'])\n",
    "\n",
    "    # Calculate directional stability as the 3-step rolling standard deviation of bearing\n",
    "    df['directional_stability'] = df.groupby('vesselId')['bearing'].transform(lambda x: x.rolling(window=3).std())\n",
    "\n",
    "    # Drop intermediate columns if not needed\n",
    "    df.drop(columns=['speed', 'bearing'], inplace=True)\n",
    "\n",
    "    # Replace NaN values that result from the first differences with 0 (or other logic if preferred)\n",
    "    df[['delta_latitude', 'delta_longitude', 'time_diff', 'movement_intensity', 'directional_stability']] = \\\n",
    "        df[['delta_latitude', 'delta_longitude', 'time_diff', 'movement_intensity', 'directional_stability']].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "baseDataset = create_historical_movement_features(baseDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1518801 entries, 0 to 1518800\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count    Dtype         \n",
      "---  ------                 --------------    -----         \n",
      " 0   time                   1518801 non-null  datetime64[ns]\n",
      " 1   cog                    1512956 non-null  float64       \n",
      " 2   sog                    1518801 non-null  float64       \n",
      " 3   heading                1513911 non-null  float64       \n",
      " 4   navstat                1518801 non-null  int64         \n",
      " 5   latitude               1518801 non-null  float64       \n",
      " 6   longitude              1518801 non-null  float64       \n",
      " 7   vesselId               1518801 non-null  object        \n",
      " 8   portId                 1517195 non-null  object        \n",
      " 9   delta_latitude         1518801 non-null  float64       \n",
      " 10  delta_longitude        1518801 non-null  float64       \n",
      " 11  time_diff              1518801 non-null  float64       \n",
      " 12  movement_intensity     1518801 non-null  float64       \n",
      " 13  directional_stability  1518801 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(10), int64(1), object(2)\n",
      "memory usage: 162.2+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now remove features that have show little prediction power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unnecessary_columns(df):\n",
    "    \"\"\"\n",
    "    Drops the columns 'sog', 'cog', 'heading', 'navstat', and 'portId' from the DataFrame, if they exist.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame from which to drop the columns.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with specified columns removed.\n",
    "    \"\"\"\n",
    "    columns_to_drop = ['sog', 'cog', 'heading', 'navstat', 'portId']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "    return df\n",
    "\n",
    "baseDataset = drop_unnecessary_columns(baseDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1518801 entries, 0 to 1518800\n",
      "Data columns (total 9 columns):\n",
      " #   Column                 Non-Null Count    Dtype         \n",
      "---  ------                 --------------    -----         \n",
      " 0   time                   1518801 non-null  datetime64[ns]\n",
      " 1   latitude               1518801 non-null  float64       \n",
      " 2   longitude              1518801 non-null  float64       \n",
      " 3   vesselId               1518801 non-null  object        \n",
      " 4   delta_latitude         1518801 non-null  float64       \n",
      " 5   delta_longitude        1518801 non-null  float64       \n",
      " 6   time_diff              1518801 non-null  float64       \n",
      " 7   movement_intensity     1518801 non-null  float64       \n",
      " 8   directional_stability  1518801 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(7), object(1)\n",
      "memory usage: 104.3+ MB\n"
     ]
    }
   ],
   "source": [
    "baseDataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now create the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We crete the final features needed for our basedataset\n",
    "def features_gen(df):\n",
    "    df['y_lat'] = df['latitude']\n",
    "    df['y_lon'] = df['longitude']\n",
    "    return df\n",
    "\n",
    "baseDataset = features_gen(baseDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vessel_data(vessel_data, batch_size_days, batch_segment):\n",
    "    # Sort the vessel data by time\n",
    "    vessel_data = vessel_data.sort_values('time').reset_index(drop=True)\n",
    "    \n",
    "    # Check for empty vessel data\n",
    "    if vessel_data.empty:\n",
    "        print(\"Vessel data is empty; skipping processing.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calculate batch interval to control overlap\n",
    "    batch_interval = batch_size_days / batch_segment\n",
    "    t_min = vessel_data['time'].min()\n",
    "    t_max = vessel_data['time'].max()\n",
    "\n",
    "    # Check if t_min and t_max are valid\n",
    "    if t_min is pd.NaT or t_max is pd.NaT:\n",
    "        print(f\"Invalid timestamps for vesselId {vessel_data['vesselId'].iloc[0]}; skipping vessel.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Generate batch start times\n",
    "    batch_starts = pd.date_range(start=t_min, end=t_max - timedelta(days=batch_size_days), freq=f'{batch_interval}D')\n",
    "\n",
    "    # Check if batch_starts is empty\n",
    "    if batch_starts.empty:\n",
    "        return pd.DataFrame()  # Skip this vessel\n",
    "\n",
    "    # Prepare a list to hold batch data\n",
    "    batches = []\n",
    "    \n",
    "    # Define features to overwrite outside the loop\n",
    "    features_to_overwrite = vessel_data.columns.difference(['time', 'y_lat', 'y_lon', 'train'])\n",
    "\n",
    "    for batch_start in batch_starts:\n",
    "        batch_end = batch_start + timedelta(days=batch_size_days)\n",
    "        batch_data = vessel_data[(vessel_data['time'] >= batch_start) & (vessel_data['time'] <= batch_end)].copy()\n",
    "\n",
    "        # Skip if this batch window contains insufficient data points\n",
    "        if batch_data.empty or len(batch_data) < 2:  # Require at least two points to define a movement\n",
    "            continue\n",
    "        \n",
    "        # Process the batch as usual\n",
    "        base_row = batch_data.iloc[0]\n",
    "        batch_data['train'] = 0\n",
    "        batch_data.at[batch_data.index[0], 'train'] = 1\n",
    "\n",
    "        # Overwrite features using base row values for all rows\n",
    "        base_values = base_row[features_to_overwrite].to_dict()\n",
    "        batch_data.loc[:, features_to_overwrite] = batch_data.apply(lambda row: pd.Series(base_values), axis=1)\n",
    "\n",
    "        # Calculate 'time_elapsed' in hours\n",
    "        batch_data['time_elapsed'] = (batch_data['time'] - base_row['time']).dt.total_seconds() / 3600\n",
    "\n",
    "        # Collect batch data\n",
    "        batches.append(batch_data)\n",
    "\n",
    "    # Handle the remaining data dynamically, even if it's shorter than batch_size_days\n",
    "    remaining_data = vessel_data[vessel_data['time'] > batch_starts[-1]].copy()\n",
    "    if not remaining_data.empty and len(remaining_data) >= 2:  # Require at least two points\n",
    "        # Use the first row as the base point\n",
    "        base_row = remaining_data.iloc[0]\n",
    "        remaining_data['train'] = 0\n",
    "        remaining_data.at[remaining_data.index[0], 'train'] = 1\n",
    "\n",
    "        # Overwrite features using base row values for all rows\n",
    "        base_values = base_row[features_to_overwrite].to_dict()\n",
    "        remaining_data.loc[:, features_to_overwrite] = remaining_data.apply(lambda row: pd.Series(base_values), axis=1)\n",
    "\n",
    "        # Calculate 'time_elapsed' in hours for remaining data\n",
    "        remaining_data['time_elapsed'] = (remaining_data['time'] - base_row['time']).dt.total_seconds() / 3600\n",
    "\n",
    "        # Add the final batch with remaining data\n",
    "        batches.append(remaining_data)\n",
    "\n",
    "    # Concatenate all batches for this vessel\n",
    "    return pd.concat(batches, ignore_index=True) if batches else pd.DataFrame()\n",
    "\n",
    "\n",
    "def create_batches(base_dataset, validation_split_ratio=0.1, batch_size_days=5, batch_segment=1, n_jobs=8):\n",
    "    # Step 1: Sort by time for initial splitting\n",
    "    base_dataset = base_dataset.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "    # Step 2: Determine the split index based on time for the entire dataset\n",
    "    split_index = int(len(base_dataset) * (1 - validation_split_ratio))\n",
    "    train_dataset = base_dataset.iloc[:split_index]\n",
    "    val_dataset = base_dataset.iloc[split_index:]\n",
    "\n",
    "    # Step 3: Re-sort by vesselId and time within each subset\n",
    "    train_dataset = train_dataset.sort_values(['vesselId', 'time']).reset_index(drop=True)\n",
    "    val_dataset = val_dataset.sort_values(['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "    # Define a wrapper to apply process_vessel_data\n",
    "    def process_group(vessel_data):\n",
    "        return process_vessel_data(vessel_data, batch_size_days, batch_segment)\n",
    "\n",
    "    # Step 4: Process each subset in parallel using joblib\n",
    "    train_batches = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_group)(vessel_data) for _, vessel_data in train_dataset.groupby('vesselId')\n",
    "    )\n",
    "    val_batches = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_group)(vessel_data) for _, vessel_data in val_dataset.groupby('vesselId')\n",
    "    )\n",
    "\n",
    "    # Step 5: Concatenate results\n",
    "    train_batches = pd.concat(train_batches, ignore_index=True) if train_batches else pd.DataFrame()\n",
    "    val_batches = pd.concat(val_batches, ignore_index=True) if val_batches else pd.DataFrame()\n",
    "\n",
    "    return train_batches, val_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for batch creation\n",
    "batch_size_days = 5            # Size of each batch in days\n",
    "batch_segment = 3              # Number of segments per batch (controls overlap)\n",
    "validation_split_ratio = 0.1   # 10% of each vessel's data for validation\n",
    "\n",
    "# Run the batch creation for training and validation\n",
    "trainset, valset = create_batches(baseDataset, validation_split_ratio, batch_size_days, batch_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features which make generalization more difficult and only add noise\n",
    "def dropNoiseColumns(df):\n",
    "    # Drop columns that add noise or are no longer needed\n",
    "    #df = df.drop(columns=['vesselId', 'time', 'eta_port'])\n",
    "    df = df[df['train'] != 1]\n",
    "    df = df.drop(columns=['vesselId', 'time', 'train'])\n",
    "    return df\n",
    "\n",
    "trainset = dropNoiseColumns(trainset)\n",
    "valset = dropNoiseColumns(valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unused dataframes from pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ais_train_df: 316.40 MB\n",
      "ais_test_df: 8.54 MB\n",
      "baseDataset: 233.23 MB\n",
      "vessels_with_low_records: 0.00 MB\n",
      "trainset: 329.63 MB\n",
      "valset: 26.46 MB\n"
     ]
    }
   ],
   "source": [
    "# Find all DataFrames in memory\n",
    "dataframes_in_memory = {name: obj for name, obj in globals().items() if isinstance(obj, pd.DataFrame)}\n",
    "\n",
    "# Print the names and memory usage of each DataFrame\n",
    "for name, df in dataframes_in_memory.items():\n",
    "    print(f\"{name}: {df.memory_usage(deep=True).sum() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del ais_train_df \n",
    "\n",
    "# Run garbage collection to free up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scaling_factor(time_elapsed):\n",
    "    \"\"\"Calculates the scaling factor based on time_elapsed in hours.\"\"\"\n",
    "    if time_elapsed < 24:\n",
    "        return 0.3\n",
    "    elif 24 <= time_elapsed < 48:\n",
    "        return 0.25\n",
    "    elif 48 <= time_elapsed < 72:\n",
    "        return 0.2\n",
    "    elif 72 <= time_elapsed < 96:\n",
    "        return 0.15\n",
    "    elif 96 <= time_elapsed < 120:\n",
    "        return 0.1\n",
    "    else:\n",
    "        return 0.1\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def calculate_scaling_factor(time_elapsed):\n",
    "    \"\"\"Calculates the scaling factor based on time_elapsed in hours.\"\"\"\n",
    "    if time_elapsed < 24:\n",
    "        return 0.3\n",
    "    elif 24 <= time_elapsed < 48:\n",
    "        return 0.25\n",
    "    elif 48 <= time_elapsed < 72:\n",
    "        return 0.2\n",
    "    elif 72 <= time_elapsed < 96:\n",
    "        return 0.15\n",
    "    elif 96 <= time_elapsed < 120:\n",
    "        return 0.1\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "def calculate_distance(row):\n",
    "    \"\"\"Calculates the weighted distance between the actual and predicted lat/long points.\"\"\"\n",
    "    if pd.isna(row['y_lat']) or pd.isna(row['latitude_predicted']):\n",
    "        return np.nan\n",
    "\n",
    "    # Calculate the geodesic distance in meters\n",
    "    distance = geodesic(\n",
    "        (row['y_lat'], row['y_lon']),\n",
    "        (row['latitude_predicted'], row['longitude_predicted'])\n",
    "    ).meters\n",
    "\n",
    "    # Determine scaling factor based on time_elapsed\n",
    "    scaling_factor = calculate_scaling_factor(row['time_elapsed'])\n",
    "    \n",
    "    # Calculate weighted distance\n",
    "    weighted_distance = distance * scaling_factor\n",
    "    return weighted_distance\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the weighted distance between the actual and predicted latitude/longitude points.\n",
    "    \"\"\"\n",
    "    # Ensure necessary columns are present\n",
    "    required_columns = [\"longitude_predicted\", \"latitude_predicted\", \"time_elapsed\"]\n",
    "    if not all(col in submission.columns for col in required_columns):\n",
    "        raise ValueError(f'Submission must contain columns: {required_columns}')\n",
    "\n",
    "    # Merge the predictions with the ground truth data (assuming aligned indexes)\n",
    "    solution_submission = solution.join(\n",
    "        submission[['longitude_predicted', 'latitude_predicted', 'time_elapsed']]\n",
    "    )\n",
    "\n",
    "    # Calculate weighted distance for each row\n",
    "    solution_submission['weighted_distance'] = solution_submission.apply(calculate_distance, axis=1)\n",
    "    \n",
    "    # Calculate and return the mean weighted distance in kilometers\n",
    "    weighted_distance = solution_submission['weighted_distance'].mean() / 1000.0\n",
    "    return weighted_distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Retrieve the best hyperparameters from the Optuna study\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mstudy\u001b[49m\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_value)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "# Retrieve the best hyperparameters from the Optuna study\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", study.best_value)\n",
    "\n",
    "# Train the final model using the best hyperparameters\n",
    "final_model = MultiOutputRegressor(\n",
    "    RandomForestRegressor(\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        min_samples_split=best_params['min_samples_split'],\n",
    "        min_samples_leaf=best_params['min_samples_leaf'],\n",
    "        max_features=best_params['max_features'],\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train on the entire training set, including 'time_elapsed'\n",
    "X_train = trainset.drop(columns=['y_lat', 'y_lon'])\n",
    "y_train = trainset[['y_lat', 'y_lon']]\n",
    "\n",
    "# Fit the final model on the full training set\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set with 'time_elapsed' included\n",
    "X_val = valset.drop(columns=['y_lat', 'y_lon'])\n",
    "y_val = valset[['y_lat', 'y_lon']]\n",
    "submission = pd.DataFrame(final_model.predict(X_val), columns=['latitude_predicted', 'longitude_predicted'])\n",
    "submission['time_elapsed'] = X_val['time_elapsed'].values\n",
    "\n",
    "# Calculate the final score using the custom metric\n",
    "final_score = score(y_val, submission)\n",
    "print(\"Final validation score:\", final_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = ais_test_df.copy()\n",
    "\n",
    "def configure_testset(base_df, test_df, train_df):\n",
    "    # Sort by 'vesselId' and 'time'\n",
    "    test_df = test_df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "    # Drop the scaling factor column\n",
    "    test_df = test_df.drop(columns=['scaling_factor'])\n",
    "\n",
    "    # Identify the last known position of each vessel from training data\n",
    "    last_known_positions = base_df.groupby('vesselId').tail(1).copy()\n",
    "    last_known_positions['train'] = 1  # Set the 'train' flag to 1 for these rows\n",
    "\n",
    "    # Prepare an empty list to collect updated test batches\n",
    "    updated_test_set = []\n",
    "\n",
    "    # Process each vessel in the test set individually\n",
    "    for vessel_id in test_df['vesselId'].unique():\n",
    "        # Select the test batch for this vessel and make a deep copy to avoid SettingWithCopyWarning\n",
    "        vessel_test_batch = test_df[test_df['vesselId'] == vessel_id].copy()\n",
    "\n",
    "        # Select the last known position from training for this vessel\n",
    "        init_row = last_known_positions[last_known_positions['vesselId'] == vessel_id]\n",
    "        \n",
    "        # Concatenate the initial row to the start of this vessel's test batch\n",
    "        vessel_test_batch = pd.concat([init_row, vessel_test_batch], ignore_index=True)\n",
    "        \n",
    "        # Set 'train' flag to 0 for all subsequent rows in the test batch\n",
    "        vessel_test_batch.loc[1:, 'train'] = 0  # Use .loc to avoid chained assignment\n",
    "\n",
    "        # Add the modified batch to the list\n",
    "        updated_test_set.append(vessel_test_batch)\n",
    "\n",
    "    # Concatenate all modified batches into a single test dataframe\n",
    "    updated_test_df = pd.concat(updated_test_set, ignore_index=True)\n",
    "\n",
    "    #updated_test_df = updated_test_df.drop(columns=['y_lat', 'y_lon', 'eta_port'])\n",
    "    updated_test_df = updated_test_df.drop(columns=['y_lat', 'y_lon'])\n",
    "    return updated_test_df\n",
    "\n",
    "\n",
    "testset = configure_testset(baseDataset, testset, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTestSet(testset, trainset):\n",
    "    \"\"\"\n",
    "    Fills the testset by propagating the initial 'train' row values for each vessel and creating time_elapsed.\n",
    "\n",
    "    Parameters:\n",
    "    - testset (pd.DataFrame): The test dataframe, which includes columns for vesselId, time, and train indicator.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed test dataframe with filled values, time_elapsed column, and removed unnecessary columns.\n",
    "    \"\"\"\n",
    "    testset['time'] = pd.to_datetime(testset['time'])\n",
    "\n",
    "    # Initialize a list to store filled data for each vessel\n",
    "    filled_test_batches = []\n",
    "\n",
    "    # Process each vessel individually\n",
    "    for vessel_id in testset['vesselId'].unique():\n",
    "        # Select data for the current vessel\n",
    "        vessel_data = testset[testset['vesselId'] == vessel_id].copy()\n",
    "\n",
    "        # Identify the first row where train = 1 to use as reference\n",
    "        initial_row = vessel_data[vessel_data['train'] == 1].iloc[0]\n",
    "\n",
    "        # Fill all rows with values from the initial row, except for 'time' and 'train'\n",
    "        for col in vessel_data.columns:\n",
    "            if col not in ['time', 'train', 'ID']:\n",
    "                vessel_data.loc[vessel_data['train'] == 0, col] = initial_row[col]\n",
    "\n",
    "        # Create the time_elapsed column in hours\n",
    "        initial_time = initial_row['time']\n",
    "        vessel_data['time_elapsed'] = (vessel_data['time'] - initial_time).dt.total_seconds() / 3600\n",
    "\n",
    "        # Append the filled vessel data to the list\n",
    "        filled_test_batches.append(vessel_data)\n",
    "\n",
    "    # Concatenate all filled batches into a single DataFrame\n",
    "    filled_testset = pd.concat(filled_test_batches, ignore_index=True)\n",
    "\n",
    "    filled_testset = filled_testset[filled_testset['train'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    filled_testset = filled_testset.sort_values(by=['ID']).reset_index(drop=True)\n",
    "\n",
    "    test_ids = filled_testset['ID'].copy()\n",
    "\n",
    "    # Drop 'time' and 'vesselId' columns, and re-order columns to match the training set\n",
    "    filled_testset = filled_testset.drop(columns=['time', 'vesselId', 'ID', 'train'])\n",
    "    training_order = [col for col in trainset.columns if col not in ['y_lat', 'y_lon']]\n",
    "    filled_testset = filled_testset[training_order]\n",
    "\n",
    "    return filled_testset, test_ids\n",
    "\n",
    "testset, test_ids = fillTestSet(testset, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_append_coordinates(predictor, testset):\n",
    "    \"\"\"\n",
    "    Predicts longitude and latitude for the test set using a MultiOutputRegressor model\n",
    "    and appends them as new columns.\n",
    "\n",
    "    Parameters:\n",
    "    - predictor (MultiOutputRegressor): The trained model for predicting both latitude and longitude.\n",
    "    - testset (pd.DataFrame): The test set including all features required for prediction.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The test set DataFrame with appended 'latitude_predicted' and 'longitude_predicted' columns.\n",
    "    \"\"\"\n",
    "    # Predict both latitude and longitude on the test set\n",
    "    predictions = predictor.predict(testset)\n",
    "\n",
    "    # Extract predictions for latitude and longitude\n",
    "    latitude_predictions = predictions[:, 0]\n",
    "    longitude_predictions = predictions[:, 1]\n",
    "\n",
    "    # Append predictions to the test set DataFrame as new columns\n",
    "    testset_with_predictions = testset.copy()\n",
    "    testset_with_predictions['latitude_predicted'] = latitude_predictions\n",
    "    testset_with_predictions['longitude_predicted'] = longitude_predictions\n",
    "\n",
    "    return testset_with_predictions\n",
    "\n",
    "# Example usage:\n",
    "testset_with_predictions = predict_and_append_coordinates(final_model, testset)\n",
    "testset_with_predictions.head()  # To verify the appended predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_prepare_submission(predictor, testset, test_ids, submission_path='submission.csv'):\n",
    "    \"\"\"\n",
    "    Predicts latitude and longitude for the test set using a MultiOutputRegressor model, \n",
    "    merges the predictions with test IDs, and creates a submission file in the required Kaggle format.\n",
    "\n",
    "    Parameters:\n",
    "    - predictor (MultiOutputRegressor): The trained model for predicting both latitude and longitude.\n",
    "    - testset (pd.DataFrame): The test set including all features required for prediction.\n",
    "    - test_ids (pd.Series or list): The IDs for each entry in the test set.\n",
    "    - submission_path (str): The path to save the submission CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The final submission dataframe.\n",
    "    \"\"\"\n",
    "    # Step 1: Predict both latitude and longitude on the test set\n",
    "    predictions = predictor.predict(testset)\n",
    "\n",
    "    # Step 2: Extract latitude and longitude predictions\n",
    "    latitude_predictions = predictions[:, 0]\n",
    "    longitude_predictions = predictions[:, 1]\n",
    "\n",
    "    # Step 3: Combine predictions with test IDs into a single DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': test_ids,\n",
    "        'longitude_predicted': longitude_predictions,\n",
    "        'latitude_predicted': latitude_predictions\n",
    "    })\n",
    "\n",
    "    # Step 4: Save to CSV with the correct column order\n",
    "    submission_df.to_csv(submission_path, index=False, columns=['ID', 'longitude_predicted', 'latitude_predicted'])\n",
    "    print(f\"Submission file saved to {submission_path}\")\n",
    "\n",
    "    return submission_dfshortNotebookRF\n",
    "\n",
    "# Example usage:\n",
    "submission_df = predict_and_prepare_submission(final_model, testset, test_ids, '../../submissions/submissionRF.csv')\n",
    "submission_df.head()  # To verify the structure of the submission file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
